import streamlit as st
import streamlit.components.v1 as components
from azure.core.credentials import AzureKeyCredential
from azure.ai.documentintelligence import DocumentIntelligenceClient
from azure.ai.documentintelligence.models import AnalyzeResult
import google.generativeai as genai
from openai import OpenAI
import json
import time
import concurrent.futures
import pandas as pd
from thefuzz import fuzz
from collections import Counter
import re

GLOBAL_FUZZ_THRESHOLD = 70


# --- 1. é é¢è¨­å®š ---
st.set_page_config(page_title="äº¤è²¨å–®ç¨½æ ¸", page_icon="ğŸ­", layout="centered")

# --- CSS æ¨£å¼ ---
st.markdown("""
<style>
/* 1. æ¨™é¡Œå¤§å°æ§åˆ¶ */
h1 {
    font-size: 1.7rem !important; 
    white-space: nowrap !important;
    overflow: hidden !important; 
    text-overflow: ellipsis !important;
}

/* 2. ä¸»åŠŸèƒ½æŒ‰éˆ• (ç´…è‰² Primary) -> è®Šå¤§ã€è®Šé«˜ */
/* é€™æœƒå½±éŸ¿ã€Œé–‹å§‹åˆ†æã€å’Œã€Œç…§ç‰‡æ¸…é™¤ã€ */
button[kind="primary"] {
    height: 60px;               
    font-size: 20px !important; 
    font-weight: bold !important;
    border-radius: 10px !important;
    margin-top: 0px !important;    
    margin-bottom: 5px !important; 
    width: 100%;                
}

/* 3. æ¬¡è¦æŒ‰éˆ• (ç°è‰² Secondary) -> ä¿æŒåŸç‹€ */
/* é€™æœƒå½±éŸ¿æ¯ä¸€å¼µç…§ç‰‡ä¸‹é¢çš„ã€ŒXã€æŒ‰éˆ•ï¼Œè®“å®ƒç¶­æŒå°å°çš„ */
button[kind="secondary"] {
    height: auto !important;
    font-weight: normal !important;
}
</style>
""", unsafe_allow_html=True)
# --- 2. ç§˜å¯†é‡‘é‘°è®€å– ---
try:
    DOC_ENDPOINT = st.secrets["DOC_ENDPOINT"]
    DOC_KEY = st.secrets["DOC_KEY"]
    GEMINI_KEY = st.secrets["GEMINI_KEY"]
    OPENAI_KEY = st.secrets.get("OPENAI_KEY", "")
except:
    st.error("æ‰¾ä¸åˆ°é‡‘é‘°ï¼è«‹åœ¨ Streamlit Cloud è¨­å®š Secretsã€‚")
    st.stop()

# --- 3. åˆå§‹åŒ– Session State ---
if 'photo_gallery' not in st.session_state: st.session_state.photo_gallery = []
if 'uploader_key' not in st.session_state: st.session_state.uploader_key = 0
if 'auto_start_analysis' not in st.session_state: st.session_state.auto_start_analysis = False

# --- å´é‚Šæ¬„æ¨¡å‹è¨­å®š (åˆä½µç‚ºå–®ä¸€é¸æ“‡) ---
with st.sidebar:
    st.header("æ¨¡å‹è¨­å®š")
    
    # é€™è£¡åŠ å…¥æœ€æ–°çš„ Gemini æ¨¡å‹
    model_options = {
        "Gemini 3 Flash preview": "gemini-3-flash-preview",
        "Gemini 2.5 Flash": "models/gemini-2.5-flash",
        "Gemini 2.5 Flash Lite": "gemini-2.5-flash-lite",
        "Gemini 2.5 Pro": "models/gemini-2.5-pro",
        "GPT-5 Mini": "models/gpt-5-mini-2025-08-07",
        "GPT-5 Nano": "models/gpt-5-nano-2025-08-07",
        
    }
    options_list = list(model_options.keys())
    
    st.subheader("ğŸ¤– ç¸½ç¨½æ ¸ Agent")
    model_selection = st.selectbox(
        "è² è²¬ï¼šè¦æ ¼ã€è£½ç¨‹ã€æ•¸é‡ã€çµ±è¨ˆå…¨åŒ…", 
        options=options_list, 
        index=1, 
        key="main_model"
    )
    main_model_name = model_options[model_selection]
    
    st.divider()
    
    default_auto = st.query_params.get("auto", "true") == "true"
    def update_url_param():
        current_state = "true" if st.session_state.enable_auto_analysis else "false"
        st.query_params["auto"] = current_state

    st.toggle(
        "âš¡ ä¸Šå‚³å¾Œè‡ªå‹•åˆ†æ", 
        value=default_auto, 
        key="enable_auto_analysis", 
        on_change=update_url_param
    )

# --- Excel è¦å‰‡è®€å–å‡½æ•¸ (å°ˆæ¥­æ¥µç°¡ç‰ˆ) ---
@st.cache_data
def get_dynamic_rules(ocr_text, debug_mode=False):
    try:
        df = pd.read_excel("rules.xlsx")
        df.columns = [c.strip() for c in df.columns]
        ocr_text_clean = str(ocr_text).upper().replace(" ", "").replace("\n", "")
        
        ai_prompt_list = []    # çµ¦ AI çš„ (ç´”æ–‡å­—)
        debug_view_list = []   # çµ¦äººçœ‹çš„ (æ’ç‰ˆæ¸…æ½”)

        for index, row in df.iterrows():
            item_name = str(row.get('Item_Name', '')).strip()
            if not item_name or "(é€šç”¨)" in item_name: continue
            
            score = fuzz.partial_ratio(item_name.upper().replace(" ", ""), ocr_text_clean)
            if score >= 85:
                # å–å€¼èˆ‡æ¸…æ´—
                def clean(v): return str(v).strip() if v and str(v) != 'nan' else None
                
                spec = clean(row.get('Standard_Spec', ''))
                logic = clean(row.get('Logic_Prompt', ''))
                u_fr = clean(row.get('Unit_Rule_Freight', ''))
                u_loc = clean(row.get('Unit_Rule_Local', ''))
                u_agg = clean(row.get('Unit_Rule_Agg', ''))

                # --- A. å»ºæ§‹ AI Prompt (ç¶­æŒä¸è®Š) ---
                if not debug_mode:
                    if spec or logic:
                        desc = f"- [åƒè€ƒè³‡è¨Š] {item_name}\n"
                        if spec: desc += f"  - æ¨™æº–è¦æ ¼: {spec}\n"
                        if logic: desc += f"  - æ³¨æ„äº‹é …: {logic}\n"
                        ai_prompt_list.append(desc)
                
                # --- B. å»ºæ§‹ Debug é¡¯ç¤º (å»é™¤åœ–æ¡ˆï¼Œæ”¹ç”¨è¡¨æ ¼æ„Ÿæ’ç‰ˆ) ---
                else:
                    # ä½¿ç”¨ Markdown çš„å¼•ç”¨å€å¡Š (>) ä¾†åšå±¤ç´šå€åˆ†ï¼Œçœ‹èµ·ä¾†å¾ˆä¹¾æ·¨
                    block = f"#### â–  {item_name} (åŒ¹é…åº¦ {score}%)\n"
                    
                    # AI å€å¡Š
                    block += "**[ AI Prompt è¼¸å…¥ ]**\n"
                    if spec or logic:
                        if spec: block += f"- è¦æ ¼æ¨™æº– : `{spec}`\n"
                        if logic: block += f"- æ³¨æ„äº‹é … : `{logic}`\n"
                    else:
                        block += "- (ç„¡ç‰¹å®šè¼¸å…¥)\n"

                    # Python å€å¡Š
                    block += "\n**[ Python ç¡¬é‚è¼¯è¨­å®š ]**\n"
                    has_py = False
                    if u_fr: 
                        block += f"- é‹è²»é‚è¼¯ : `{u_fr}`\n"
                        has_py = True
                    if u_loc:
                        block += f"- å–®é …è¦å‰‡ : `{u_loc}`\n"
                        has_py = True
                    if u_agg:
                        block += f"- èšåˆè¦å‰‡ : `{u_agg}`\n"
                        has_py = True
                    
                    if not has_py:
                        block += "- (ä½¿ç”¨é è¨­é‚è¼¯)\n"
                    
                    block += "\n---\n"
                    debug_view_list.append(block)

        if debug_mode:
            if not debug_view_list: return "ç„¡ç‰¹å®šè¦å‰‡å‘½ä¸­ã€‚"
            return "\n".join(debug_view_list)
        else:
            return "\n".join(ai_prompt_list) if ai_prompt_list else ""

    except Exception as e:
        return f"è®€å–éŒ¯èª¤: {e}"

# --- 4. æ ¸å¿ƒå‡½æ•¸ï¼šAzure ç¥ä¹‹çœ¼ ---
def extract_layout_with_azure(file_obj, endpoint, key):
    client = DocumentIntelligenceClient(endpoint=endpoint, credential=AzureKeyCredential(key))
    file_content = file_obj.getvalue()
    
    poller = client.begin_analyze_document("prebuilt-layout", file_content, content_type="application/octet-stream")
    result: AnalyzeResult = poller.result()
    
    markdown_output = ""
    full_content_text = ""
    real_page_num = "Unknown"
    
    bottom_stop_keywords = ["æ³¨æ„äº‹é …", "ä¸­æ©Ÿå“æª¢å–®ä½", "ä¿å­˜æœŸé™", "è¡¨å–®ç·¨è™Ÿ", "FORM NO", "ç°½ç« "]
    top_right_noise_keywords = [
        "æª¢é©—é¡åˆ¥", "å°ºå¯¸æª¢é©—", "ä¾åœ–é¢æ¨™è¨˜", "ææ–™æª¢é©—", "æˆä»½åˆ†æ", 
        "éç ´å£æ€§", "æ­£å¸¸åŒ–", "é€€ç«", "æ·¬.å›ç«", "è¡¨é¢ç¡¬åŒ–", "è©¦è»Š",
        "æ€§èƒ½æ¸¬è©¦", "è©¦å£“è©¦æ¼", "å‹•.éœå¹³è¡¡è©¦é©—", ":selected:", ":unselected:",
        "æŠ—æ‹‰", "ç¡¬åº¦è©¦é©—", "UT", "PT", "MT"
    ]
    
    if result.tables:
        for idx, table in enumerate(result.tables):
            # 1. å–å¾—é ç¢¼ (ä¿ç•™åŸé‚è¼¯)
            page_num = "Unknown"
            if table.bounding_regions: page_num = table.bounding_regions[0].page_number
            
            # =========================================================
            # ğŸ” [æ–°å¢] æ™ºæ…§æ¨™ç±¤åµæ¸¬ï¼šåœ¨è™•ç†è¡¨æ ¼å‰ï¼Œå…ˆåˆ¤æ–·å®ƒæ˜¯èª°
            # =========================================================
            table_tag = "æœªçŸ¥è¡¨æ ¼"
            
            # æŠ€å·§ï¼šæŠ“å–è¡¨æ ¼ã€Œç¬¬ä¸€åˆ— (row_index=0)ã€çš„æ‰€æœ‰æ–‡å­—ä¾†åˆ¤æ–·
            # é€™æ¨£ä¸ç”¨è®€å®Œæ•´å¼µè¡¨ï¼Œåªè¦çœ‹è¡¨é ­å°±çŸ¥é“å®ƒæ˜¯ç¸½è¡¨é‚„æ˜¯æ˜ç´°
            first_cells = [c.content for c in table.cells if c.row_index == 0]
            first_row_text = "".join(first_cells)
            
            # å®šç¾©é—œéµå­— (æ‚¨å¯ä»¥æ ¹æ“šå¯¦éš›è¡¨æ ¼å¾®èª¿)
            summary_keywords = ["å¯¦äº¤", "ç”³è«‹", "åç¨±åŠè¦ç¯„", "å®Œæˆäº¤è²¨æ—¥æœŸ", "å­˜æ”¾ä½ç½®"]
            detail_keywords = ["è¦ç¯„æ¨™æº–", "æª¢é©—ç´€éŒ„", "å¯¦æ¸¬", "ç·¨è™Ÿ", "å°ºå¯¸", "W3 #", "å…¬å·®"]

            if any(k in first_row_text for k in summary_keywords):
                table_tag = "SUMMARY_TABLE (ç¸½è¡¨)"
            elif any(k in first_row_text for k in detail_keywords):
                table_tag = "DETAIL_TABLE (æ˜ç´°è¡¨)"
            
            # ğŸ“ [ä¿®æ”¹] è¼¸å‡ºæ¨™é ­ï¼šé€™è£¡ä¸å†åªå¯« Table Xï¼Œè€Œæ˜¯åŠ ä¸Šæˆ‘å€‘åˆ¤æ–·çš„æ¨™ç±¤
            # åŠ ä¸Š "===" æ˜¯ç‚ºäº†è®“ Prompt è£¡çš„ã€Œæ³¨æ„ç¯„åœã€æŒ‡ä»¤èƒ½ç²¾æº–é–å®š
            markdown_output += f"\n\n=== [{table_tag} | Page {page_num}] ===\n"
            # =========================================================

            rows = {}
            stop_processing_table = False 
            
            # --- ä»¥ä¸‹ä¿ç•™æ‚¨åŸæœ¬çš„ Cell è™•ç†é‚è¼¯ï¼Œå®Œå…¨ä¸ç”¨å‹• ---
            for cell in table.cells:
                if stop_processing_table: break
                content = cell.content.replace("\n", " ").strip()
                
                for kw in bottom_stop_keywords:
                    if kw in content:
                        stop_processing_table = True
                        break
                if stop_processing_table: break
                
                is_noise = False
                for kw in top_right_noise_keywords:
                    if kw in content:
                        is_noise = True
                        break
                if is_noise: content = "" 

                r, c = cell.row_index, cell.column_index
                if r not in rows: rows[r] = {}
                rows[r][c] = content
            
            for r in sorted(rows.keys()):
                row_cells = []
                if rows[r]:
                    max_col = max(rows[r].keys())
                    for c in range(max_col + 1): 
                        row_cells.append(rows[r].get(c, ""))
                    markdown_output += "| " + " | ".join(row_cells) + " |\n"

    if result.content:
        match = re.search(r"(?:é …æ¬¡|Page|é æ¬¡|NO\.)[:\s]*(\d+)\s*[/ï¼]\s*\d+", result.content, re.IGNORECASE)
        if match:
            real_page_num = match.group(1)

        cut_index = len(result.content)
        for keyword in bottom_stop_keywords:
            idx = result.content.find(keyword)
            if idx != -1 and idx < cut_index:
                cut_index = idx
        
        temp_text = result.content[:cut_index]
        for noise in top_right_noise_keywords:
            temp_text = temp_text.replace(noise, "")
            
        full_content_text = temp_text
        header_snippet = full_content_text[:800]
    else:
        full_content_text = ""
        header_snippet = ""

    return markdown_output, header_snippet, full_content_text, None, real_page_num
    
def python_engineering_audit(dimension_data):
    """
    Python å·¥ç¨‹å¼•æ“ (æ–°å¢ï¼šè² è²¬ Excel å¼·åˆ¶åˆ†é¡èˆ‡æ•¸å€¼æª¢æŸ¥)
    1. é€™æ˜¯åŸæœ¬æˆ‘å€‘è¦ä¿®æ”¹çš„é‚è¼¯ï¼Œç¾åœ¨ç¨ç«‹å‡ºä¾†ï¼Œä¸èˆ‡è¡¨é ­æª¢æŸ¥è¡çªã€‚
    2. è² è²¬åŸ·è¡Œï¼šRange(å†ç”Ÿ), Un_regen(æœ¬é«”), Max, Min, Exempt(è±å…)ã€‚
    """
    issues = []
    import re

    # è¼”åŠ©ï¼šæ•¸å€¼æå–
    def get_val(val_str):
        clean_v = "".join(re.findall(r"[\d\.\-]+", str(val_str)))
        try: return float(clean_v)
        except: return None

    # æ ¸å¿ƒæª¢æŸ¥è¿´åœˆ
    for item in dimension_data:
        p_num = item.get("page", "?")
        title = item.get("item_title", "Unknown")
        ds_str = item.get("ds", "")
        
        # 1. å–å¾—åˆ†é¡ (é€™è£¡æœƒå»å‘¼å«æˆ‘å€‘ç­‰ä¸‹è¦æ›´æ–°çš„ assign_category_by_python)
        # é€™ä¸€æ­¥æœ€é—œéµï¼å®ƒæœƒå»è®€ Excel çœ‹æœ‰æ²’æœ‰å¼·åˆ¶è¦å‰‡
        final_category = assign_category_by_python(title)
        
        # 2. âš¡ï¸ è±å…æ©Ÿåˆ¶ï¼šè‹¥ Excel è¨­å®šç‚ºã€Œè±å…ã€ï¼Œç›´æ¥è·³é
        if final_category == "exempt":
            continue

        # 3. åŸ·è¡Œå„é¡åˆ¥æª¢æŸ¥
        
        # A. Un_regen (æœ¬é«”æœªå†ç”Ÿ - å¼·åˆ¶æ•´æ•¸æª¢æŸ¥)
        if final_category == "un_regen":
            for pair in ds_str.split("|"):
                if ":" not in pair: continue
                rid, val_s = pair.split(":")[:2]
                val = get_val(val_s)
                
                if val is not None:
                    # æª¢æŸ¥æ˜¯å¦ç‚ºæ•´æ•¸ (å…è¨± 0.05 èª¤å·®)
                    if abs(val - round(val)) > 0.05:
                         issues.append({
                            "page": p_num,
                            "item": title,
                            "issue_type": "âš ï¸ç•°å¸¸(æœªå†ç”Ÿ)",
                            "common_reason": "æ‡‰ç‚ºæ•´æ•¸ (Excelè¦å‰‡:æœ¬é«”æœªå†ç”Ÿ)",
                            "failures": [{"id": rid, "val": val, "calc": "éæ•´æ•¸"}],
                            "source": "ğŸ å·¥ç¨‹å¼•æ“"
                        })

        # B. Range (å†ç”Ÿè»Šä¿® - å€é–“æª¢æŸ¥)
        elif final_category == "range":
            # é€™è£¡æ‚¨å¯ä»¥å‘¼å«åŸæœ¬å¯«å¥½çš„ check_range é‚è¼¯
            # æˆ–è€…æš«æ™‚ç•™ç©ºï¼Œè‡³å°‘å®ƒä¸æœƒèª¤åˆ¤æˆ "æœªå†ç”Ÿ"
            pass 

        # C. Max/Min Limit (è»¸é ¸/éŠ²è£œ)
        elif final_category == "max_limit" or final_category == "min_limit":
             # é€™è£¡å‘¼å«åŸæœ¬çš„ check_limit é‚è¼¯
             pass 

    return issues

def assign_category_by_python(item_title):
    """
    Python åˆ†é¡å®˜ (v11: ç†±è™•ç†/å‹•å¹³è¡¡è±å…ç‰ˆ)
    1. [è±å…]: å‹•å¹³è¡¡ã€ç†±è™•ç† -> ç›´æ¥ Exempt (ä¸é©—å°ºå¯¸)ã€‚
    2. [æ—¢æœ‰åŠŸèƒ½]: è»¸ä½/è»¸é ¸ Max Limitã€SKIP åˆ¤æ–·ç­‰ã€‚
    """
    import pandas as pd
    from thefuzz import fuzz
    import re

    def clean_text(text):
        return str(text).replace(" ", "").replace("\n", "").replace("\r", "").replace('"', '').replace("'", "").strip()

    title_clean = clean_text(item_title)
    t = str(item_title).upper().replace(" ", "").replace("\n", "").replace('"', "")

    # âš¡ï¸ [æ–°å¢] å‹•å¹³è¡¡ã€ç†±è™•ç†ç›´æ¥è±å… (ä¸é©—å°ºå¯¸ï¼Œä½†æœƒè¨ˆç…§å¸¸)
    if any(k in t for k in ["å‹•å¹³è¡¡", "BALANCING", "ç†±è™•ç†", "HEAT", "TREATING"]):
        return "exempt"

    try:
        df = pd.read_excel("rules.xlsx")
        df.columns = [c.strip() for c in df.columns]
        
        best_score = 0
        forced_rule = None
        
        for _, row in df.iterrows():
            rule_val = str(row.get('Category_Rule', '')).strip()
            if not rule_val or rule_val.lower() == 'nan': continue
            
            iname = str(row.get('Item_Name', '')).strip()
            iname_clean = clean_text(iname)
            
            score = fuzz.partial_ratio(iname_clean, title_clean)
            if score < 95: 
                 t_no = re.sub(r"[\(ï¼ˆ].*?[\)ï¼‰]", "", title_clean)
                 sc_no = fuzz.partial_ratio(iname_clean, t_no)
                 if sc_no > score: score = sc_no
            
            if score > 85: 
                if score > best_score:
                    best_score = score
                    forced_rule = rule_val
                elif score == best_score:
                    if len(rule_val) > len(forced_rule if forced_rule else ""):
                        forced_rule = rule_val

        if forced_rule:
            fr = forced_rule.upper()
            if "è±å…" in fr or "EXEMPT" in fr or "SKIP" in fr: return "exempt"
            
            if "å†ç”Ÿ" in fr or "ç²¾è»Š" in fr or "RANGE" in fr: return "range"
            if "éŠ²" in fr or "ç„Š" in fr or "MIN" in fr: return "min_limit"
            if "è»¸é ¸" in fr or "è»¸é ­" in fr or "è»¸ä½" in fr or "MAX" in fr: return "max_limit"
            if "æœ¬é«”" in fr or "UN_REGEN" in fr: return "un_regen"
            
    except Exception: pass

    has_weld = any(k in t for k in ["éŠ²è£œ", "éŠ²æ¥", "ç„Š", "WELD", "é‰€"])
    has_unregen = any(k in t for k in ["æœªå†ç”Ÿ", "UN_REGEN", "ç²—è»Š"])
    has_regen = any(k in t for k in ["å†ç”Ÿ", "ç ”ç£¨", "ç²¾åŠ å·¥", "è»Šä¿®", "KEYWAY", "GRIND", "MACHIN", "ç²¾è»Š", "çµ„è£", "æ‹†è£", "è£é…", "ASSY"])
    
    if has_weld: return "min_limit"
    if has_unregen:
        if any(k in t for k in ["è»¸é ¸", "è»¸é ­", "è»¸ä½", "å…§å­”", "JOURNAL"]): return "max_limit"
        return "un_regen"
    if has_regen: return "range"

    return "unknown"

def consolidate_issues(issues):
    """
    ğŸ—‚ï¸ ç•°å¸¸åˆä½µå™¨ï¼šå°‡ã€Œé …ç›®ã€ã€ã€ŒéŒ¯èª¤é¡å‹ã€ã€ã€ŒåŸå› ã€å®Œå…¨ç›¸åŒçš„ç•°å¸¸åˆä½µæˆä¸€å¼µå¡ç‰‡
    """
    grouped = {}
    for i in issues:
        key = (i.get('item', ''), i.get('issue_type', ''), i.get('common_reason', ''))
        if key not in grouped:
            grouped[key] = i.copy()
            grouped[key]['pages_set'] = {str(i.get('page', '?'))}
            grouped[key]['failures'] = i.get('failures', []).copy()
        else:
            grouped[key]['pages_set'].add(str(i.get('page', '?')))
            grouped[key]['failures'].extend(i.get('failures', []))
            
    result = []
    for key, val in grouped.items():
        sorted_pages = sorted(list(val['pages_set']), key=lambda x: int(x) if x.isdigit() else 999)
        val['page'] = ", ".join(sorted_pages)
        del val['pages_set']
        result.append(val)
    return result

# --- 5. ç¸½ç¨½æ ¸ Agent (é›™æ ¸å¿ƒå¼•æ“ç‰ˆï¼šGemini + OpenAI) ---
def agent_unified_check(combined_input, full_text_for_search, api_key, model_name):
    # 1. æº–å‚™ Prompt (è¦å‰‡èˆ‡æŒ‡ä»¤)
    dynamic_rules = get_dynamic_rules(full_text_for_search)

    system_prompt = f"""
    ä½ æ˜¯ä¸€ä½æ¥µåº¦åš´è¬¹çš„ä¸­é‹¼æ©Ÿæ¢°å“ç®¡ã€æ•¸æ“šæŠ„éŒ„å“¡ã€‘ã€‚ä½ å¿…é ˆåƒã€Œé›»è…¦ç¨‹å¼ã€ä¸€æ¨£åŸ·è¡Œä»»å‹™ã€‚
    
    {dynamic_rules}

    ---

    #### âš”ï¸ æ¨¡çµ„ Aï¼šå·¥ç¨‹å°ºå¯¸æ•¸æ“šæå– (AI ä»»å‹™ï¼šç´”æŠ„éŒ„)
    âš ï¸ **æ³¨æ„ç¯„åœ**ï¼šä½ åªèƒ½å¾æ¨™è¨˜ç‚º `=== [DETAIL_TABLE (æ˜ç´°è¡¨)] ===` çš„å€åŸŸæå–æ•¸æ“šã€‚
    
    1. **è¦æ ¼æŠ„éŒ„ (std_spec)**ï¼šç²¾ç¢ºæŠ„éŒ„æ¨™é¡Œä¸­å« `mm`ã€`Â±`ã€`+`ã€`-` çš„åŸå§‹æ–‡å­—ã€‚
    
    2. **æ¨™é¡ŒæŠ„éŒ„ (item_title)**ï¼šâš ï¸ æ¥µåº¦é‡è¦ï¼å¿…é ˆå®Œæ•´æŠ„éŒ„é …ç›®æ¨™é¡Œï¼Œ**åš´ç¦éºæ¼**ã€Œæœªå†ç”Ÿã€ã€ã€ŒéŠ²è£œã€ã€ã€Œè»Šä¿®ã€ã€ã€Œè»¸é ¸ã€ç­‰é—œéµå­—ã€‚
    
    3. **ç›®æ¨™æ•¸é‡æå– (item_pc_target)**ï¼š
       - è«‹å¾æ¨™é¡Œä¸­æå–æ‹¬è™Ÿå…§çš„æ•¸é‡è¦æ±‚ï¼ˆä¾‹å¦‚æ¨™é¡Œå« `(4SET)` å‰‡æå– `4`ï¼Œ`(10PC)` å‰‡æå– `10`ï¼‰ã€‚
       - è‹¥ç„¡æ‹¬è™Ÿæ¨™è¨»æ•¸é‡ï¼Œè«‹å¡« `0`ã€‚
    
    4. **ç‰¹æ®Šæ‰¹é‡ç¸½æ•¸æå– (batch_total_qty)ï¼š
       - è‹¥æ¨™é¡ŒåŒ…å«ã€Œç†±è™•ç†ã€ã€ã€Œç ”ç£¨ã€ã€ã€Œå‹•å¹³è¡¡ã€ä¸”å…§æ–‡ç¬¬ä¸€æ¬„ç‚ºåˆä½µå„²å­˜æ ¼é¡¯ç¤ºç¸½é‡ (å¦‚ 2425KG, 8293.80 IN2)ï¼š
       - è«‹å°‡è©²æ•¸å€¼æå–è‡³ JSON çš„ "batch_total_qty" æ¬„ä½ (ç´”æ•¸å­—)ã€‚
         (æ³¨æ„ï¼šç ”ç£¨èˆ‡å‹•å¹³è¡¡è‹¥å¾ŒçºŒé‚„æœ‰å€‹åˆ¥ ID èˆ‡å°ºå¯¸ï¼Œè«‹ç…§å¸¸æŠ„éŒ„åˆ° "ds"ã€‚)

    5. **åˆ†é¡ (category)**ï¼š**è«‹ç›´æ¥å›å‚³ `null`**ã€‚ç”±å¾Œç«¯ç¨‹å¼åˆ¤å®šã€‚

    6. **æ•¸æ“šæŠ„éŒ„ (ds) èˆ‡ å­—ä¸²ä¿è­·è¦ç¯„**ï¼š
       - **æ ¼å¼**ï¼šè¼¸å‡ºç‚º `"ID:å€¼|ID:å€¼"` çš„å­—ä¸²æ ¼å¼ã€‚
       - **ç¦æ­¢ç°¡åŒ–**ï¼šå¯¦æ¸¬å€¼è‹¥é¡¯ç¤º `349.90`ï¼Œå¿…é ˆè¼¸å‡º `"349.90"`ï¼Œä¿ç•™å°¾æ•¸ 0ã€‚
       - **ğŸš« é‡åˆ°å¹²æ“¾ä¸é‘½ç‰›è§’å°–**ï¼šè‹¥å„²å­˜æ ¼å…§çš„æ•¸å€¼å› æ‰‹å¯«å¡—æ”¹ã€åœ“åœˆé®æ“‹ã€æ±¡é»ã€å­—è·¡é»é€£æˆ–å…‰ç·šåå…‰ï¼Œå°è‡´ä½ ç„¡æ³•ã€Œ100% ç¢ºå®šã€åŸå§‹æ‰“å°æ•¸å­—æ™‚ï¼Œ**åš´ç¦è…¦è£œæˆ–çŒœæ¸¬**ã€‚
       - **å£è»Œæ¨™è¨˜ [BAD]**ï¼šè«‹å°‡è©²ç­†æ•¸å€¼ç›´æ¥æ¨™è¨˜ç‚º `[!]`ã€‚
       - **ç¯„ä¾‹**ï¼šè‹¥ ID æ¸…æ¥šä½†æ•¸å€¼æ¨¡ç³Š -> `"V100:[!]"`ï¼›è‹¥æ•´å€‹å„²å­˜æ ¼éƒ½çœ‹ä¸æ¸… -> `"[!] : [!]"`ã€‚
       - **è·³éç­–ç•¥**ï¼šä¸€æ—¦æ¨™è¨˜ç‚º `[!]`ï¼Œè«‹ç«‹å³è·³åˆ°ä¸‹ä¸€æ ¼ï¼Œä¸è¦æµªè²» Token æè¿°é›œè¨Šã€‚

    #### ğŸ’° æ¨¡çµ„ Bï¼šæœƒè¨ˆæŒ‡æ¨™æå– (AI ä»»å‹™ï¼šæŠ„éŒ„)
    âš ï¸ **æ³¨æ„ç¯„åœ**ï¼šä½ åªèƒ½å¾æ¨™è¨˜ç‚º `=== [SUMMARY_TABLE (ç¸½è¡¨)] ===` çš„å€åŸŸæå–æ•¸æ“šã€‚
    1. **çµ±è¨ˆè¡¨**ï¼šè«‹æå–æ¯ä¸€è¡Œçš„ä»¥ä¸‹ä¸‰å€‹æ¬„ä½ï¼š
       - **é …ç›®åç¨± (title)**
       - **ç”³è«‹æ•¸é‡ (apply_qty)**ï¼šé€šå¸¸åœ¨å·¦å´ã€‚
       - **å¯¦äº¤æ•¸é‡ (delivery_qty)**ï¼šé€šå¸¸åœ¨å³å´ (é€™æ˜¯æœƒè¨ˆæ ¸å°çš„åŸºæº–)ã€‚
       
    2. **é ç¢¼æ¨™è¨»**ï¼šè«‹å‹™å¿…åœ¨æ¯å€‹ `summary_rows` ç‰©ä»¶ä¸­è¨˜éŒ„è©²è¡Œæ‰€åœ¨çš„é ç¢¼ (`page`)ã€‚

    #### ğŸ“‹ æ¨¡çµ„ Cï¼šè¡¨é ­è³‡è¨Š (Header Info)
    âš ï¸ **æ³¨æ„ç¯„åœ**ï¼šä½ åªèƒ½å¾æ¨™è¨˜ç‚º `=== [SUMMARY_TABLE (ç¸½è¡¨)] ===` çš„å€åŸŸæå–æ•¸æ“šã€‚
    1. **å·¥ä»¤å–®è™Ÿ (job_no)**ï¼šé€šå¸¸æ˜¯ 10 ç¢¼ï¼Œç”±è‹±æ–‡å­—æ¯ (W, R, O, Y) é–‹é ­ï¼Œä¸¦ä¸”ä¸æœƒå«è¶…é3å€‹è‹±æ–‡å­—æ¯ã€‚
    2. **é å®šäº¤è²¨æ—¥ (scheduled_date)**ï¼šè«‹å°‡æ—¥æœŸçµ±ä¸€æ ¼å¼åŒ–ç‚º "YYYY/MM/DD"ã€‚
    3. **å¯¦éš›äº¤è²¨æ—¥ (actual_date)**ï¼šè«‹å°‡æ—¥æœŸçµ±ä¸€æ ¼å¼åŒ–ç‚º "YYYY/MM/DD"ã€‚

    ### ğŸ“ è¼¸å‡ºè¦ç¯„ (Output Format)
    å¿…é ˆå›å‚³å–®ä¸€ JSONã€‚

    {{
      "header_info": {{
          "job_no": "Wxxxxxxxxx",
          "scheduled_date": "YYYY/MM/DD",
          "actual_date": "YYYY/MM/DD"
      }},
      "summary_rows": [ 
          {{ "page": é ç¢¼, "title": "å", "apply_qty": æ•¸å­—, "delivery_qty": æ•¸å­— }} 
      ], 
      "issues": [], 
      "dimension_data": [
         {{
           "page": æ•¸å­—, "item_title": "æ¨™é¡Œ", "batch_total_qty": 0, "category": null, 
           "item_pc_target": 0,
           "std_spec": "åŸå§‹è¦æ ¼æ–‡å­—",
           "ds": "ID:å€¼|ID:å€¼" 
         }}
      ]
    }}
    """
    
    # 2. åˆ¤æ–·è¦ä½¿ç”¨å“ªä¸€é¡†å¼•æ“
    raw_content = ""
    
    # --- å¼•æ“ A: OpenAI GPT ç³»åˆ— ---
    if "gpt" in model_name.lower():
        try:
            # å¿…é ˆä½¿ç”¨å…¨åŸŸè®Šæ•¸ OPENAI_KEYï¼Œå› ç‚ºå‚³å…¥çš„ api_key åƒæ•¸é€šå¸¸æ˜¯ GEMINI_KEY
            openai_key = st.secrets.get("OPENAI_KEY", "")
            if not openai_key:
                return {"job_no": "Error: ç¼ºå°‘ OPENAI_KEY", "issues": [], "dimension_data": []}
                
            client = OpenAI(api_key=openai_key)
            
            response = client.chat.completions.create(
                model=model_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": combined_input}
                ],
                temperature=0.0,
                response_format={"type": "json_object"} # GPT-4o æ”¯æ´å¼·åˆ¶ JSON æ¨¡å¼
            )
            raw_content = response.choices[0].message.content
            
            # æ¨¡æ“¬ Token ç”¨é‡ (OpenAI æ ¼å¼ä¸åŒï¼Œé€™è£¡åšå€‹ç°¡å–®è½‰æ›ä»¥ä¾¿çµ±ä¸€é¡¯ç¤º)
            input_tokens = response.usage.prompt_tokens
            output_tokens = response.usage.completion_tokens
            
        except Exception as e:
            return {"job_no": f"OpenAI Error: {str(e)}", "issues": [], "dimension_data": []}

    # --- å¼•æ“ B: Google Gemini ç³»åˆ— ---
    else:
        try:
            genai.configure(api_key=api_key) # é€™è£¡ç”¨å‚³å…¥çš„ GEMINI_KEY
            generation_config = {"response_mime_type": "application/json", "temperature": 0.0}
            model = genai.GenerativeModel(model_name)
            
            # Gemini 2.0 å¯èƒ½éœ€è¦ä¸åŒçš„å‘¼å«æ–¹å¼ï¼Œé€™è£¡ä¿æŒé€šç”¨æ¥å£
            response = model.generate_content([system_prompt, combined_input], generation_config=generation_config)
            raw_content = response.text
            
            input_tokens = response.usage_metadata.prompt_token_count
            output_tokens = response.usage_metadata.candidates_token_count
            
        except Exception as e:
            return {"job_no": f"Gemini Error: {str(e)}", "issues": [], "dimension_data": []}

    # 3. çµ±ä¸€è§£æèˆ‡å›å‚³
    try:
        # ğŸ›¡ï¸ è¶…ç´šè§£æå™¨ï¼šé˜²æ­¢ AI è¼¸å‡ºå¸¶æœ‰ Markdown æ¨™ç±¤æˆ–å»¢è©±
        import re
        json_match = re.search(r"\{.*\}", raw_content, re.DOTALL)
        if json_match:
            raw_content = json_match.group()
            
        parsed_data = json.loads(raw_content)
        
        # çµ±ä¸€ Token ç”¨é‡æ ¼å¼
        parsed_data["_token_usage"] = {
            "input": input_tokens, 
            "output": output_tokens
        }
        return parsed_data

    except Exception as e:
        return {"job_no": f"JSON Parsing Error: {str(e)}", "issues": [], "dimension_data": []}

# --- é‡é»ï¼šPython å¼•æ“ç¨ç«‹æ–¼ agent å‡½å¼ä¹‹å¤– ---

def python_numerical_audit(dimension_data):
    """
    Python å·¥ç¨‹å¼•æ“ (v29: å…¨åŸŸçµ±ä¸€ç‰¹è¦ç‰ˆ)
    å‡ç´šå…§å®¹ï¼š
    1. [çµ±ä¸€é…å°]: å¼•å…¥èˆ‡æœƒè¨ˆåŒç´šçš„é…å°é‚è¼¯ (GLOBAL_FUZZ_THRESHOLD + fuzz.ratio)ã€‚
    2. [è¦å‰‡å„ªå…ˆ]: è‹¥ Excel ç‰¹è¦é…å°æˆåŠŸä¸”è¨­å®šç‚º SKIP/EXEMPTï¼Œç›´æ¥è±å…ã€‚
    3. [åŸæœ‰é‚è¼¯]: ä¿ç•™ç†±è™•ç†/å‹•å¹³è¡¡è±å…ï¼Œä»¥åŠå„ç¨®æ•¸å€¼æª¢æŸ¥é‚è¼¯ã€‚
    """
    grouped_errors = {}
    import re
    import pandas as pd
    from thefuzz import fuzz

    # ğŸ”¥ 1. è®€å–å…¨åŸŸé–€æª» (èˆ‡æœƒè¨ˆåŒæ­¥)
    CURRENT_THRESHOLD = globals().get('GLOBAL_FUZZ_THRESHOLD', 95)

    if not dimension_data: return []

    # ğŸ”¥ 2. é å…ˆè¼‰å…¥è¦å‰‡ (åªè¼‰å…¥ä¸€æ¬¡)
    rules_map = {}
    try:
        df = pd.read_excel("rules.xlsx")
        df.columns = [c.strip() for c in df.columns]
        for _, row in df.iterrows():
            iname = str(row.get('Item_Name', '')).strip()
            if iname: 
                # å·¥ç¨‹ä¸»è¦çœ‹ Local è¦å‰‡ (æ˜¯å¦è±å…)
                rules_map[str(iname).replace(" ", "").replace("\n", "").strip()] = {
                    "u_local": str(row.get('Unit_Rule_Local', '')).strip()
                }
    except: pass

    for item in dimension_data:
        ds = str(item.get("ds", ""))
        if not ds: continue
        raw_entries = [p.split(":") for p in ds.split("|") if ":" in p]
        
        title = str(item.get("item_title", "")).replace(" ", "").replace('"', "")
        cat = str(item.get("category", "")).strip()
        page_num = item.get("page", "?")
        raw_spec = str(item.get("std_spec", "")).replace('"', "")
        
        # =========================================================
        # ğŸ”¥ 3. åŸ·è¡Œç‰¹è¦é…å° (çµ±ä¸€é‚è¼¯)
        # =========================================================
        title_clean = title.strip()
        rule_set = None
        
        # A. å®Œå…¨åŒ¹é…
        if title_clean in rules_map:
            rule_set = rules_map[title_clean]
        
        # B. å»æ‹¬è™ŸåŒ¹é…
        if not rule_set:
            t_no = re.sub(r"[\(ï¼ˆ].*?[\)ï¼‰]", "", title_clean)
            if t_no in rules_map:
                rule_set = rules_map[t_no]
        
        # C. æ¨¡ç³ŠåŒ¹é… (ä½¿ç”¨å…¨åŸŸé–€æª» + åš´æ ¼æ¯”å°)
        if not rule_set and rules_map:
            best_score = 0
            for k, v in rules_map.items():
                sc = fuzz.token_sort_ratio(k, title_clean) # åš´æ ¼æ¯”å°
                if sc > CURRENT_THRESHOLD and sc > best_score:
                    best_score = sc
                    rule_set = v
        # =========================================================

        # âš¡ï¸ [æ—¢æœ‰è±å…] å‹•å¹³è¡¡ã€ç†±è™•ç†ç›´æ¥è·³é (é—œéµå­—å„ªå…ˆ)
        t_upper = title.upper()
        if any(k in t_upper for k in ["å‹•å¹³è¡¡", "BALANCING", "ç†±è™•ç†", "HEAT"]):
            continue
            
        # âš¡ï¸ [è¦å‰‡è±å…] å¦‚æœ Excel è¦å‰‡èªªè¦ SKIPï¼Œå°±è·³é
        if rule_set:
            u_local = rule_set.get("u_local", "").upper()
            if "SKIP" in u_local or "EXEMPT" in u_local or "è±å…" in u_local:
                continue

        # --- ä»¥ä¸‹ç‚ºæ•¸å€¼æå–èˆ‡æª¢æŸ¥é‚è¼¯ (ä¿æŒ v28 åŸè²Œ) ---
        
        mm_nums = [float(n) for n in re.findall(r"(\d+\.?\d*)\s*mm", raw_spec)]
        all_nums = [float(n) for n in re.findall(r"(\d+\.?\d*)", raw_spec)]
        noise = [350.0, 300.0, 200.0, 145.0, 130.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0]
        clean_std = [n for n in all_nums if (n in mm_nums) or (n not in noise and n > 5)]

        s_ranges = []
        spec_parts = re.split(r"[\n\r]|[ä¸€äºŒä¸‰å››äº”å…­]|[ï¼ˆ(]\d+[)ï¼‰]|[;ï¼›]", raw_spec)
        
        for part in spec_parts:
            clean_part = part.replace(" ", "").replace("\n", "").replace("mm", "").replace("MM", "").strip()
            if not clean_part: continue
            
            pm_matches = list(re.finditer(r"(\d+\.?\d*)?Â±(\d+\.?\d*)", clean_part))
            if pm_matches:
                for match in pm_matches:
                    base_str, offset_str = match.group(1), match.group(2)
                    b = float(base_str) if base_str else 0.0
                    o = float(offset_str)
                    s_ranges.append([round(b - o, 4), round(b + o, 4)])
                continue 

            tilde_matches = list(re.finditer(r"(\d+\.?\d*)[~ï½-](\d+\.?\d*)", clean_part))
            has_valid_tilde = False
            if tilde_matches:
                for match in tilde_matches:
                    n1, n2 = float(match.group(1)), float(match.group(2))
                    if abs(n1 - n2) < n1 * 0.5:
                        s_ranges.append([round(min(n1, n2), 4), round(max(n1, n2), 4)])
                        has_valid_tilde = True
            
            if has_valid_tilde: continue

            all_numbers = re.findall(r"[-+]?\d+\.?\d*", clean_part)
            if not all_numbers: continue

            try:
                bases = []
                offsets = []
                for token in all_numbers:
                    val = float(token)
                    if val > 10.0: bases.append(val)
                    elif abs(val) < 10.0: offsets.append(val)
                
                if bases:
                    for b in bases:
                        if offsets:
                            endpoints = [round(b + o, 4) for o in offsets]
                            if len(endpoints) == 1: endpoints.append(b)
                            s_ranges.append([min(endpoints), max(endpoints)])
                        else:
                            s_ranges.append([b, b])
            except: continue
                    
        logic = item.get("sl", {})
        l_type = logic.get("lt", "")
        
        # 4. é ç®—åŸºæº–
        if "SKIP" in l_type.upper() or "EXEMPT" in l_type.upper() or "è±å…" in l_type:
            un_regen_target = None
            
        elif l_type in ["range", "max_limit", "min_limit"]:
            un_regen_target = None
            
        else:
            s_threshold = logic.get("t", 0)
            un_regen_target = None
            if l_type in ["un_regen", "æœªå†ç”Ÿ"] or ("æœªå†ç”Ÿ" in (cat + title) and not any(k in (cat + title) for k in ["è»¸é ¸", "è»¸é ­", "è»¸ä½"])):
                cands = [n for n in clean_std if n >= 120.0]
                if s_threshold and float(s_threshold) >= 120.0: cands.append(float(s_threshold))
                if cands: un_regen_target = max(cands)

        for entry in raw_entries:
            if len(entry) < 2: continue
            rid = str(entry[0]).strip().replace(" ", "")
            val_raw = str(entry[1]).strip().replace(" ", "")
            
            if not val_raw or val_raw in ["N/A", "nan", "M10"]: continue

            try:
                is_passed, reason, t_used, engine_label = True, "", "N/A", "æœªçŸ¥"

                if "[!]" in val_raw:
                    is_passed = False
                    reason = "ğŸ›‘æ•¸æ“šæå£(å£è»Œ)"
                    val_str = "[!]"
                    val = -999.0 
                else:
                    v_m = re.findall(r"\d+\.?\d*", val_raw)
                    val_str = v_m[0] if v_m else val_raw
                    val = float(val_str)

                if val_str != "[!]":
                    is_two_dec = "." in val_str and len(val_str.split(".")[-1]) == 2
                    is_pure_int = "." not in val_str
                else:
                    is_two_dec, is_pure_int = True, True 

                if "SKIP" in l_type.upper() or "EXEMPT" in l_type.upper():
                    continue

                elif "min_limit" in l_type or "éŠ²è£œ" in (cat + title):
                    engine_label = "éŠ²è£œ"
                    if not is_pure_int: is_passed, reason = False, "æ‡‰ç‚ºç´”æ•´æ•¸"
                    elif clean_std:
                        t_used = min(clean_std, key=lambda x: abs(x - val))
                        if val < t_used: is_passed, reason = False, "æ•¸å€¼ä¸è¶³"
                
                elif un_regen_target is not None:
                    engine_label = "æœªå†ç”Ÿ"
                    t_used = un_regen_target
                    if val <= t_used:
                        if not is_pure_int: is_passed, reason = False, "æ‡‰ç‚ºæ•´æ•¸"
                    elif not is_two_dec: 
                        is_passed, reason = False, "æ‡‰å¡«å…©ä½å°æ•¸"

                elif l_type == "max_limit" or (any(k in (cat + title) for k in ["è»¸é ¸", "è»¸é ­", "è»¸ä½"]) and ("æœªå†ç”Ÿ" in (cat + title))):
                    engine_label = "è»¸é ¸(ä¸Šé™)"
                    candidates = clean_std
                    target = max(candidates) if candidates else 0
                    t_used = target
                    if target > 0:
                        if not is_pure_int: is_passed, reason = False, "æ‡‰ç‚ºç´”æ•´æ•¸"
                        elif val > target: is_passed, reason = False, f"è¶…éä¸Šé™ {target}"

                elif l_type == "range" or (any(x in (cat + title) for x in ["å†ç”Ÿ", "ç²¾åŠ å·¥", "ç ”ç£¨", "è»Šä¿®", "çµ„è£", "æ‹†è£", "çœŸåœ“åº¦"]) and "æœªå†ç”Ÿ" not in (cat + title)):
                    engine_label = "ç²¾åŠ å·¥"
                    if not is_two_dec:
                        is_passed, reason = False, "æ‡‰å¡«å…©ä½å°æ•¸"
                    elif s_ranges:
                        t_used = str(s_ranges)
                        if not any(r[0] <= val <= r[1] for r in s_ranges): 
                            is_passed, reason = False, "ä¸åœ¨å€é–“å…§"

                if not is_passed:
                    key = (page_num, title, reason)
                    if key not in grouped_errors:
                        grouped_errors[key] = {
                            "page": page_num, "item": title, 
                            "issue_type": f"ç•°å¸¸({engine_label})", 
                            "common_reason": reason, "failures": [],
                            "source": "ğŸ å·¥ç¨‹å¼•æ“"
                        }
                    grouped_errors[key]["failures"].append({"id": rid, "val": val_str, "target": f"åŸºæº–:{t_used}"})
            except: continue
                
    return list(grouped_errors.values())
    
def python_accounting_audit(dimension_data, res_main):
    """
    Python æœƒè¨ˆå®˜ (v53: å…¨åŸŸç‰¹è¦æ¨¡ç³Šæ¯”å°ç‰ˆ)
    ä¿®æ”¹é‡é»ï¼š
    1. [å…¨åŸŸé€£å‹•]: ä¸å†ä½¿ç”¨å¯«æ­»çš„ FUZZ_THRESHOLDã€‚
       - æ”¹ç‚ºè®€å– globals().get('GLOBAL_FUZZ_THRESHOLD', 90)ã€‚
       - è®“æœƒè¨ˆã€å·¥ç¨‹ã€æµç¨‹èƒ½çµ±ä¸€ä½¿ç”¨å¤–éƒ¨è¨­å®šçš„é–€æª»ã€‚
    2. [åŠŸèƒ½ä¿ç•™]: 
       - åŸ·è¡Œé«˜é–€æª»æ¯”å° (é è¨­ä½¿ç”¨å…¨åŸŸè¨­å®š)ã€‚
       - å°‡å‘½ä¸­ç´€éŒ„æ‰“åŒ…å›å‚³ (HIDDEN_DATA)ã€‚
       - æ ¸å¿ƒç±ƒå­é‚è¼¯ç¶­æŒä¸è®Šã€‚
    """
    accounting_issues = []
    from thefuzz import fuzz
    from collections import Counter
    import re
    import pandas as pd 

    # --- 0. è¨­å®š (æ”¹ç‚ºè®€å–å…¨åŸŸè®Šæ•¸) ---
    # å˜—è©¦è®€å–å…¨åŸŸè¨­å®šï¼Œå¦‚æœæ²’è¨­å®šå‰‡é è¨­ç‚º 90 (ä¾æ‚¨æä¾›çš„ä»£ç¢¼é è¨­å€¼)
    CURRENT_THRESHOLD = globals().get('GLOBAL_FUZZ_THRESHOLD', 90)

    def clean_text(text):
        return str(text).replace(" ", "").replace("\n", "").replace("\r", "").replace('"', '').replace("'", "").strip()

    def safe_float(value):
        if value is None or str(value).upper() == 'NULL': return 0.0
        if "[!]" in str(value): return "BAD_DATA" 
        cleaned = "".join(re.findall(r"[\d\.]+", str(value).replace(',', '')))
        try: return float(cleaned) if cleaned else 0.0
        except: return 0.0

    def parse_ratio(rule_str):
        if not rule_str: return 1.0
        match = re.search(r"(\d+)\s*/\s*(\d+)", str(rule_str))
        if match:
            n, d = float(match.group(1)), float(match.group(2))
            if d != 0: return n / d
        return 1.0

    # --- 1. è¼‰å…¥è¦å‰‡ ---
    rules_map = {}
    try:
        df = pd.read_excel("rules.xlsx")
        df.columns = [c.strip() for c in df.columns]
        for _, row in df.iterrows():
            iname = str(row.get('Item_Name', '')).strip()
            if iname: 
                rules_map[clean_text(iname)] = {
                    "u_local": str(row.get('Unit_Rule_Local', '')).strip(),
                    "u_fr": str(row.get('Unit_Rule_Freight', '')).strip(),
                    "u_agg": str(row.get('Unit_Rule_Agg', '')).strip()
                }
    except: pass 

    summary_rows = res_main.get("summary_rows", [])
    
    # ğŸ”¥ ç‰¹è¦å‘½ä¸­ç´€éŒ„å™¨
    rule_hits_log = {} 

    # =================================================
    # ğŸ•µï¸â€â™‚ï¸ ç¬¬ä¸€é—œï¼šç¸½è¡¨å…§æˆ°
    # =================================================
    global_sum_tracker = {}
    for s in summary_rows:
        s_title = s.get('title', 'Unknown')
        q_apply = safe_float(s.get('apply_qty', 0))      
        q_deliver = safe_float(s.get('delivery_qty', 0)) 
        if q_deliver == 0 and 'target' in s: q_deliver = safe_float(s.get('target', 0))

        if abs(q_apply - q_deliver) > 0.01:
             accounting_issues.append({
                "page": s.get('page', "ç¸½è¡¨"), 
                "item": f"{s_title}", 
                "issue_type": "ğŸš¨ ç¸½è¡¨æ•¸é‡ç•°å¸¸", 
                "common_reason": f"ç”³è«‹({q_apply}) != å¯¦äº¤({q_deliver})", 
                "failures": [
                    {"é ç¢¼": "ç¸½è¡¨", "é …ç›®åç¨±": "ğŸ“ ç”³è«‹æ•¸é‡", "æ•¸é‡": q_apply, "å‚™è¨»": "åŸå§‹å€¼"},
                    {"é ç¢¼": "ç¸½è¡¨", "é …ç›®åç¨±": "ğŸš› å¯¦äº¤æ•¸é‡", "æ•¸é‡": q_deliver, "å‚™è¨»": "æ ¸å°å€¼"}
                ], 
                "source": "ğŸ æœƒè¨ˆå¼•æ“"
            })
        global_sum_tracker[s_title] = {
            "target": q_deliver, "actual": 0, "details": [], "page": s.get('page', "ç¸½è¡¨")
        }

    # =================================================
    # ğŸ•µï¸â€â™‚ï¸ ç¬¬äºŒé—œï¼šé€é …æƒæ
    # =================================================
    for item in dimension_data:
        raw_title = item.get("item_title", "")
        title_clean = clean_text(raw_title) 
        page = item.get("page", "?")
        target_pc = safe_float(item.get("item_pc_target", 0)) 
        batch_qty = safe_float(item.get("batch_total_qty", 0))
        
        # 2.1 è¦å‰‡åŒ¹é… (ç´€éŒ„é‚è¼¯)
        rule_set = None
        matched_rule_name = None
        match_type = ""
        match_score = 0

        # A. å®Œå…¨åŒ¹é…
        if title_clean in rules_map:
            rule_set = rules_map[title_clean]
            matched_rule_name = title_clean
            match_type = "å®Œå…¨åŒ¹é…"
            match_score = 100
        
        # B. å»æ‹¬è™ŸåŒ¹é…
        if not rule_set:
            t_no = re.sub(r"[\(ï¼ˆ].*?[\)ï¼‰]", "", title_clean)
            if t_no in rules_map:
                rule_set = rules_map[t_no]
                matched_rule_name = t_no
                match_type = "å»æ‹¬è™ŸåŒ¹é…"
                match_score = 100

        # C. æ¨¡ç³ŠåŒ¹é… (ä½¿ç”¨å…¨åŸŸè®Šæ•¸ CURRENT_THRESHOLD)
        if not rule_set and rules_map:
            best_score = 0
            best_rule = None
            for k, v in rules_map.items():
                sc = fuzz.token_sort_ratio(k, title_clean) 
                # ğŸ”¥ æ”¹ç”¨ CURRENT_THRESHOLD
                if sc > CURRENT_THRESHOLD and sc > best_score:
                    best_score = sc
                    rule_set = v
                    best_rule = k
            
            if rule_set:
                matched_rule_name = best_rule
                match_type = "æ¨¡ç³ŠåŒ¹é…"
                match_score = best_score
        
        # è¨˜éŒ„å‘½ä¸­
        if matched_rule_name:
            if matched_rule_name not in rule_hits_log:
                rule_hits_log[matched_rule_name] = []
            
            rule_hits_log[matched_rule_name].append({
                "æ˜ç´°åç¨±": raw_title,
                "åŒ¹é…é¡å‹": match_type,
                "åˆ†æ•¸": match_score,
                "é ç¢¼": page
            })

        # --- ä»¥ä¸‹ç‚ºæ—¢æœ‰é‚è¼¯ ---
        u_local = rule_set.get("u_local", "") if rule_set else ""
        u_fr = rule_set.get("u_fr", "") if rule_set else ""
        u_agg = rule_set.get("u_agg", "") if rule_set else ""
        
        ds = str(item.get("ds", ""))
        data_list = [pair.split(":") for pair in ds.split("|") if ":" in pair]
        raw_count = len(data_list) if data_list else 0
        id_counts = Counter([str(e[0]).strip() for e in data_list if len(e)>0])

        # A. å–®é …æª¢æŸ¥
        is_local_exempt = "è±å…" in str(u_local) or "SKIP" in str(u_local).upper() or "EXEMPT" in str(u_local).upper()
        actual_item_qty = raw_count if batch_qty > 0 else raw_count * parse_ratio(u_local)
        if not is_local_exempt and abs(actual_item_qty - target_pc) > 0.01 and target_pc > 0:
             accounting_issues.append({
                 "page": page, "item": raw_title, "issue_type": "ğŸ›‘ çµ±è¨ˆä¸ç¬¦(å–®é …)", 
                 "common_reason": f"æ¨™é¡Œ {target_pc} != å…§æ–‡ {actual_item_qty}", 
                 "failures": [], "source": "ğŸ æœƒè¨ˆå¼•æ“"
             })

        # B. é‡è¤‡æª¢æŸ¥
        journal_family = ["è»¸é ¸", "è»¸é ­", "è»¸ä½", "å…§å­”", "JOURNAL"]
        if "æœ¬é«”" in title_clean:
             for rid, count in id_counts.items():
                if count > 1: accounting_issues.append({"page": page, "item": raw_title, "issue_type": "âš ï¸ç·¨è™Ÿé‡è¤‡(æœ¬é«”)", "common_reason": f"{rid} é‡è¤‡ {count}æ¬¡", "failures": []})
        elif any(k in title_clean for k in journal_family):
             for rid, count in id_counts.items():
                if count > 2: accounting_issues.append({"page": page, "item": raw_title, "issue_type": "âš ï¸ç·¨è™Ÿé‡è¤‡(è»¸é ¸)", "common_reason": f"{rid} é‡è¤‡ {count}æ¬¡", "failures": []})

        # C. é‹è²» & æ­¸æˆ¶
        fr_multiplier = parse_ratio(u_fr)
        freight_val = 0.0
        f_note = ""
        u_fr_upper = str(u_fr).upper()
        is_fr_exempt = "è±å…" in u_fr_upper or "SKIP" in u_fr_upper
        is_forced_include = "è¨ˆå…¥" in str(u_fr) or "INCLUDED" in u_fr_upper
        is_default_target = ("æœ¬é«”" in title_clean and "æœªå†ç”Ÿ" in title_clean) or ("æ–°å“çµ„è£" in title_clean)
        
        if not is_fr_exempt and (is_default_target or is_forced_include or fr_multiplier != 1.0):
            freight_val = actual_item_qty * fr_multiplier
            f_note = f"x{fr_multiplier}" if fr_multiplier != 1.0 else ""

        # ç¢ºå®š Agg Mode
        agg_mode = "B" 
        if u_agg:
            p_clean = str(u_agg).upper().replace(" ", "")
            if "EXEMPT" in p_clean or "SKIP" in p_clean: agg_mode = "EXEMPT"
            elif "AB" in p_clean: agg_mode = "AB"
            elif "A" in p_clean: agg_mode = "A"

        agg_multiplier = parse_ratio(u_agg)
        qty_agg = batch_qty if batch_qty > 0 else actual_item_qty * agg_multiplier

        if agg_mode != "EXEMPT":
            for s_title, data in global_sum_tracker.items():
                s_clean = clean_text(s_title)
                
                if (fuzz.partial_ratio("è¼¥è¼ªæ‹†è£.è»Šä¿®æˆ–éŠ²è£œé‹è²»", s_clean) > 70) or ("é‹è²»" in s_clean):
                    if freight_val > 0:
                        data["actual"] += freight_val
                        data["details"].append({"page": page, "title": raw_title, "val": freight_val, "note": f"é‹è²» {f_note}"})
                    continue

                # =========================================================
                # ğŸ§º æ­¥é©Ÿ 1: ç±ƒå­æ’ˆäºº (v52)
                # =========================================================
                match_A = (fuzz.partial_ratio(s_clean, title_clean) > 90)
                match_B = False
                
                s_upper_check = s_clean.upper() 

                is_dis = fuzz.partial_ratio("ROLLæ‹†è£", s_upper_check) > 80
                is_mac = fuzz.partial_ratio("ROLLè»Šä¿®", s_upper_check) > 80
                is_weld = (fuzz.partial_ratio("ROLLéŠ²è£œ", s_upper_check) > 80) or \
                          ("ç„Š" in s_upper_check) or \
                          ("é‰€" in s_upper_check)
                
                has_part_body = "æœ¬é«”" in title_clean
                has_part_journal = any(k in title_clean for k in journal_family)
                
                # ç™½åå–®é‚„åŸ: åªä¿ç•™åš´æ ¼å‹•ä½œ
                has_act_mac = any(k in title_clean for k in ["å†ç”Ÿ", "ç²¾è»Š", "æœªå†ç”Ÿ", "ç²—è»Š"])
                
                has_act_weld = ("éŠ²è£œ" in title_clean or "ç„Š" in title_clean or "é‰€" in title_clean)
                is_assy = ("çµ„è£" in title_clean or "æ‹†è£" in title_clean)
                
                if is_dis and is_assy: match_B = True
                elif is_mac and (has_part_body or has_part_journal) and has_act_mac: match_B = True
                elif is_weld and (has_part_body or has_part_journal) and has_act_weld: match_B = True
                
                if agg_mode == "A": match = match_A
                elif agg_mode == "AB": match = match_A or match_B
                else: match = match_B if match_B else match_A

                # =========================================================
                # ğŸ›‘ æ­¥é©Ÿ 2: æ””æˆªè€…
                # =========================================================
                if match:
                    s_upper = s_clean.upper()
                    t_upper = title_clean.upper()
                    
                    s_is_unregen = "æœªå†ç”Ÿ" in s_clean or "ç²—è»Š" in s_clean
                    t_is_unregen = "æœªå†ç”Ÿ" in title_clean or "ç²—è»Š" in title_clean
                    s_is_regen = ("å†ç”Ÿ" in s_clean or "ç²¾è»Š" in s_clean) and not s_is_unregen
                    t_is_regen = ("å†ç”Ÿ" in title_clean or "ç²¾è»Š" in title_clean or "è»Šä¿®" in title_clean) and not t_is_unregen
                    
                    s_is_body = "æœ¬é«”" in s_clean
                    t_is_body = "æœ¬é«”" in title_clean
                    s_is_journal = any(k in s_clean for k in journal_family)
                    t_is_journal = any(k in title_clean for k in journal_family)

                    if s_is_regen and t_is_unregen: match = False
                    if s_is_unregen and t_is_regen: match = False
                    if s_is_body and not s_is_journal and t_is_journal: match = False
                    if s_is_journal and not s_is_body and t_is_body: match = False
                    if "TOP" in s_upper and "BOTTOM" in t_upper: match = False
                    if "BOTTOM" in s_upper and "TOP" in t_upper: match = False

                if match:
                    data["actual"] += qty_agg
                    c_msg = f"x{agg_multiplier}" if agg_multiplier != 1.0 else ""
                    data["details"].append({"page": page, "title": raw_title, "val": qty_agg, "note": c_msg})

    # =================================================
    # ğŸ•µï¸â€â™‚ï¸ ç¬¬ä¸‰é—œï¼šæ˜ç´°ç¸½çµç®—
    # =================================================
    for s_title, data in global_sum_tracker.items():
        if abs(data["actual"] - data["target"]) > 0.01: 
            fail_table = []
            fail_table.append({"é ç¢¼": "ç¸½è¡¨", "é …ç›®åç¨±": f"ğŸ¯ ç›®æ¨™ (å¯¦äº¤)", "æ•¸é‡": data["target"], "å‚™è¨»": "åŸºæº–"})
            for d in data["details"]:
                fail_table.append({"é ç¢¼": f"P.{d['page']}", "é …ç›®åç¨±": d['title'], "æ•¸é‡": d['val'], "å‚™è¨»": d['note']})
            fail_table.append({"é ç¢¼": "âˆ‘", "é …ç›®åç¨±": "åŠ ç¸½çµæœ", "æ•¸é‡": data["actual"], "å‚™è¨»": "ç¸½è¨ˆ"})

            accounting_issues.append({
                "page": data["page"], "item": s_title, 
                "issue_type": "ğŸ›‘ æ˜ç´°åŒ¯ç¸½ä¸ç¬¦", 
                "common_reason": f"å¯¦äº¤({data['target']}) != æ˜ç´°åŠ ç¸½({data['actual']})", 
                "failures": fail_table, "source": "ğŸ æœƒè¨ˆå¼•æ“"
            })
            
    # ğŸ”¥ğŸ”¥ğŸ”¥ [é—œéµ]: å°‡å‘½ä¸­è³‡æ–™ç•¶ä½œä¸€å€‹éš±è—çš„ ISSUE å›å‚³ (TYPE=HIDDEN_DATA)
    if rule_hits_log:
        accounting_issues.append({
            "issue_type": "HIDDEN_DATA",
            "rule_hits": rule_hits_log,
            "fuzz_threshold": CURRENT_THRESHOLD # ğŸ”¥ é¡¯ç¤ºç›®å‰å¯¦éš›ä½¿ç”¨çš„é–€æª»
        })
            
    return accounting_issues

def python_process_audit(dimension_data):
    """
    Python æµç¨‹å¼•æ“ (v24: å…¨åŸŸçµ±ä¸€ç‰¹è¦ç‰ˆ)
    å‡ç´šå…§å®¹ï¼š
    1. [çµ±ä¸€é…å°]: å¼•å…¥èˆ‡æœƒè¨ˆ/å·¥ç¨‹åŒç´šçš„é…å°é‚è¼¯ (GLOBAL_FUZZ_THRESHOLD + fuzz.ratio)ã€‚
       - å¾¹åº•è§£æ±º "è¦å‰‡åŠ«æŒ" å°è‡´çš„éŒ¯èª¤å·¥åºåˆ¤å®šã€‚
    2. [è¦å‰‡å„ªå…ˆ]: è‹¥ Excel ç‰¹è¦é…å°æˆåŠŸä¸”è¨­å®šç‚º SKIP/EXEMPTï¼Œç›´æ¥è·³éæª¢æŸ¥ã€‚
    3. [æ—¢æœ‰åŠŸèƒ½]: ä¿ç•™ç†±è™•ç†/å‹•å¹³è¡¡é—œéµå­—æ’é™¤ã€å·¥åºæº¯æºã€å°ºå¯¸é‚è¼¯ã€‚
    """
    process_issues = []
    import re
    import pandas as pd
    from thefuzz import fuzz

    # ğŸ”¥ 1. è®€å–å…¨åŸŸé–€æª» (èˆ‡æœƒè¨ˆ/å·¥ç¨‹åŒæ­¥)
    CURRENT_THRESHOLD = globals().get('GLOBAL_FUZZ_THRESHOLD', 95)

    def clean_text(text):
        return str(text).replace(" ", "").replace("\n", "").replace("\r", "").replace('"', '').replace("'", "").strip()

    # 2. è¼‰å…¥è¦å‰‡
    rules_map = {}
    try:
        df = pd.read_excel("rules.xlsx")
        df.columns = [c.strip() for c in df.columns]
        for _, row in df.iterrows():
            iname = str(row.get('Item_Name', '')).strip()
            p_rule = str(row.get('Process_Rule', '')).strip()
            # æµç¨‹å¼•æ“ä¸»è¦çœ‹ Process_Rule
            if iname and p_rule and p_rule.lower() != 'nan':
                rules_map[clean_text(iname)] = p_rule
    except: pass

    STAGE_MAP = { 1: "æœªå†ç”Ÿ/ç²—è»Š", 2: "éŠ²è£œ/ç„Šè£œ", 3: "å†ç”Ÿ/ç²¾è»Š", 4: "ç ”ç£¨" }
    history = {} 

    if not dimension_data: return []

    for item in dimension_data:
        p_num = item.get("page", "?")
        title = str(item.get("item_title", "")).strip()
        title_clean = clean_text(title)
        ds = str(item.get("ds", ""))
        
        # âš¡ï¸ [æ—¢æœ‰è±å…] å‹•å¹³è¡¡ã€ç†±è™•ç†ç›´æ¥è·³éæµç¨‹æª¢æŸ¥ (é—œéµå­—å„ªå…ˆ)
        t_upper = title_clean.upper()
        if any(k in t_upper for k in ["å‹•å¹³è¡¡", "BALANCING", "ç†±è™•ç†", "HEAT"]):
            continue

        # =========================================================
        # ğŸ”¥ 3. åŸ·è¡Œç‰¹è¦é…å° (çµ±ä¸€é‚è¼¯)
        # =========================================================
        forced_rule = None
        
        # A. å®Œå…¨åŒ¹é…
        if title_clean in rules_map:
            forced_rule = rules_map[title_clean]
        
        # B. å»æ‹¬è™ŸåŒ¹é…
        if not forced_rule:
            t_no = re.sub(r"[\(ï¼ˆ].*?[\)ï¼‰]", "", title_clean)
            if t_no in rules_map:
                forced_rule = rules_map[t_no]

        # C. æ¨¡ç³ŠåŒ¹é… (ä½¿ç”¨å…¨åŸŸé–€æª» + åš´æ ¼æ¯”å°)
        if not forced_rule and rules_map:
            best_score = 0
            for k, v in rules_map.items():
                sc = fuzz.token_sort_ratio(k, title_clean) # åš´æ ¼æ¯”å° (åŸç‚º partial_ratio)
                if sc > CURRENT_THRESHOLD and sc > best_score:
                    best_score = sc
                    forced_rule = v
        # =========================================================

        track = "Unknown"
        stage = 0
        
        # å¦‚æœé…å°åˆ°è¦å‰‡ï¼Œè§£æè¦å‰‡å…§å®¹
        if forced_rule:
            fr = forced_rule.upper()
            # âš¡ï¸ [è¦å‰‡è±å…] å¦‚æœè¦å‰‡èªª SKIPï¼Œè·³é
            if "è±å…" in fr or "EXEMPT" in fr or "SKIP" in fr: 
                continue 
            
            if "æœ¬é«”" in fr: track = "æœ¬é«”"
            elif "è»¸é ¸" in fr or "è»¸é ­" in fr or "è»¸ä½" in fr: track = "è»¸é ¸"
            
            if "æœªå†ç”Ÿ" in fr or "ç²—è»Š" in fr: stage = 1
            elif "éŠ²" in fr or "ç„Š" in fr or "é‰€" in fr: stage = 2
            elif "å†ç”Ÿ" in fr or "ç²¾è»Š" in fr: stage = 3
            elif "ç ”ç£¨" in fr: stage = 4

        # å¦‚æœè¦å‰‡æ²’æŒ‡å®š(æˆ–æ²’é…åˆ°)ï¼Œä½¿ç”¨é è¨­é—œéµå­—åˆ¤æ–·
        if stage == 0:
            if "ç ”ç£¨" in title: stage = 4
            elif any(k in title for k in ["éŠ²è£œ", "éŠ²æ¥", "ç„Š", "é‰€"]): stage = 2
            elif "æœªå†ç”Ÿ" in title or "ç²—è»Š" in title: stage = 1
            elif "å†ç”Ÿ" in title or "ç²¾è»Š" in title: stage = 3

        if track == "Unknown":
            if "æœ¬é«”" in title: track = "æœ¬é«”"
            elif any(k in title for k in ["è»¸é ¸", "è»¸é ­", "è»¸ä½", "å…§å­”", "JOURNAL"]): track = "è»¸é ¸"
        
        if track == "Unknown" or stage == 0: continue 

        # --- ä»¥ä¸‹ç‚ºæ•¸å€¼æ”¶é›†é‚è¼¯ (ä¿æŒä¸è®Š) ---
        segments = ds.split("|")
        for seg in segments:
            parts = seg.split(":")
            if len(parts) < 2: continue
            rid = parts[0].strip().upper()
            val_str = parts[1].strip()
            nums = re.findall(r"\d+\.?\d*", val_str)
            if not nums: continue
            val = float(nums[0])
            
            key = (rid, track)
            if key not in history: history[key] = {}
            history[key][stage] = {
                "val": val, "page": p_num, "title": title
            }

    # --- ä»¥ä¸‹ç‚ºæª¢æŸ¥é‚è¼¯ (ç¼ºæ¼å·¥åº + å°ºå¯¸å€’ç½®) ä¿æŒä¸è®Š ---
    for (rid, track), stages_data in history.items():
        present_stages = sorted(stages_data.keys())
        if not present_stages: continue
        max_stage = present_stages[-1]
        
        missing_stages = []
        for req_s in range(1, max_stage):
            if req_s not in stages_data: missing_stages.append(STAGE_MAP[req_s])
        
        if missing_stages:
            last_info = stages_data[max_stage]
            process_issues.append({
                "page": last_info['page'],
                "item": f"{last_info['title']}",
                "issue_type": "ğŸ›‘æº¯æºç•°å¸¸(ç¼ºæ¼å·¥åº)",
                "common_reason": f"[{track}] é€²åº¦è‡³ã€{STAGE_MAP[max_stage]}ã€‘ï¼Œç¼ºå‰ç½®ï¼š{', '.join(missing_stages)}",
                "failures": [{"id": rid, "val": "ç¼ºæ¼", "calc": "å±¥æ­·ä¸å®Œæ•´"}],
                "source": "ğŸ æµç¨‹å¼•æ“"
            })

        size_rank = { 1: 10, 4: 20, 3: 30, 2: 40 }
        for i in range(len(present_stages)):
            for j in range(i + 1, len(present_stages)):
                s_a = present_stages[i]
                s_b = present_stages[j]
                info_a = stages_data[s_a]
                info_b = stages_data[s_b]
                
                expect_a_smaller = size_rank[s_a] < size_rank[s_b]
                is_violation = False
                if expect_a_smaller:
                    if info_a['val'] >= info_b['val']: is_violation = True
                else:
                    if info_a['val'] <= info_b['val']: is_violation = True
                    
                if is_violation:
                    sign = "<" if expect_a_smaller else ">"
                    process_issues.append({
                        "page": info_b['page'],
                        "item": f"[{track}] å°ºå¯¸é‚è¼¯",
                        "issue_type": "ğŸ›‘æµç¨‹ç•°å¸¸(å°ºå¯¸å€’ç½®)",
                        "common_reason": f"å°ºå¯¸é‚è¼¯éŒ¯èª¤ï¼š{STAGE_MAP[s_a]} æ‡‰ {sign} {STAGE_MAP[s_b]}",
                        "failures": [{"id": STAGE_MAP[s_a], "val": info_a['val'], "calc": "å‰"}, {"id": STAGE_MAP[s_b], "val": info_b['val'], "calc": "å¾Œ"}],
                        "source": "ğŸ æµç¨‹å¼•æ“"
                    })

    return process_issues
    
def python_header_audit_batch(photo_gallery, ai_res_json):
    """
    Python è¡¨é ­ç¨½æ ¸å®˜ (Batch æ¶æ§‹é©é…ç‰ˆ v30)
    1. [Raw Text] æƒææ¯ä¸€é  OCR æ–‡å­—ï¼Œæª¢æŸ¥å·¥ä»¤æ˜¯å¦æ··å–® (Regex)ã€‚
    2. [AI JSON] æª¢æŸ¥ AI è®€å‡ºçš„å·¥ä»¤æ ¼å¼ (10ç¢¼)ã€‚
    3. [AI JSON] æª¢æŸ¥æ—¥æœŸé‚è¼¯ (å¯¦éš› <= é å®š)ã€‚
    """
    header_issues = []
    import re
    from datetime import datetime

    # --- 1. æ··å–®æª¢æŸ¥ (åˆ©ç”¨ OCR åŸå§‹æ–‡å­—) ---
    # ç­–ç•¥ï¼šç›´æ¥ç”¨ Regex åœ¨æ¯ä¸€é çš„æ–‡å­—è£¡æ’ˆ W/R/O/Y é–‹é ­çš„å­—ä¸²
    job_pattern = r"([WROY][A-Z0-9]{9})" # æŠ“ 10 ç¢¼
    found_jobs_map = {} # { "å·¥ä»¤è™Ÿ": [é ç¢¼list] }

    for idx, item in enumerate(photo_gallery):
        txt = item.get('full_text', '').upper().replace(" ", "").replace("-", "")
        # å°‹æ‰¾æ‰€æœ‰ç–‘ä¼¼å·¥ä»¤çš„å­—ä¸²
        matches = re.findall(job_pattern, txt)
        for job in matches:
            if job not in found_jobs_map: found_jobs_map[job] = []
            found_jobs_map[job].append(idx + 1)

    # å¦‚æœæ‰¾åˆ°å¤šç¨®ä¸åŒçš„å·¥ä»¤ -> å ±è­¦
    if len(found_jobs_map) > 1:
        details = [f"{k} (P.{v})" for k, v in found_jobs_map.items()]
        header_issues.append({
            "page": "å¤šé ", "item": "å·¥ä»¤å–®è™Ÿ", "issue_type": "ğŸš¨ åš´é‡æ··å–®",
            "common_reason": f"åµæ¸¬åˆ°å¤šç¨®å·¥ä»¤ï¼š{', '.join(details)}",
            "failures": [{"id": "å…§å®¹", "val": str(found_jobs_map)}],
            "source": "ğŸ è¡¨é ­ç¨½æ ¸(OCR)"
        })

    # --- 2. æ ¼å¼èˆ‡æ—¥æœŸæª¢æŸ¥ (åˆ©ç”¨ AI JSON) ---
    h_info = ai_res_json.get("header_info", {})
    
    # å·¥ä»¤æ ¼å¼ (é‡å° AI æœ€çµ‚èªå®šçš„é‚£ä¸€çµ„)
    ai_job = h_info.get("job_no", "Unknown")
    if ai_job and ai_job != "Unknown":
        clean_job = ai_job.upper().replace(" ", "").replace("-", "")
        if not re.match(r"^[WROY][A-Z0-9]{9}$", clean_job):
            header_issues.append({
                "page": "è¡¨é ­", "item": "å·¥ä»¤æ ¼å¼", "issue_type": "âš ï¸ æ ¼å¼éŒ¯èª¤",
                "common_reason": f"AI è­˜åˆ¥å·¥ä»¤ {ai_job} æ ¼å¼ä¸ç¬¦ (éœ€10ç¢¼ï¼ŒW/R/O/Yé–‹é ­)",
                "failures": [{"id": "è­˜åˆ¥å€¼", "val": ai_job}],
                "source": "ğŸ è¡¨é ­ç¨½æ ¸(AI)"
            })

    # æ—¥æœŸé‚è¼¯ (å¯¦éš› <= é å®š)
    d_sch = h_info.get("scheduled_date", "Unknown")
    d_act = h_info.get("actual_date", "Unknown")
    
    if d_sch != "Unknown" and d_act != "Unknown":
        try:
            # å˜—è©¦è§£æ YYYY/MM/DD
            dt_sch = datetime.strptime(d_sch.replace("-", "/"), "%Y/%m/%d")
            dt_act = datetime.strptime(d_act.replace("-", "/"), "%Y/%m/%d")
            
            if dt_act > dt_sch:
                 header_issues.append({
                    "page": "è¡¨é ­", "item": "äº¤è²¨æ™‚æ•ˆ", "issue_type": "â° é€¾æœŸäº¤è²¨",
                    "common_reason": f"å¯¦éš› {d_act} æ™šæ–¼ é å®š {d_sch}",
                    "failures": [{"id": "å»¶é²å¤©æ•¸", "val": f"{(dt_act - dt_sch).days} å¤©"}], 
                    "source": "ğŸ è¡¨é ­ç¨½æ ¸(AI)"
                })
        except:
            pass # æ—¥æœŸæ ¼å¼è®€ä¸æ‡‚ï¼Œè·³é

    return header_issues
    
# --- 6. æ‰‹æ©Ÿç‰ˆ UI èˆ‡ æ ¸å¿ƒåŸ·è¡Œé‚è¼¯ ---
st.title("ğŸ­ äº¤è²¨å–®ç¨½æ ¸")

data_source = st.radio(
    "è«‹é¸æ“‡è³‡æ–™ä¾†æºï¼š", 
    ["ğŸ“¸ ä¸Šå‚³ç…§ç‰‡", "ğŸ“‚ ä¸Šå‚³ JSON æª”", "ğŸ“Š ä¸Šå‚³ Excel æª”"], 
    horizontal=True
)

with st.container(border=True):
    # --- æƒ…æ³ A: ä¸Šå‚³ç…§ç‰‡ ---
    if data_source == "ğŸ“¸ ä¸Šå‚³ç…§ç‰‡":
        if st.session_state.get('source_mode') == 'json' or st.session_state.get('source_mode') == 'excel':
            st.session_state.photo_gallery = []
            st.session_state.source_mode = 'image'

        uploaded_files = st.file_uploader(
            "è«‹é¸æ“‡ JPG/PNG ç…§ç‰‡...", 
            type=['jpg', 'png', 'jpeg'], 
            accept_multiple_files=True, 
            key=f"uploader_{st.session_state.uploader_key}"
        )
        
        if uploaded_files:
            for f in uploaded_files: 
                if not any(x['file'].name == f.name for x in st.session_state.photo_gallery if x['file']):
                    st.session_state.photo_gallery.append({
                        'file': f, 
                        'table_md': None, 
                        'header_text': None,
                        'full_text': None,
                        'raw_json': None
                    })
            st.session_state.uploader_key += 1
            if st.session_state.enable_auto_analysis:
                st.session_state.auto_start_analysis = True
            components.html("""<script>window.parent.document.body.scrollTo(0, window.parent.document.body.scrollHeight);</script>""", height=0)
            st.rerun()

    # --- æƒ…æ³ B: ä¸Šå‚³ JSON ---
    elif data_source == "ğŸ“‚ ä¸Šå‚³ JSON æª”":
        st.info("ğŸ’¡ è«‹é»æ“Šä¸‹æ–¹æŒ‰éˆ•ï¼Œå¾ä½ çš„è³‡æ–™å¤¾é¸æ“‡ä¹‹å‰ä¸‹è¼‰çš„ `.json` æª”ã€‚")
        uploaded_json = st.file_uploader("ä¸Šå‚³JSONæª”", type=['json'], key="json_uploader")
        
        if uploaded_json:
            try:
                current_file_name = uploaded_json.name
                if st.session_state.get('last_loaded_json_name') != current_file_name:
                    json_data = json.load(uploaded_json)
                    st.session_state.photo_gallery = []
                    st.session_state.source_mode = 'json'
                    st.session_state.last_loaded_json_name = current_file_name
                    
                    import re
                    for page in json_data:
                        real_page = "Unknown"
                        full_text = page.get('full_text', '')
                        if full_text:
                            match = re.search(r"(?:é …æ¬¡|Page|é æ¬¡|NO\.)[:\s]*(\d+)\s*[/ï¼]\s*\d+", full_text, re.IGNORECASE)
                            if match:
                                real_page = match.group(1)
                        
                        st.session_state.photo_gallery.append({
                            'file': None,
                            'table_md': page.get('table_md'),
                            'header_text': page.get('header_text'),
                            'full_text': full_text,
                            'raw_json': page.get('raw_json'),
                            'real_page': real_page
                        })
                    
                    st.toast(f"âœ… æˆåŠŸè¼‰å…¥ JSON: {current_file_name}", icon="ğŸ“‚")
                    if st.session_state.enable_auto_analysis:
                        st.session_state.auto_start_analysis = True
                    st.rerun()
                else:
                    st.success(f"ğŸ“‚ ç›®å‰è¼‰å…¥ JSONï¼š**{uploaded_json.name}**")
            except Exception as e:
                st.error(f"JSON æª”æ¡ˆæ ¼å¼éŒ¯èª¤: {e}")

        # --- æƒ…æ³ C: ä¸Šå‚³ Excel (æ–°å¢çš„æ”¾åœ¨é€™) ---
    elif data_source == "ğŸ“Š ä¸Šå‚³ Excel æª”":
        st.info("ğŸ’¡ ä¸Šå‚³ Excel æª”å¾Œï¼Œç³»çµ±æœƒå°‡è¡¨æ ¼å…§å®¹è½‰æ›ç‚ºæ–‡å­—ä¾› AI ç¨½æ ¸ã€‚")
        # é€™è£¡è¨˜å¾—ç¶­æŒæˆ‘å€‘ä¸Šæ¬¡æ”¹çš„ xlsm æ”¯æ´
        uploaded_xlsx = st.file_uploader("ä¸Šå‚³ Excel æª”", type=['xlsx', 'xls', 'xlsm'], key="xlsx_uploader")
        
        if uploaded_xlsx:
            try:
                current_file_name = uploaded_xlsx.name
                if st.session_state.get('last_loaded_xlsx_name') != current_file_name:
                    # 1. è®€å– Excel (header=None ä¿æŒä¸è®Š)
                    df_dict = pd.read_excel(uploaded_xlsx, sheet_name=None, header=None)
                    
                    st.session_state.photo_gallery = []
                    st.session_state.source_mode = 'excel'
                    st.session_state.last_loaded_xlsx_name = current_file_name
                    
                    for sheet_name, df in df_dict.items():
                        df = df.fillna("")
                        
                        # ğŸ”¥ğŸ”¥ğŸ”¥ [æ–°å¢é€™æ®µï¼šæš´åŠ›å£“å¹³æ›è¡Œç¬¦è™Ÿ] ğŸ”¥ğŸ”¥ğŸ”¥
                        # é€™è¡ŒæŒ‡ä»¤æœƒæŠŠæ‰€æœ‰æ ¼å­è£¡çš„ "\n" (æ›è¡Œ) æ›¿æ›æˆ " " (ç©ºæ ¼)
                        # é€™æ¨£ "W3...\næœ¬é«”..." å°±æœƒè®Šæˆ "W3... æœ¬é«”..." (åŒä¸€è¡Œ)
                        df = df.astype(str).replace(r'\n', ' ', regex=True).replace(r'\r', ' ', regex=True)
                        
                        md_table = df.to_markdown(index=False)
                        st.session_state.photo_gallery.append({
                            'file': None,
                            'table_md': md_table,
                            'header_text': f"ä¾†æºåˆ†é : {sheet_name}",
                            'full_text': f"Excel å…§å®¹ - åˆ†é  {sheet_name}\n" + md_table,
                            'raw_json': None,
                            'real_page': sheet_name
                        })
                    st.toast(f"âœ… æˆåŠŸè¼‰å…¥ Excel: {current_file_name}", icon="ğŸ“Š")
                    if st.session_state.enable_auto_analysis:
                        st.session_state.auto_start_analysis = True
                    st.rerun()
                else:
                    st.success(f"ğŸ“Š ç›®å‰è¼‰å…¥ Excelï¼š**{uploaded_xlsx.name}**")
            except Exception as e:
                st.error(f"Excel è®€å–å¤±æ•—: {e}")

if st.session_state.photo_gallery:
    st.caption(f"å·²ç´¯ç© {len(st.session_state.photo_gallery)} é æ–‡ä»¶")
    col_btn1, col_btn2 = st.columns([1, 1], gap="small")
    with col_btn1: start_btn = st.button("ğŸš€ é–‹å§‹åˆ†æ", type="primary", use_container_width=True)
    with col_btn2: 
        clear_btn = st.button("ğŸ—‘ï¸ç…§ç‰‡æ¸…é™¤", help="æ¸…é™¤", use_container_width=True)

    if clear_btn:
        st.session_state.photo_gallery = []
        st.session_state.analysis_result_cache = None
        if 'last_loaded_json_name' in st.session_state:
            del st.session_state.last_loaded_json_name 
        st.rerun()

    is_auto_start = st.session_state.auto_start_analysis
    if is_auto_start:
        st.session_state.auto_start_analysis = False

    if 'analysis_result_cache' not in st.session_state:
        st.session_state.analysis_result_cache = None

    trigger_analysis = start_btn or is_auto_start

    if trigger_analysis:
        # å¼·åˆ¶æ¸…é™¤ä¸Šä¸€ç­†
        st.session_state.analysis_result_cache = None 
        st.session_state.auto_start_analysis = False
        total_start = time.time()
        
        with st.status("ç¸½ç¨½æ ¸å®˜æ­£åœ¨é€²è¡Œå…¨æ–¹ä½åˆ†æ...", expanded=True) as status_box:
            progress_bar = st.progress(0)
            
            # 1. OCR
            status_box.write("ğŸ‘€ æ­£åœ¨é€²è¡Œ OCR æ–‡å­—è­˜åˆ¥...")
            ocr_start = time.time()
            
            def process_task(index, item):
                if item.get('full_text'): return index, item.get('header_text',''), item['full_text'], None
                try:
                    item['file'].seek(0)
                    _, h, f, _, _ = extract_layout_with_azure(item['file'], DOC_ENDPOINT, DOC_KEY)
                    return index, h, f, None
                except Exception as e: return index, None, None, str(e)

            with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
                futures = [executor.submit(process_task, i, item) for i, item in enumerate(st.session_state.photo_gallery)]
                for future in concurrent.futures.as_completed(futures):
                    idx, h_txt, f_txt, err = future.result()
                    if not err:
                        st.session_state.photo_gallery[idx].update({'header_text': h_txt, 'full_text': f_txt, 'file': None})
                    progress_bar.progress(0.4 * ((idx + 1) / len(st.session_state.photo_gallery)))

            ocr_duration = time.time() - ocr_start
            
            # 2. çµ„åˆæ–‡å­—
            combined_input = ""
            for i, p in enumerate(st.session_state.photo_gallery):
                combined_input += f"\n=== Page {i+1} ===\n{p.get('full_text','')}\n"

                        # ... (ä¸Šé¢æ˜¯ 2. çµ„åˆæ–‡å­— combined_inputï¼Œä¸ç”¨å‹•) ...

            # 3. AI åˆ†æ (åŠ å…¥è¨ˆæ™‚)
            status_box.write("ğŸ¤– AI æ­£åœ¨å…¨å·åˆ†æ...")
            
            ai_start_time = time.time()  # â±ï¸ [è¨ˆæ™‚é–‹å§‹] AI
            res_main = agent_unified_check(combined_input, combined_input, GEMINI_KEY, main_model_name)
            ai_duration = time.time() - ai_start_time # â±ï¸ [è¨ˆæ™‚çµæŸ] AI
            
            progress_bar.progress(0.8)
            
            # 4. Python é‚è¼¯æª¢æŸ¥ (åŠ å…¥è¨ˆæ™‚)
            status_box.write("ğŸ Python æ­£åœ¨é€²è¡Œé‚è¼¯æ¯”å°...")
            
            py_start_time = time.time() # â±ï¸ [è¨ˆæ™‚é–‹å§‹] Python
            
            dim_data = res_main.get("dimension_data", [])
            for item in dim_data:
                new_cat = assign_category_by_python(item.get("item_title", ""))
                item["category"] = new_cat
                if "sl" not in item: item["sl"] = {}
                item["sl"]["lt"] = new_cat
            
            python_numeric_issues = python_numerical_audit(dim_data)
            python_accounting_issues = python_accounting_audit(dim_data, res_main)
            python_process_issues = python_process_audit(dim_data)
            python_header_issues = python_header_audit_batch(st.session_state.photo_gallery, res_main)

            ai_filtered_issues = []
            ai_raw_issues = res_main.get("issues", [])
            if isinstance(ai_raw_issues, list):
                for i in ai_raw_issues:
                    if isinstance(i, dict):
                        i['source'] = 'ğŸ¤– ç¸½ç¨½æ ¸ AI'
                        if not any(k in i.get("issue_type", "") for k in ["æµç¨‹", "è¦æ ¼æå–å¤±æ•—", "æœªåŒ¹é…"]):
                            ai_filtered_issues.append(i)

            all_issues = ai_filtered_issues + python_numeric_issues + python_accounting_issues + python_process_issues + python_header_issues
            
            py_duration = time.time() - py_start_time # â±ï¸ [è¨ˆæ™‚çµæŸ] Python

            # 5. å­˜æª” (Cache)
            usage = res_main.get("_token_usage", {"input": 0, "output": 0})
            
            # ä¿®æ­£å·¥ä»¤è®€å–é‚è¼¯
            final_job_no = res_main.get("header_info", {}).get("job_no")
            if not final_job_no or final_job_no == "Unknown":
                 final_job_no = res_main.get("job_no", "Unknown")
            
            st.session_state.analysis_result_cache = {
                "job_no": final_job_no,
                "header_info": res_main.get("header_info", {}),
                "all_issues": all_issues,
                "total_duration": time.time() - total_start,
                "ocr_duration": ocr_duration,
                "ai_duration": ai_duration,     # AI è€—æ™‚
                "py_duration": py_duration,     # Python è€—æ™‚
                
                "cost_twd": (usage.get("input", 0)*0.3 + usage.get("output", 0)*2.5) / 1000000 * 32.5,
                "total_in": usage.get("input", 0),
                "total_out": usage.get("output", 0),
                
                "ai_extracted_data": dim_data,
                "freight_target": res_main.get("freight_target", 0),
                "summary_rows": res_main.get("summary_rows", []),
                "full_text_for_search": combined_input,
                "combined_input": combined_input
            }
            
            progress_bar.progress(1.0)
            status_box.update(label="âœ… åˆ†æå®Œæˆï¼", state="complete", expanded=False)
            st.rerun()

       # --- ğŸ’¡ é¡¯ç¤ºçµæœå€å¡Š ---
    if st.session_state.analysis_result_cache:
        cache = st.session_state.analysis_result_cache
        all_issues = cache.get('all_issues', [])

        # --- ğŸ“‹ è¡¨é ­è³‡è¨Šåµæ¸¬ (æ‰‹æ©Ÿç‰ˆå¼·è£½æ©«æ’å„ªåŒ–) ---
        st.divider()
        st.subheader("ğŸ“‹ è¡¨é ­è³‡è¨Šåµæ¸¬")
        
        h_info = cache.get("header_info", {}) 
        current_job = h_info.get("job_no", "æœªåµæ¸¬")
        sch_date = h_info.get("scheduled_date", "æœªåµæ¸¬")
        act_date = h_info.get("actual_date", "æœªåµæ¸¬")

        # 1. å…ˆè™•ç†ç´…è‰²è­¦ç¤ºçš„ HTML æ¨£å¼å­—ä¸²
        act_date_html = f"<b>{act_date}</b>"
        try:
            if act_date != "æœªåµæ¸¬" and sch_date != "æœªåµæ¸¬" and act_date > sch_date:
                # å¦‚æœé€¾æœŸï¼Œè®Šç´…è‰² (#ff4b4b æ˜¯ Streamlit çš„æ¨™æº–ç´…)
                act_date_html = f"<b style='color: #ff4b4b;'>{act_date} (é€¾æœŸ)</b>"
        except: pass

        # 2. ä½¿ç”¨ HTML Flexbox å¼·åˆ¶æ©«å‘æ’åˆ—
        st.markdown(f"""
        <div style="display: flex; flex-direction: row; justify-content: space-between; width: 100%;">
            <div style="flex: 1; padding-right: 5px;">
                <div style="font-size: 12px; color: gray; margin-bottom: 2px;">å·¥ä»¤å–®è™Ÿ</div>
                <div style="font-size: 16px; font-weight: bold;">{current_job}</div>
            </div>
            <div style="flex: 1; padding-right: 5px;">
                <div style="font-size: 12px; color: gray; margin-bottom: 2px;">é å®šäº¤è²¨æ—¥</div>
                <div style="font-size: 16px; font-weight: bold;">{sch_date}</div>
            </div>
            <div style="flex: 1;">
                <div style="font-size: 12px; color: gray; margin-bottom: 2px;">å¯¦éš›äº¤è²¨æ—¥</div>
                <div style="font-size: 16px;">{act_date_html}</div>
            </div>
        </div>
        """, unsafe_allow_html=True)
        
        st.divider()

        # 3. é ‚éƒ¨ç‹€æ…‹æ¢ (ä¿®æ”¹ç‰ˆï¼šè©³ç´°æ™‚é–“æ‹†è§£)
        # æ ¼å¼ï¼šç¸½è€—æ™‚ (OCR | AI | Python)
        total_t = cache.get('total_duration', 0)
        ocr_t = cache.get('ocr_duration', 0)
        ai_t = cache.get('ai_duration', 0)
        py_t = cache.get('py_duration', 0)
        
        st.success(
            f"ç¸½è€—æ™‚: {total_t:.1f}s  "
            f"( OCR: {ocr_t:.1f}s | AI: {ai_t:.1f}s | Py: {py_t:.2f}s )"
        )
        
        st.info(f"ğŸ’° æœ¬æ¬¡æˆæœ¬: NT$ {cache['cost_twd']:.2f} (In: {cache['total_in']:,} / Out: {cache['total_out']:,})")
        
        # 4. è¦å‰‡å±•ç¤º (v58: å®Œæ•´æ¬„ä½å…­å®®æ ¼ç‰ˆ)
        with st.expander("ğŸ—ï¸ æª¢è¦– Excel é‚è¼¯èˆ‡è¦å‰‡åƒæ•¸", expanded=False):
            
            # 1. ä¿®æ­£è³‡æ–™æºï¼šæ”¹è®€ analysis_result_cache
            target_list = []
            if st.session_state.analysis_result_cache:
                target_list = st.session_state.analysis_result_cache.get('all_issues', [])
            
            # 2. æ‰¾å‡ºéš±è—åŒ…è£¹ (HIDDEN_DATA)
            hidden_payload = {}
            for item in target_list:
                if item.get('issue_type') == 'HIDDEN_DATA':
                    hidden_payload = item
                    break
            
            # 3. è§£æè³‡æ–™
            rule_hits = hidden_payload.get('rule_hits', {})
            current_fuzz = globals().get('GLOBAL_FUZZ_THRESHOLD', hidden_payload.get('fuzz_threshold', 90))

            st.caption(f"â„¹ï¸ å…¨åŸŸçµ±ä¸€ç‰¹è¦é–€æª»: **{current_fuzz} åˆ†**")
            
            try:
                # å˜—è©¦è®€å– Excel æª”æ¡ˆ
                df_rules = pd.read_excel("rules.xlsx")
                df_rules.columns = [c.strip() for c in df_rules.columns]
                
                # å»ºç«‹å¿«é€ŸæŸ¥è©¢è¡¨
                rule_info_map = {}
                rules_map_for_xray = {} 
                
                for _, row in df_rules.iterrows():
                    r_name = str(row.get('Item_Name', '')).strip()
                    clean_k = r_name.replace(" ", "").replace("\n", "").replace("\r", "").replace('"', '').replace("'", "").strip()
                    rule_info_map[clean_k] = row
                    rules_map_for_xray[clean_k] = row

                # 4. é¡¯ç¤ºçµæœ (å¦‚æœæœ‰å‘½ä¸­)
                if rule_hits:
                    st.success(f"ğŸ¯ ç³»çµ±åµæ¸¬åˆ° {len(rule_hits)} ç¨®ç‰¹è¦é …ç›®ï¼")
                    
                    for rule_key, hits in rule_hits.items():
                        info = rule_info_map.get(rule_key, {})
                        
                        st.markdown(f"#### âœ… {rule_key}")
                        
                        # ğŸ”¥ğŸ”¥ğŸ”¥ [ç‰ˆé¢ä¿®æ”¹] æ”¹ç‚º 2 æ¬„æ’åˆ—ï¼Œé¡¯ç¤º 6 å€‹æ¬„ä½ ğŸ”¥ğŸ”¥ğŸ”¥
                        c_left, c_right = st.columns(2)
                        
                        with c_left:
                            st.markdown(f"**Local:** `{info.get('Unit_Rule_Local', '-')}`")
                            st.markdown(f"**Freight:** `{info.get('Unit_Rule_Freight', '-')}`")
                            st.markdown(f"**Agg:** `{info.get('Unit_Rule_Agg', '-')}`")
                            
                        with c_right:
                            # å˜—è©¦è®€å–æ›´å¤šæ¬„ä½ï¼Œè‹¥ Excel æ²’é€™æ¬„ä½æœƒé¡¯ç¤º '-'
                            st.markdown(f"**Category:** `{info.get('Category', '-')}`")
                            st.markdown(f"**Process:** `{info.get('Process_Rule', '-')}`")
                            st.markdown(f"**Logic:** `{info.get('Logic_Prompt', '-')}`")
                        # -----------------------------------------------------
                        
                        # é¡¯ç¤ºæ˜ç´°è¡¨æ ¼
                        hit_df = pd.DataFrame(hits)
                        cols_to_show = ["æ˜ç´°åç¨±", "åˆ†æ•¸", "åŒ¹é…é¡å‹", "é ç¢¼"]
                        final_cols = [c for c in cols_to_show if c in hit_df.columns]
                        
                        if "åˆ†æ•¸" in final_cols:
                            st.dataframe(hit_df[final_cols].style.format({"åˆ†æ•¸": "{:.0f}"}), use_container_width=True, hide_index=True)
                        else:
                            st.dataframe(hit_df, use_container_width=True, hide_index=True)
                        st.divider()
                else:
                    if target_list:
                        st.info(f"æœ¬æ¬¡å·¥ä»¤æœªè§¸ç™¼ä»»ä½•ç‰¹è¦é …ç›® (é–€æª»: {current_fuzz})ã€‚")
                    else:
                        st.warning("âš ï¸ å°šæœªåŸ·è¡Œåˆ†ææˆ–ç„¡åˆ†æçµæœã€‚")

                # åº•éƒ¨ï¼šå®Œæ•´çš„è¦å‰‡ç¸½è¡¨
                st.markdown("---")
                with st.expander("ğŸ“‹ æŸ¥çœ‹å®Œæ•´è¦å‰‡ç¸½è¡¨ (All Rules)", expanded=False):
                    st.dataframe(df_rules, use_container_width=True, hide_index=True)

                # ğŸ”¥ Xå…‰æ©Ÿ (ä¿ç•™)
                st.markdown("---")
                st.subheader("ğŸ•µï¸â€â™‚ï¸ Xå…‰æª¢æ¸¬ï¼šç‚ºä»€éº¼æ²’æŠ“åˆ°ï¼Ÿ")
                st.caption(f"é€™è£¡åˆ—å‡ºå‰ 10 ç­†é …ç›®çš„æœ€é«˜åˆ†è¦å‰‡ï¼Œå¹«æ‚¨æ±ºå®š GLOBAL_FUZZ_THRESHOLD è©²è¨­å¤šå°‘ (ç›®å‰: {current_fuzz})")
                
                sample_items = []
                acc_input = st.session_state.get('analysis_result_cache', {}).get('ai_extracted_data', [])
                if acc_input:
                    sample_items = [item.get('item_title', '') for item in acc_input[:10]]
                
                if sample_items:
                    debug_data = []
                    for item_title in sample_items:
                        clean_title = item_title.replace(" ", "").replace("\n", "").strip()
                        best_score = 0
                        best_rule = "ç„¡"
                        
                        # è¨˜å¾—é€™è£¡è¦è·Ÿæ‚¨æœ€å¾Œæ±ºå®šä½¿ç”¨çš„ fuzz æ–¹å¼åŒæ­¥ (ç›®å‰å»ºè­° token_sort_ratio)
                        for k in rules_map_for_xray.keys():
                            sc = fuzz.token_sort_ratio(k, clean_title)
                            if sc > best_score:
                                best_score = sc
                                best_rule = k
                        
                        status = "ğŸ”´ è½æ¦œ"
                        if best_score > current_fuzz: status = "ğŸŸ¢ éŒ„å–"
                        
                        debug_data.append({
                            "å·¥ä»¤é …ç›®": clean_title,
                            "æœ€åƒçš„è¦å‰‡": best_rule,
                            "è¨ˆç®—åˆ†æ•¸": best_score,
                            "ç‹€æ…‹": status
                        })
                    st.dataframe(pd.DataFrame(debug_data))

            except Exception as e:
                st.error(f"UI é¡¯ç¤ºéŒ¯èª¤: {e}")
                
        # 5. åŸå§‹æ•¸æ“šæª¢è¦–
        with st.expander("ğŸ“Š æª¢è¦– AI æŠ„éŒ„åŸå§‹æ•¸æ“š", expanded=False):
            st.markdown("**1. æ ¸å¿ƒæŒ‡æ¨™æ‘˜è¦**")
            sum_rows_len = len(cache.get("summary_rows", []))
            summary_df = pd.DataFrame([{
                "å·¥ä»¤å–®è™Ÿ": cache.get("job_no", "N/A"),
                "ç¸½è¡¨è¡Œæ•¸": sum_rows_len,
                "ç¸½è¡¨ç‹€æ…‹": "æ­£å¸¸" if sum_rows_len > 0 else "ç©ºå€¼"
            }])
            st.dataframe(summary_df, hide_index=True, use_container_width=True)
            st.divider()
 
            st.markdown("**2. å·¦ä¸Šè§’çµ±è¨ˆè¡¨ (Summary Rows)**")
            sum_rows = cache.get("summary_rows", [])
            
            if sum_rows:
                df_sum = pd.DataFrame(sum_rows)
                
                # 1. ç¢ºä¿é ç¢¼æ¬„ä½å­˜åœ¨
                if "page" not in df_sum.columns: df_sum["page"] = "?"
                
                # 2. æ¬„ä½æ›´å (å…¼å®¹èˆŠç‰ˆ target èˆ‡æ–°ç‰ˆ delivery_qty)
                rename_map = {
                    "page": "é ç¢¼", 
                    "title": "é …ç›®åç¨±", 
                    "apply_qty": "ç”³è«‹æ•¸é‡",    # âœ… æ–°å¢ï¼šç”³è«‹æ•¸é‡
                    "delivery_qty": "å¯¦äº¤æ•¸é‡", # âœ… æ–°å¢ï¼šå¯¦äº¤æ•¸é‡
                    "target": "å¯¦äº¤æ•¸é‡"        # èˆŠç‰ˆå…¼å®¹ (è‹¥ç„¡ delivery_qty å‰‡ç”¨ target)
                }
                df_sum.rename(columns=rename_map, inplace=True)
                
                # 3. æŒ‡å®šé¡¯ç¤ºé †åº (ç¢ºä¿æ¬„ä½ä¸æœƒæ¶ˆå¤±)
                # å…ˆåˆ—å‡ºæˆ‘å€‘æƒ³è¦çš„é †åº
                desired_cols = ["é ç¢¼", "é …ç›®åç¨±", "ç”³è«‹æ•¸é‡", "å¯¦äº¤æ•¸é‡"]
                # åªä¿ç•™ DataFrame ä¸­çœŸçš„å­˜åœ¨çš„æ¬„ä½
                final_cols = [c for c in desired_cols if c in df_sum.columns]
                
                st.dataframe(df_sum[final_cols], hide_index=True, use_container_width=True)
            else:
                st.caption("ç„¡æ•¸æ“š")

            st.divider()
            st.markdown("**3. å…¨å·è©³ç´°æŠ„éŒ„æ•¸æ“š (JSON)**")
            st.json(cache.get("ai_extracted_data", []), expanded=True)

        # ========================================================
        # âš¡ï¸ [ä¿®æ­£é‡é»]ï¼šç¾åœ¨ all_issues å·²ç¶“å®šç¾©äº†ï¼Œé€™è£¡å°±ä¸æœƒå ±éŒ¯äº†
        # ========================================================
        
        # 1. åŸ·è¡Œåˆä½µ
        consolidated_list = consolidate_issues(all_issues)

        # 2. éæ¿¾å‡ºã€ŒçœŸæ­£çš„éŒ¯èª¤ã€
        real_errors_consolidated = [i for i in consolidated_list if "æœªåŒ¹é…" not in i.get('issue_type', '')]

        # 3. é¡¯ç¤ºçµè«–
        if not all_issues:
            st.balloons()
            st.success("âœ… å…¨æ•¸åˆæ ¼ï¼")
        elif not real_errors_consolidated:
            st.success(f"âœ… æ•¸å€¼åˆæ ¼ï¼ (ä½†æœ‰ {len(consolidated_list)} é¡é …ç›®æœªåŒ¹é…è¦å‰‡)")
        else:
            st.error(f"ç™¼ç¾ {len(real_errors_consolidated)} é¡ç•°å¸¸")

        # 4. å¡ç‰‡å¾ªç’°é¡¯ç¤º (v39: æ•¸å€¼ç²¾ä¿®ç‰ˆ)
        for item in consolidated_list:
            #  [å°±åœ¨é€™è£¡ï¼æ’å…¥é€™å…©è¡Œ] 
            if item.get('issue_type') == 'HIDDEN_DATA':
                continue
                
            with st.container(border=True):
                c1, c2 = st.columns([3, 1])
                source_label = item.get('source', '')
                issue_type = item.get('issue_type', 'ç•°å¸¸')
                
                # é ç¢¼è™•ç†
                page_str = item.get('page', '?')
                if "," in str(page_str):
                    page_display = f"Pages: {page_str}"
                else:
                    page_display = f"P.{page_str}"

                c1.markdown(f"**{page_display} | {item.get('item')}** `{source_label}`")
                
                # ç‡ˆè™Ÿé‚è¼¯
                if any(kw in issue_type for kw in ["çµ±è¨ˆ", "æ•¸é‡", "æµç¨‹", "æº¯æº", "ç¸½è¡¨", "åŒ¯ç¸½", "ğŸš¨", "ğŸ›‘"]):
                    c2.error(f"{issue_type}")
                else:
                    c2.warning(f"{issue_type}")
                
                st.caption(f"åŸå› : {item.get('common_reason', '')}")
                
                failures = item.get('failures', [])
                if failures:
                    # 1. è½‰æˆ DataFrame
                    df = pd.DataFrame(failures)
                    
                    # 2. æ¬„ä½ä¸­æ–‡åŒ–
                    rename_map = {
                        "id": "ç·¨è™Ÿ",
                        "val": "å¯¦æ¸¬",
                        "target": "ç›®æ¨™",
                        "calc": "ç‹€æ…‹",
                        "note": "å‚™è¨»"
                    }
                    df.rename(columns=rename_map, inplace=True)
                    
                    # 3. æ¨£å¼èª¿æ•´ (ç½®ä¸­èˆ‡é å·¦)
                    styler = df.style.set_properties(**{
                        'text-align': 'center', 
                        'white-space': 'nowrap'
                    })
                    
                    styler.set_table_styles([
                        dict(selector='th', props=[('text-align', 'center')])
                    ])

                    # é‡å°æ–‡å­—è¼ƒé•·çš„æ¬„ä½é å·¦
                    left_align_cols = [c for c in ["é …ç›®åç¨±", "ç·¨è™Ÿ", "Item"] if c in df.columns]
                    if left_align_cols:
                        styler.set_properties(subset=left_align_cols, **{'text-align': 'left'})

                    # ğŸ”¥ [æ–°å¢] 4. æ™ºèƒ½æ•¸å€¼æ ¼å¼åŒ– (Smart Formatting)
                    # é‚è¼¯ï¼šæ•´æ•¸é¡¯ç¤ºæ•´æ•¸ (10)ï¼Œå°æ•¸é¡¯ç¤ºå…©ä½ (10.53)
                    def smart_fmt(x):
                        try:
                            f = float(x)
                            # å¦‚æœè·Ÿå››æ¨äº”å…¥å¾Œçš„è‡ªå·±å·®å¾ˆå°ï¼Œå°±ç•¶ä½œæ•´æ•¸
                            if abs(f - round(f)) < 0.000001: 
                                return f"{int(f)}"
                            return f"{f:.2f}"
                        except:
                            return str(x)

                    # é–å®šå¯èƒ½å‡ºç¾æ•¸å­—çš„æ¬„ä½
                    target_cols = [c for c in ["å¯¦æ¸¬", "ç›®æ¨™", "æ•¸é‡"] if c in df.columns]
                    if target_cols:
                        styler.format(smart_fmt, subset=target_cols)

                    # 5. é¡¯ç¤ºè¡¨æ ¼
                    st.dataframe(styler, use_container_width=True, hide_index=True)

            st.divider()
        
        # ä¸‹è¼‰æŒ‰éˆ•é‚è¼¯
        current_job_no = cache.get('job_no', 'Unknown')
        safe_job_no = str(current_job_no).replace("/", "_").replace("\\", "_").strip()
        file_name_str = f"{safe_job_no}_cleaned.json"

        # æº–å‚™åŒ¯å‡ºè³‡æ–™
        export_data = []
        for item in st.session_state.photo_gallery:
            export_data.append({
                "table_md": item.get('table_md'),
                "header_text": item.get('header_text'),
                "full_text": item.get('full_text'),
                "raw_json": item.get('raw_json')
            })
        json_str = json.dumps(export_data, indent=2, ensure_ascii=False)

        st.subheader("ğŸ’¾ æ¸¬è©¦è³‡æ–™å­˜æª”")
        st.caption(f"å·²è­˜åˆ¥å·¥ä»¤ï¼š**{current_job_no}**ã€‚ä¸‹è¼‰å¾Œå¯ä¾›ä¸‹æ¬¡æ¸¬è©¦ä½¿ç”¨ã€‚")
        
        st.download_button(
            label=f"â¬‡ï¸ ä¸‹è¼‰æ¸¬è©¦è³‡æ–™ ({file_name_str})",
            data=json_str,
            file_name=file_name_str,
            mime="application/json",
            type="primary"
        )

        with st.expander("ğŸ‘€ æŸ¥çœ‹å‚³çµ¦ AI çš„æœ€çµ‚æ–‡å­— (Prompt Input)"):
            st.caption("é€™æ‰æ˜¯ AI çœŸæ­£è®€åˆ°çš„å…§å®¹ (å·²éæ¿¾é›œè¨Š)ï¼š")
            st.code(cache.get('combined_input', 'ç„¡è³‡æ–™'), language='markdown')
    
    if st.session_state.photo_gallery and st.session_state.get('source_mode') != 'json':
        st.caption("å·²æ‹æ”ç…§ç‰‡ï¼š")
        cols = st.columns(4)
        for idx, item in enumerate(st.session_state.photo_gallery):
            with cols[idx % 4]:
                if item.get('file'):
                    st.image(item['file'], caption=f"P.{idx+1}", use_container_width=True)
                if st.button("âŒ", key=f"del_{idx}"):
                    st.session_state.photo_gallery.pop(idx)
                    st.session_state.analysis_result_cache = None
                    st.rerun()
else:
    st.info("ğŸ‘† è«‹é»æ“Šä¸Šæ–¹æŒ‰éˆ•é–‹å§‹æ–°å¢ç…§ç‰‡")
