import streamlit as st
import streamlit.components.v1 as components
from azure.core.credentials import AzureKeyCredential
from azure.ai.documentintelligence import DocumentIntelligenceClient
from azure.ai.documentintelligence.models import AnalyzeResult
import google.generativeai as genai
from openai import OpenAI
import json
import time
import concurrent.futures
import pandas as pd
from thefuzz import fuzz
from collections import Counter
import re

# --- 1. é é¢è¨­å®š ---
st.set_page_config(page_title="äº¤è²¨å–®ç¨½æ ¸", page_icon="ğŸ­", layout="centered")

# --- CSS æ¨£å¼ ---
st.markdown("""
<style>
/* 1. æ¨™é¡Œå¤§å°æ§åˆ¶ */
h1 {
    font-size: 1.7rem !important; 
    white-space: nowrap !important;
    overflow: hidden !important; 
    text-overflow: ellipsis !important;
}

/* 2. ä¸»åŠŸèƒ½æŒ‰éˆ• (ç´…è‰² Primary) -> è®Šå¤§ã€è®Šé«˜ */
/* é€™æœƒå½±éŸ¿ã€Œé–‹å§‹åˆ†æã€å’Œã€Œç…§ç‰‡æ¸…é™¤ã€ */
button[kind="primary"] {
    height: 60px;               
    font-size: 20px !important; 
    font-weight: bold !important;
    border-radius: 10px !important;
    margin-top: 0px !important;    
    margin-bottom: 5px !important; 
    width: 100%;                
}

/* 3. æ¬¡è¦æŒ‰éˆ• (ç°è‰² Secondary) -> ä¿æŒåŸç‹€ */
/* é€™æœƒå½±éŸ¿æ¯ä¸€å¼µç…§ç‰‡ä¸‹é¢çš„ã€ŒXã€æŒ‰éˆ•ï¼Œè®“å®ƒç¶­æŒå°å°çš„ */
button[kind="secondary"] {
    height: auto !important;
    font-weight: normal !important;
}
</style>
""", unsafe_allow_html=True)
# --- 2. ç§˜å¯†é‡‘é‘°è®€å– ---
try:
    DOC_ENDPOINT = st.secrets["DOC_ENDPOINT"]
    DOC_KEY = st.secrets["DOC_KEY"]
    GEMINI_KEY = st.secrets["GEMINI_KEY"]
    OPENAI_KEY = st.secrets.get("OPENAI_KEY", "")
except:
    st.error("æ‰¾ä¸åˆ°é‡‘é‘°ï¼è«‹åœ¨ Streamlit Cloud è¨­å®š Secretsã€‚")
    st.stop()

# --- 3. åˆå§‹åŒ– Session State ---
if 'photo_gallery' not in st.session_state: st.session_state.photo_gallery = []
if 'uploader_key' not in st.session_state: st.session_state.uploader_key = 0
if 'auto_start_analysis' not in st.session_state: st.session_state.auto_start_analysis = False

# --- å´é‚Šæ¬„æ¨¡å‹è¨­å®š (åˆä½µç‚ºå–®ä¸€é¸æ“‡) ---
with st.sidebar:
    st.header("æ¨¡å‹è¨­å®š")
    
    # é€™è£¡åŠ å…¥æœ€æ–°çš„ Gemini æ¨¡å‹
    model_options = {
        "Gemini 3 Flash preview": "gemini-3-flash-preview",
        "Gemini 2.5 Flash": "models/gemini-2.5-flash",
        "Gemini 2.5 Pro": "models/gemini-2.5-pro",
        #"GPT-5(ç„¡æ•ˆ)": "models/gpt-5",
        #"GPT-5 Mini(ç„¡æ•ˆ)": "models/gpt-5-mini",
    }
    options_list = list(model_options.keys())
    
    st.subheader("ğŸ¤– ç¸½ç¨½æ ¸ Agent")
    model_selection = st.selectbox(
        "è² è²¬ï¼šè¦æ ¼ã€è£½ç¨‹ã€æ•¸é‡ã€çµ±è¨ˆå…¨åŒ…", 
        options=options_list, 
        index=0, 
        key="main_model"
    )
    main_model_name = model_options[model_selection]
    
    st.divider()
    
    default_auto = st.query_params.get("auto", "true") == "true"
    def update_url_param():
        current_state = "true" if st.session_state.enable_auto_analysis else "false"
        st.query_params["auto"] = current_state

    st.toggle(
        "âš¡ ä¸Šå‚³å¾Œè‡ªå‹•åˆ†æ", 
        value=default_auto, 
        key="enable_auto_analysis", 
        on_change=update_url_param
    )

# --- Excel è¦å‰‡è®€å–å‡½æ•¸ (å–®ä¸€ä»£ç†æ•´åˆç‰ˆ) ---
@st.cache_data
def get_dynamic_rules(ocr_text, debug_mode=False):
    try:
        df = pd.read_excel("rules.xlsx")
        df.columns = [c.strip() for c in df.columns]
        ocr_text_clean = str(ocr_text).upper().replace(" ", "").replace("\n", "")
        specific_rules = []

        for index, row in df.iterrows():
            item_name = str(row.get('Item_Name', '')).strip()
            # ğŸ’¡ è·³éåŸæœ¬çš„ã€Œ(é€šç”¨)ã€é …ç›®ï¼ŒåªæŠ“ç‰¹è¦
            if not item_name or "(é€šç”¨)" in item_name: continue
            
            # ä½¿ç”¨æ¨¡ç³ŠåŒ¹é…åˆ¤æ–·æ˜¯å¦ç‚ºç•¶å‰è™•ç†çš„é …ç›®
            score = fuzz.partial_ratio(item_name.upper().replace(" ", ""), ocr_text_clean)
            if score >= 85:
                # æå–ç‰¹è¦è³‡è¨Š
                spec = str(row.get('Standard_Spec', ''))
                logic = str(row.get('Logic_Prompt', ''))
                u_local = str(row.get('Unit_Rule_Local', ''))
                u_agg = str(row.get('Unit_Rule_Agg', ''))
                u_freight = str(row.get('Unit_Rule_Freight', ''))
                
                desc = f"- **[ç‰¹å®šé …ç›®è¦å‰‡] {item_name}**\n"
                if spec != 'nan' and spec: desc += f"  - [å¼·åˆ¶è¦æ ¼]: {spec}\n"
                if logic != 'nan' and logic: desc += f"  - [ä¾‹å¤–æŒ‡ä»¤]: {logic}\n"
                if u_local != 'nan' and u_local: desc += f"  - [æœƒè¨ˆå–®é …]: {u_local}\n"
                if u_agg != 'nan' and u_agg: desc += f"  - [æœƒè¨ˆèšåˆ]: {u_agg}\n"
                if u_freight != 'nan' and u_freight: desc += f"  - [æœƒè¨ˆé‹è²»]: {u_freight}\n"
                specific_rules.append(desc)
        
        return "\n".join(specific_rules) if specific_rules else "ç„¡ç‰¹å®šå°ˆæ¡ˆè¦å‰‡ï¼Œè«‹ä¾ç…§é€šç”¨æ†²æ³•åŸ·è¡Œã€‚"
    except Exception as e:
        return f"è®€å–è¦å‰‡æª”æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}"
        
# --- 4. æ ¸å¿ƒå‡½æ•¸ï¼šAzure ç¥ä¹‹çœ¼ ---
def extract_layout_with_azure(file_obj, endpoint, key):
    client = DocumentIntelligenceClient(endpoint=endpoint, credential=AzureKeyCredential(key))
    file_content = file_obj.getvalue()
    
    poller = client.begin_analyze_document("prebuilt-layout", file_content, content_type="application/octet-stream")
    result: AnalyzeResult = poller.result()
    
    markdown_output = ""
    full_content_text = ""
    real_page_num = "Unknown"
    
    bottom_stop_keywords = ["æ³¨æ„äº‹é …", "ä¸­æ©Ÿå“æª¢å–®ä½", "ä¿å­˜æœŸé™", "è¡¨å–®ç·¨è™Ÿ", "FORM NO", "ç°½ç« "]
    top_right_noise_keywords = [
        "æª¢é©—é¡åˆ¥", "å°ºå¯¸æª¢é©—", "ä¾åœ–é¢æ¨™è¨˜", "ææ–™æª¢é©—", "æˆä»½åˆ†æ", 
        "éç ´å£æ€§", "æ­£å¸¸åŒ–", "é€€ç«", "æ·¬.å›ç«", "è¡¨é¢ç¡¬åŒ–", "è©¦è»Š",
        "æ€§èƒ½æ¸¬è©¦", "è©¦å£“è©¦æ¼", "å‹•.éœå¹³è¡¡è©¦é©—", ":selected:", ":unselected:",
        "æŠ—æ‹‰", "ç¡¬åº¦è©¦é©—", "UT", "PT", "MT"
    ]
    
    if result.tables:
        for idx, table in enumerate(result.tables):
            page_num = "Unknown"
            if table.bounding_regions: page_num = table.bounding_regions[0].page_number
            markdown_output += f"\n### Table {idx + 1} (Page {page_num}):\n"
            rows = {}
            stop_processing_table = False 
            
            for cell in table.cells:
                if stop_processing_table: break
                content = cell.content.replace("\n", " ").strip()
                
                for kw in bottom_stop_keywords:
                    if kw in content:
                        stop_processing_table = True
                        break
                if stop_processing_table: break
                
                is_noise = False
                for kw in top_right_noise_keywords:
                    if kw in content:
                        is_noise = True
                        break
                if is_noise: content = "" 

                r, c = cell.row_index, cell.column_index
                if r not in rows: rows[r] = {}
                rows[r][c] = content
            
            for r in sorted(rows.keys()):
                row_cells = []
                if rows[r]:
                    max_col = max(rows[r].keys())
                    for c in range(max_col + 1): 
                        row_cells.append(rows[r].get(c, ""))
                    markdown_output += "| " + " | ".join(row_cells) + " |\n"
    
    if result.content:
        match = re.search(r"(?:é …æ¬¡|Page|é æ¬¡|NO\.)[:\s]*(\d+)\s*[/ï¼]\s*\d+", result.content, re.IGNORECASE)
        if match:
            real_page_num = match.group(1)

        cut_index = len(result.content)
        for keyword in bottom_stop_keywords:
            idx = result.content.find(keyword)
            if idx != -1 and idx < cut_index:
                cut_index = idx
        
        temp_text = result.content[:cut_index]
        for noise in top_right_noise_keywords:
            temp_text = temp_text.replace(noise, "")
            
        full_content_text = temp_text
        header_snippet = full_content_text[:800]
    else:
        full_content_text = ""
        header_snippet = ""

    return markdown_output, header_snippet, full_content_text, None, real_page_num

# --- Python ç¡¬é‚è¼¯ï¼šè¡¨é ­ä¸€è‡´æ€§æª¢æŸ¥ (é•·åº¦æ•æ„Ÿç‰ˆ) ---
def python_header_check(photo_gallery):
    issues = []
    if not photo_gallery:
        return issues, []

    # å®šç¾© Regex (é‡å° "å»ç©ºç™½+å»æ›è¡Œ" å¾Œçš„å­—ä¸²è¨­è¨ˆ)
    patterns = {
        # ã€ä¿®æ”¹é» 1ã€‘å·¥ä»¤ Regex æ”¾å¯¬ï¼š
        # åŸæœ¬åªæŠ“ W é–‹é ­ï¼Œç¾åœ¨æ”¹æŠ“ "ç·¨è™Ÿ" å¾Œé¢æ¥çš„ "ä»»ä½•è‹±æ•¸å­—ä¸²"
        # é€™æ¨£å°±ç®—å®ƒå¯« WW363... æˆ–æ˜¯ 12345... éƒ½èƒ½æ•´ä¸²æŠ“å‡ºä¾†æ¯”å°
        "å·¥ä»¤ç·¨è™Ÿ": r"[å·¥åœŸä¸‹][ä»¤å†·ä»Š]ç·¨è™Ÿ[:\.]*([A-Za-z0-9\-\_]+)", 
        
        "é å®šäº¤è²¨": r"[é é¢„é …é ‚][å®šäº¤].*?(\d{2,4}[\.\-/]\d{1,2}[\.\-/]\d{1,2})",
        "å¯¦éš›äº¤è²¨": r"[å¯¦çœŸ][éš›äº¤].*?(\d{2,4}[\.\-/]\d{1,2}[\.\-/]\d{1,2})"
    }

    extracted_data = [] 
    all_values = {key: [] for key in patterns}

    for i, page in enumerate(photo_gallery):
        # æš´åŠ›æ¸…æ´—ï¼šå»æ›è¡Œã€å»ç©ºæ ¼ã€è½‰å¤§å¯«
        raw_text = page.get('header_text', '') + page.get('full_text', '')
        clean_text = raw_text.replace("\n", "").replace(" ", "").replace("\r", "").upper()
        
        # ã€ä¿®æ”¹é» 2ã€‘é ç¢¼é˜²å‘†ï¼šç¢ºä¿ä¸€å®šæœ‰å€¼
        # å„ªå…ˆæŠ“ real_pageï¼ŒæŠ“ä¸åˆ°å°±ç”¨ index
        r_page = page.get('real_page')
        if not r_page or r_page == "Unknown":
            page_label = f"P.{i + 1}"
        else:
            page_label = f"P.{r_page}"
            
        page_result = {"é æ•¸": page_label}
        
        for key, pattern in patterns.items():
            match = re.search(pattern, clean_text)
            if match:
                val = match.group(1).strip()
                
                # ã€ä¿®æ”¹é» 3ã€‘é‡å°å·¥ä»¤çš„ç‰¹æ®Šè™•ç† (å¦‚æœå¤ªé•·å¯èƒ½å°±æ˜¯é‡è¤‡æ‰“å­—)
                if key == "å·¥ä»¤ç·¨è™Ÿ":
                    # å¦‚æœä½ ç¢ºå®šå·¥ä»¤åªæœ‰ 10 ç¢¼ï¼Œä½†æŠ“åˆ°äº† 11 ç¢¼ä»¥ä¸Š (å¦‚ WW...)
                    # æˆ‘å€‘ä¿ç•™é€™å€‹éŒ¯èª¤çš„å€¼ï¼Œè®“å¾Œé¢çš„å¤šæ•¸æ±ºå»æŠŠå®ƒæªå‡ºä¾†
                    pass 
                
                page_result[key] = val
                all_values[key].append(val)
            else:
                page_result[key] = "N/A"
        
        extracted_data.append(page_result)

    # æ­¥é©Ÿ 2: æ±ºå®šã€Œæ­£ç¢ºæ¨™æº–ã€ (ä½¿ç”¨å¤šæ•¸æ±º)
    standard_data = {}
    for key, values in all_values.items():
        if values:
            # æ¿¾æ‰ N/A å¾Œå†æŠ•ç¥¨
            valid_values = [v for v in values if v != "N/A"]
            if valid_values:
                most_common = Counter(valid_values).most_common(1)[0][0]
                standard_data[key] = most_common
            else:
                standard_data[key] = "N/A"
        else:
            standard_data[key] = "N/A"

    # æ­¥é©Ÿ 3: æ¯”å°æ¯ä¸€é 
    for data in extracted_data:
        page_num = data['é æ•¸']
        
        for key, standard_val in standard_data.items():
            current_val = data[key]
            
            if standard_val == "N/A": continue # å…¨å·éƒ½æ²’æŠ“åˆ°å°±ä¸æ¯”äº†

            # é–‹å§‹æ¯”å° (å­—ä¸²ä¸ç›¸ç­‰)
            if current_val != standard_val:
                
                # åˆ¤æ–·æ˜¯å¦ç‚ºé•·åº¦ç•°å¸¸ (é‡å°å·¥ä»¤)
                reason = "èˆ‡å…¨å·å¤šæ•¸é é¢ä¸ä¸€è‡´"
                if key == "å·¥ä»¤ç·¨è™Ÿ" and len(current_val) != len(standard_val):
                    reason += f" (é•·åº¦ç•°å¸¸: {len(current_val)}ç¢¼ vs æ¨™æº–{len(standard_val)}ç¢¼)"

                issue = {
                    "page": page_num.replace("P.", ""),
                    "item": f"è¡¨é ­æª¢æŸ¥-{key}",
                    "rule_used": "Pythonç¡¬é‚è¼¯æª¢æŸ¥",
                    "issue_type": "è·¨é è³‡è¨Šä¸ç¬¦",
                    "spec_logic": f"æ‡‰ç‚º {standard_val}",
                    "common_reason": reason,
                    "failures": [
                        {"id": "å…¨å·åŸºæº–", "val": standard_val, "calc": "å¤šæ•¸æ±ºæ¨™æº–"},
                        {"id": f"æœ¬é ({page_num})", "val": current_val, "calc": "ç•°å¸¸/æ¼æŠ“"}
                    ],
                    "source": "ğŸ¤– ç³»çµ±è‡ªå‹•"
                }
                issues.append(issue)
                
    return issues, extracted_data

    # --- 5. ç¸½ç¨½æ ¸ Agent (æ•´åˆç‰ˆ - å¼·é‚è¼¯å„ªåŒ–) ---
def agent_unified_check(combined_input, full_text_for_search, api_key, model_name):
    dynamic_rules = get_dynamic_rules(full_text_for_search)

    system_prompt = f"""
    ä½ æ˜¯ä¸€ä½æ¥µåº¦åš´è¬¹çš„ä¸­é‹¼æ©Ÿæ¢°å“ç®¡ã€æ•¸æ“šæŠ„éŒ„å“¡ã€‘ã€‚ä½ å¿…é ˆåƒã€Œé›»è…¦ç¨‹å¼ã€ä¸€æ¨£åŸ·è¡Œæå–ä»»å‹™ã€‚
    
    {dynamic_rules}

    ---

    #### âš”ï¸ æ¨¡çµ„ Aï¼šæ•¸æ“šæŠ„éŒ„èˆ‡åˆ†é¡ (AI ç¿»è­¯å®˜)
    1. **è¦æ ¼æŠ„éŒ„ (std_spec)**ï¼šç²¾ç¢ºæŠ„éŒ„æ¨™é¡Œä¸­å« `mm`ã€`Â±`ã€`+`ã€`-`ã€`è‡³...å†ç”Ÿ` çš„æ–‡å­—ã€‚
       - **ğŸš« ç¦ä»¤**ï¼šåš´ç¦åŸ·è¡ŒåŠ æ¸›æ³•é‹ç®—ï¼Œä¿æŒ std_ranges ç‚ºç©ºï¼Œå°‡åŸå§‹æ–‡å­—æŠ„éŒ„åˆ° std_spec å³å¯ã€‚
    2. **æ•¸æ“šæŠ„éŒ„ (ds)**ï¼šæ¡ç”¨å£“ç¸®æ ¼å¼ `"ID:å€¼|ID:å€¼"`ã€‚
       - **å­—ä¸²ä¿è­·**ï¼šå¯¦æ¸¬å€¼é¡¯ç¤º `349.90` å¿…å¯« `"349.90"`ã€‚ç¦æ­¢ç°¡åŒ–æ•¸å­—ã€‚
       - **å£è»Œæ¨™è¨˜ [!]**ï¼šè‹¥å„²å­˜æ ¼è¾¨è­˜ä¸è‰¯ï¼ˆæ±™é»/é®æ“‹/é»é€£ï¼‰ï¼Œåš´ç¦è…¦è£œï¼Œç›´æ¥æ¨™è¨˜ç‚º `[!]`ã€‚
    3. **åˆ†é¡è­˜åˆ¥ (category) æ±ºç­–æµ**ï¼š
       - LEVEL 1ï¼šå«ã€ŒéŠ²è£œã€ -> `min_limit`ã€‚
       - LEVEL 2ï¼šå«ã€Œæœªå†ç”Ÿã€ã€‚a.å«ã€Œè»¸é ¸ã€-> `max_limit`ï¼›b.ä¸å«ã€Œè»¸é ¸ã€-> `un_regen`ã€‚
       - LEVEL 3ï¼šå«ã€Œå†ç”Ÿ/ç ”ç£¨/ç²¾åŠ å·¥/è»Šä¿®/çµ„è£/æ‹†è£/çœŸåœ“åº¦ã€ -> `range`ã€‚

    #### ğŸ’° æ¨¡çµ„ Bï¼šæœƒè¨ˆæŒ‡æ¨™æå– (ç”± AI æŠ„éŒ„å‚³ç¥¨)
    1. **ç¸½è¡¨æå–**ï¼šæŠ„éŒ„å·¦ä¸Šè§’çµ±è¨ˆè¡¨æ¯ä¸€è¡Œçš„åç¨±èˆ‡å¯¦äº¤æ•¸é‡åˆ° `summary_rows`ã€‚
    2. **æŒ‡æ¨™æå–**ï¼šæå–é‹è²»é …æ¬¡åˆ° `freight_target`ï¼Œæå–é …ç›®æ‹¬è™Ÿå…§çš„æ•¸å­—åˆ° `item_pc_target`ã€‚
    3. **âš–ï¸ æµç¨‹ç¨½æ ¸**ï¼šæª¢æŸ¥ç‰©ç†ä½éš `æœªå†ç”Ÿ < ç ”ç£¨ < å†ç”Ÿ < éŠ²è£œ`ã€‚è‹¥è·¨é é¢å¾Œæ®µå°ºå¯¸å°æ–¼å‰æ®µï¼ˆéŠ²è£œé™¤å¤–ï¼‰ï¼Œå ± `ğŸ›‘æµç¨‹ç•°å¸¸`ã€‚

    ---
    
    ### ğŸ“ è¼¸å‡ºè¦ç¯„ (Output Format)
    å¿…é ˆå›å‚³å–®ä¸€ JSONã€‚çµ±è¨ˆä¸ç¬¦æ™‚å¿…é ˆã€Œé€è¡Œæ‹†åˆ†ã€ä¾†æºæ˜ç´°ã€‚

    {{
      "job_no": "å·¥ä»¤",
      "summary_rows": [ {{ "title": "å", "target": æ•¸å­— }} ],
      "freight_target": 0,
      "issues": [ 
         {{ "page": "é ç¢¼", "item": "é …ç›®", "issue_type": "çµ±è¨ˆä¸ç¬¦ / ğŸ›‘æµç¨‹ç•°å¸¸", "common_reason": "åŸå› ", "failures": [] }}
      ],
      "dimension_data": [
         {{
           "page": æ•¸å­—, "item_title": "æ¨™é¡Œ", "category": "åˆ†é¡åç¨±", "item_pc_target": 0,
           "accounting_rules": {{ "local": "", "agg": "", "freight": "" }},
           "sl": {{ "lt": "åˆ†é¡æ¨™ç±¤", "t": 0 }},
           "std_spec": "åŸå§‹è¦æ ¼æ–‡å­—",
           "ds": "ID:å€¼|ID:å€¼" 
         }}
      ]
    }}
    """
    
    # ä¿®æ”¹å¾Œçš„å»ºè­°é…ç½®
    generation_config = {
    "temperature": 0.0,             # âš¡ï¸ è¨­ç‚º 0ï¼šæœ€å¿«ä¸”æœ€ç©©å®šï¼Œä¸è®“ AI å¤šæƒ³
    "max_output_tokens": 4096,      # âš¡ï¸ å…ˆé™å› 4096ï¼šé€šå¸¸ 4 é è³‡æ–™é€™å€‹é•·åº¦å°±å¤ äº†ï¼Œæ¸›å°‘ AI å»¢è©±
    # "response_mime_type": "application/json" # âš¡ï¸ æš«æ™‚è¨»è§£æ‰é€™è¡Œï¼
    }

    
    try:
        genai.configure(api_key=api_key)
        
        # 2. è¨­å®š AI (é–‹å•Ÿ JSON æ¨¡å¼ä»¥ç¢ºä¿æˆåŠŸç‡)
        model = genai.GenerativeModel(
            model_name=model_name,
            generation_config={
                "temperature": 0.0,            # æœ€ç©©å®š
                "max_output_tokens": 8192,     # çµ¦äºˆè¶³å¤ é•·åº¦å¯«å®Œå¤§è¡¨
                "response_mime_type": "application/json" # âš¡ï¸ å¼·åˆ¶ JSON æ¨¡å¼ (é¿å…è§£æå¤±æ•—)
            },
            safety_settings={
                "HARM_CATEGORY_HARASSMENT": "BLOCK_NONE",
                "HARM_CATEGORY_HATE_SPEECH": "BLOCK_NONE",
                "HARM_CATEGORY_SEXUALLY_EXPLICIT": "BLOCK_NONE",
                "HARM_CATEGORY_DANGEROUS_CONTENT": "BLOCK_NONE",
            }
        )
        
        # 3. å‘¼å« AI (é€™è£¡æœƒè·‘ 20-40 ç§’æ˜¯æ­£å¸¸çš„)
        with st.spinner('ğŸ¤– AI æ­£åœ¨å…¨åŠ›æŠ„å¯«æ•¸æ“šä¸­...'):
            response = model.generate_content([system_prompt, combined_input])
        
        # 4. æª¢æŸ¥æ˜¯å¦æœ‰å…§å®¹
        raw_content = response.text.strip()
        
        # ç§»é™¤å¯èƒ½çš„å¤šé¤˜æ¨™è¨˜ (é›™é‡ä¿éšª)
        if raw_content.startswith("```json"):
            raw_content = raw_content[7:]
        if raw_content.endswith("```"):
            raw_content = raw_content[:-3]
        raw_content = raw_content.strip()

        # 5. è§£æ JSON
        parsed_data = json.loads(raw_content)
        
        # è¨˜éŒ„ Token
        parsed_data["_token_usage"] = {
            "input": response.usage_metadata.prompt_token_count, 
            "output": response.usage_metadata.candidates_token_count
        }
        return parsed_data

    except json.JSONDecodeError as e:
        # ğŸš¨ é€™è£¡å°±æ˜¯æŠ“å‡ºã€Œç‚ºä»€éº¼è·‘äº†29ç§’å»å¤±æ•—ã€çš„é—œéµ
        st.error("âŒ JSON è§£æå¤±æ•—ï¼è«‹æŸ¥çœ‹ä¸‹æ–¹ AI çš„åŸå§‹å›æ‡‰ï¼š")
        with st.expander("ğŸ‘€ é»æ“ŠæŸ¥çœ‹ AI åˆ°åº•å›å‚³äº†ä»€éº¼"):
            # å¦‚æœ AI æœ‰å›å‚³æ±è¥¿ï¼Œå°å‡ºä¾†çœ‹
            if 'raw_content' in locals():
                st.code(raw_content)
            elif 'response' in locals():
                st.code(response.text)
        # å›å‚³éŒ¯èª¤çµæ§‹ï¼Œé¿å…ç¨‹å¼ç•¶æ©Ÿ
        return {"job_no": "JSON Error", "issues": [], "dimension_data": []}

    except Exception as e:
        # å…¶ä»–éŒ¯èª¤ (ä¾‹å¦‚ç¶²è·¯ä¸­æ–·ã€API éŒ¯èª¤)
        st.error(f"âŒ ç³»çµ±ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        return {"job_no": f"Error: {str(e)}", "issues": [], "dimension_data": []}
    
# --- é‡é»ï¼šPython å¼•æ“ç¨ç«‹æ–¼ agent å‡½å¼ä¹‹å¤– ---

def python_numerical_audit(dimension_data):
    grouped_errors = {}
    import re
    if not dimension_data: return []

    for item in dimension_data:
        ds = str(item.get("ds", ""))
        if not ds: continue
        raw_entries = [p.split(":") for p in ds.split("|") if ":" in p]
        
        title = str(item.get("item_title", "")).replace(" ", "").replace('"', "")
        raw_spec = str(item.get("std_spec", "")).replace('"', "")
        cat = str(item.get("category", "")).strip()
        page_num = item.get("page", "?")

        # ğŸ’¡ [æ–°å¢ï¼šPython è‡ªå‹•è§£æå…¬å·®]
        s_ranges = []
        clean_part = raw_spec.replace(" ", "")
        pm = re.search(r"(\d+\.?\d*)?Â±(\d+\.?\d*)", clean_part)
        devs = re.findall(r"([+-]\d+\.?\d*)", clean_part)
        mm_match = re.findall(r"(\d+\.?\d*)mm", clean_part)
        clean_std = [float(n) for n in mm_match if float(n) > 5]

        if pm:
            b = float(pm.group(1)) if pm.group(1) else 0.0
            o = float(pm.group(2))
            s_ranges.append([round(b - o, 4), round(b + o, 4)])
        elif base_val := (clean_std[0] if clean_std else None):
            if len(devs) >= 2:
                calc_nums = [base_val + float(o) for o in devs]
                s_ranges.append([round(min(calc_nums), 4), round(max(calc_nums), 4)])

        for entry in raw_entries:
            if len(entry) < 2: continue
            rid, val_raw = entry[0].strip(), entry[1].strip()
            if not val_raw or val_raw in ["N/A", "nan"]: continue

            try:
                is_passed, reason, t_used, e_label = True, "", "N/A", "æœªçŸ¥"
                
                # --- 7.1 å£è»Œåµæ¸¬ ---
                if "[!]" in val_raw:
                    is_passed, reason, val_str, val = False, "ğŸ›‘æ•¸æ“šæå£(å£è»Œ)", "[!]", -999.0
                else:
                    v_m = re.findall(r"\d+\.?\d*", val_raw)
                    val_str = v_m[0] if v_m else val_raw
                    val = float(val_str)

                # --- 7.2 æ ¼å¼åˆ¤å®š ---
                if val_str != "[!]":
                    is_two_dec = "." in val_str and len(val_str.split(".")[-1]) == 2
                    is_pure_int = "." not in val_str
                else: is_two_dec, is_pure_int = True, True

                # --- 7.3 åˆ¤å®šé‚è¼¯ (éŠ²è£œ > æœªå†ç”Ÿ > ç²¾åŠ å·¥) ---
                if "min_limit" in cat or "éŠ²è£œ" in (cat + title):
                    e_label = "éŠ²è£œ"
                    t_used = min(clean_std) if clean_std else "N/A"
                    if not is_pure_int: is_passed, reason = False, "æ‡‰ç‚ºç´”æ•´æ•¸"
                    elif t_used != "N/A" and val < t_used: is_passed, reason = False, "æ•¸å€¼ä¸è¶³"
                
                elif "un_regen" in cat or "max_limit" in cat or "æœªå†ç”Ÿ" in (cat + title):
                    if "è»¸é ¸" in (cat + title):
                        e_label = "è»¸é ¸(ä¸Šé™)"
                        target = max(clean_std) if clean_std else 0
                        t_used = target
                        if target > 0 and val > target: is_passed, reason = False, f"è¶…éä¸Šé™ {target}"
                        if target > 0 and not is_pure_int: is_passed, reason = False, "æ‡‰ç‚ºç´”æ•´æ•¸"
                    else:
                        e_label = "æœªå†ç”Ÿ(æœ¬é«”)"
                        candidates = [n for n in clean_std if n >= 120.0]
                        target = max(candidates) if candidates else 196.0
                        t_used = target
                        if val <= target and not is_pure_int: is_passed, reason = False, "æ‡‰ç‚ºæ•´æ•¸"
                        elif val > target and not is_two_dec: is_passed, reason = False, "æ‡‰å¡«å…©ä½å°æ•¸"

                elif any(x in (cat + title) for x in ["å†ç”Ÿ", "ç²¾åŠ å·¥", "ç ”ç£¨", "è»Šä¿®", "çµ„è£"]):
                    e_label = "ç²¾åŠ å·¥"
                    if not is_two_dec: is_passed, reason = False, "æ‡‰å¡«å…©ä½å°æ•¸"
                    elif s_ranges:
                        t_used = str(s_ranges)
                        if not any(r[0] <= val <= r[1] for r in s_ranges): is_passed, reason = False, "ä¸åœ¨å€é–“å…§"

                if not is_passed:
                    key = (page_num, title, reason)
                    if key not in grouped_errors:
                        grouped_errors[key] = {"page": page_num, "item": title, "issue_type": f"æ•¸å€¼ç•°å¸¸({e_label})", "common_reason": reason, "failures": []}
                    grouped_errors[key]["failures"].append({"id": rid, "val": val_str, "target": f"åŸºæº–:{t_used}"})
            except: continue
    return list(grouped_errors.values())
    
def python_accounting_audit(dimension_data, res_main):
    """
    Python æœƒè¨ˆå®˜ï¼šã€è‡ªå‹•æŸ¥è¡¨å®Œå…¨é«”ã€‘
    æ•´åˆåŠŸèƒ½ï¼šå–®é …æ ¸å°(å»é‡)ã€è»¸é ¸é™æ¬¡ã€KGé‡é‡ç´¯åŠ ã€A/Bç¸½è¡¨æ¨¡å¼ã€é‹è²»å‹•æ…‹è§£æã€‚
    """
    accounting_issues = []
    from thefuzz import fuzz
    from collections import Counter
    import re
    import pandas as pd

    # ğŸ’¡ [æ–°å¢ï¼šè‡ªå‹•æŸ¥è¡¨æº–å‚™] è®€å–å…¨åŸŸ Excel è¦å‰‡æª”
    try:
        df_rules = pd.read_excel("rules.xlsx")
        df_rules.columns = [c.strip() for c in df_rules.columns]
    except:
        df_rules = None

    # ğŸ’¡ [è¼”åŠ©å·¥å…·ï¼šå®‰å…¨è½‰å‹æ•¸å­—] 
    def safe_float(value):
        if value is None or str(value).upper() == 'NULL': return 0.0
        val_str = str(value).strip()
        if "[!]" in val_str: return "BAD_DATA" 
        cleaned = "".join(re.findall(r"[\d\.]+", val_str.replace(',', '')))
        try: return float(cleaned) if cleaned else 0.0
        except: return 0.0

    # 1. å–å¾—å°å¸³åŸºæº– (ä¾†è‡ªå·¦ä¸Šè§’çµ±è¨ˆè¡¨)
    summary_rows = res_main.get("summary_rows", [])
    global_sum_tracker = {
        s['title']: {"target": safe_float(s['target']), "actual": 0, "details": []} 
        for s in summary_rows if s.get('title')
    }
    
    freight_target = safe_float(res_main.get("freight_target", 0))
    freight_actual_sum = 0
    freight_details = []

    # 2. é–‹å§‹é€é …éæ­·
    for item in dimension_data:
        title = item.get("item_title", "")
        page = item.get("page", "?")
        target_pc = safe_float(item.get("item_pc_target", 0)) 
        
        # ğŸ’¡ [é—œéµåŠŸèƒ½ï¼šPython è‡ªå‹•æŸ¥è¡¨è£œä½]
        # ä¸å†ä¾è³´ AI æŠ„éŒ„ï¼Œç›´æ¥å¾æ¨™é¡ŒåŒ¹é… Excel è£¡çš„ä¸‰å€‹æœƒè¨ˆæ¬„ä½
        matched_rule = {"local": "", "agg": "", "freight": ""}
        if df_rules is not None:
            for _, row in df_rules.iterrows():
                # ä½¿ç”¨æ¨™é¡Œæ¨¡ç³ŠåŒ¹é… Excel è£¡çš„ Item_Name
                if fuzz.partial_ratio(str(row.get('Item_Name', '')), title) >= 85:
                    matched_rule = {
                        "local": str(row.get('Unit_Rule_Local', '')),
                        "agg": str(row.get('Unit_Rule_Agg', '')),
                        "freight": str(row.get('Unit_Rule_Freight', ''))
                    }
                    break
        
        # ğŸ’¡ è§£é–‹æ•¸æ“šå­—ä¸² ds
        ds = str(item.get("ds", ""))
        data_list = [pair.split(":") for pair in ds.split("|") if ":" in pair]
        if not data_list: continue
        
        ids = [str(e[0]).strip() for e in data_list if len(e) > 0]
        id_counts = Counter(ids)

        # --- 2.1 å–®é … PC æ•¸æ ¸å° (å«å£è»Œç›¸å®¹é‚è¼¯) ---
        u_local = matched_rule["local"]
        is_body = "æœ¬é«”" in title
        is_journal = any(k in title for k in ["è»¸é ¸", "å…§å­”", "Journal"])
        
        # åˆ¤æ–·æ˜¯å¦ç‚ºé‡é‡è¨ˆä»¶ (KG)
        is_weight_mode = "KG" in title.upper() or target_pc > 100

        if is_weight_mode:
            current_sum = 0
            has_bad_sector = False
            for e in data_list:
                temp_val = safe_float(e[1])
                if temp_val == "BAD_DATA": has_bad_sector = True
                else: current_sum += temp_val
            actual_item_qty = current_sum
            if has_bad_sector:
                accounting_issues.append({
                    "page": page, "item": title, "issue_type": "âš ï¸æ•¸æ“šææ¯€",
                    "common_reason": "åŒ…å«ç„¡æ³•è¾¨è­˜çš„é‡é‡æ•¸æ“šï¼Œç¸½é‡å¯èƒ½ä¸æº–",
                    "failures": [{"id": "è­¦å‘Š", "val": "[!]", "calc": "æ•¸æ“šææ¯€"}]
                })
        else:
            # æ•¸é‡æ¨¡å¼ï¼š1SETæ›ç®—ã€æœ¬é«”å»é‡ã€å…¶é¤˜è¨ˆè¡Œ
            if "1SET=4PCS" in u_local: actual_item_qty = len(data_list) / 4
            elif "1SET=2PCS" in u_local: actual_item_qty = len(data_list) / 2
            elif is_body or "PC=PC" in u_local: actual_item_qty = len(set(ids)) 
            else: actual_item_qty = len(data_list)

        # å–®é …æ•¸é‡æ¯”å°
        if not is_weight_mode and actual_item_qty != target_pc and target_pc > 0:
            accounting_issues.append({
                "page": page, "item": title, "issue_type": "çµ±è¨ˆä¸ç¬¦(å–®é …)",
                "common_reason": f"è¦æ±‚ {target_pc}PCï¼Œå…§æ–‡æ ¸ç®—ç‚º {actual_item_qty}",
                "failures": [{"id": "æ¨™é¡Œç›®æ¨™", "val": target_pc}, {"id": "å…§æ–‡å¯¦éš›", "val": actual_item_qty}]
            })

        # --- 2.2 è»¸é ¸é‡è¤‡æ€§æª¢æŸ¥ ---
        if is_journal:
            for rid, count in id_counts.items():
                if count >= 3:
                    accounting_issues.append({
                        "page": page, "item": title, "issue_type": "ğŸ›‘ç·¨è™Ÿé‡è¤‡ç•°å¸¸",
                        "common_reason": f"ç·¨è™Ÿ {rid} å‡ºç¾ {count} æ¬¡ï¼Œè»¸é ¸é™ 2 æ¬¡",
                        "failures": [{"id": rid, "val": count, "calc": "ç¦æ­¢è¶…é2æ¬¡"}]
                    })

        # --- 2.3 ç¸½è¡¨å°å¸³ (Aèšåˆ/Bä¸€èˆ¬) ---
        u_agg_raw = matched_rule["agg"]
        agg_parts = [p.strip() for p in u_agg_raw.split(",")]
        is_exempt_from_basket = "è±å…" in agg_parts
        
        agg_multiplier = 1.0
        for p in agg_parts:
            conv = re.search(r"(\d+)SET=1PC", p)
            if conv: agg_multiplier = 1.0 / float(conv.group(1))

        for s_title, data in global_sum_tracker.items():
            is_rep = any(k in s_title for k in ["ROLLè»Šä¿®", "å†ç”Ÿ"])
            is_weld = "éŠ²è£œ" in s_title
            is_assem = any(k in s_title for k in ["æ‹†è£", "çµ„è£"])
            
            match = False
            # A æ¨¡å¼ (èšåˆç±ƒå­)
            if (is_rep or is_weld or is_assem) and not is_exempt_from_basket:
                if is_rep and any(k in title for k in ["æœªå†ç”Ÿ", "å†ç”Ÿ", "ç ”ç£¨", "è»Šä¿®"]): match = True
                elif is_weld and "éŠ²è£œ" in title: match = True
                elif is_assem and any(k in title for k in ["æ‹†è£", "çµ„è£", "çœŸåœ“åº¦"]): match = True
            # B æ¨¡å¼ (åå­—å°å¸³)
            if not match and fuzz.partial_ratio(s_title.upper(), title.upper()) > 90: match = True

            if match:
                val_for_agg = actual_item_qty * agg_multiplier
                data["actual"] += val_for_agg
                data["details"].append({"id": f"{title} (P.{page})", "val": val_for_agg, "calc": "è¨ˆå…¥ç¸½å¸³"})

        # --- 2.4 é‹è²»æ ¸å° (å‹•æ…‹è§£æ XPC=1) ---
        u_fr = matched_rule["freight"]
        if ("è¨ˆå…¥" in u_fr or (is_body and "æœªå†ç”Ÿ" in title)) and "è±å…" not in u_fr:
            fr_divisor = 1.0
            fr_match = re.search(r"(\d+)PC=1", u_fr)
            if fr_match: fr_divisor = float(fr_match.group(1))
            
            val_for_fr = actual_item_qty / fr_divisor
            freight_actual_sum += val_for_fr
            freight_details.append({"id": f"{title} (P.{page})", "val": val_for_fr, "calc": "è¨ˆå…¥é‹è²»"})

    # 3. çµç®—ç•°å¸¸å ±å‘Š
    for s_title, data in global_sum_tracker.items():
        if abs(data["actual"] - data["target"]) > 0.01 and data["target"] > 0:
            icon = "ğŸšš" if "é‹è²»" in s_title else "ğŸ”"
            accounting_issues.append({
                "page": "ç¸½è¡¨", "item": s_title, "issue_type": "çµ±è¨ˆä¸ç¬¦",
                "common_reason": f"æ¨™è¨» {data['target']} != åŠ ç¸½ {data['actual']}",
                "failures": [{"id": f"{icon} åŸºæº–", "val": data["target"]}] + data["details"] + [{"id": "ğŸ§® ç¸½è¨ˆ", "val": data["actual"]}]
            })

    if abs(freight_actual_sum - freight_target) > 0.01 and freight_target > 0:
        accounting_issues.append({
            "page": "ç¸½è¡¨", "item": "é‹è²»æ ¸å°", "issue_type": "çµ±è¨ˆä¸ç¬¦(é‹è²»)",
            "common_reason": f"åŸºæº– {freight_target} != åŠ ç¸½ {freight_actual_sum}",
            "failures": [{"id": "ğŸšš åŸºæº–", "val": freight_target}] + freight_details + [{"id": "ğŸ§® ç¸½è¨ˆ", "val": freight_actual_sum}]
        })
        
    return accounting_issues
    
def python_process_audit(dimension_data):
    """
    Python æµç¨‹ç¨½æ ¸å“¡ï¼šè·¨é é¢æª¢æŸ¥æ¯ä¸€æ”¯ç·¨è™Ÿçš„å°ºå¯¸æ¼”é€²æ˜¯å¦ç¬¦åˆç‰©ç†è¦å¾‹
    """
    process_issues = []
    roll_history = {} 
    import re
    if not dimension_data: return []

    # 1. å»ºç«‹ã€Œå·¥ä»¶å±¥æ­·è³‡æ–™åº«ã€
    for item in dimension_data:
        p_num = item.get("page", "?")
        ds = str(item.get("ds", ""))
        cat = str(item.get("category", "")).strip()
        title = str(item.get("item_title", ""))
        
        # è§£æå£“ç¸®å­—ä¸²
        pairs = [p.split(":") for p in ds.split("|") if ":" in p]
        for rid, val_str in pairs:
            # ğŸ’¡ å£è»Œåµæ¸¬ï¼šå¦‚æœæ•¸å€¼çœ‹ä¸æ¸…ï¼Œä¸åˆ—å…¥ä½éšæ¯”å°ï¼Œé¿å…èª¤åˆ¤
            if "[!]" in val_str: continue 
            
            try:
                # æå–ç´”æ•¸å­—
                val_match = re.findall(r"\d+\.?\d*", val_str)
                val = float(val_match[0]) if val_match else None
                if val is None: continue
                
                rid_clean = rid.strip()
                if rid_clean not in roll_history: roll_history[rid_clean] = []
                
                # å°‡é€™ç­†ç´€éŒ„å­˜é€²è©²ç·¨è™Ÿçš„å±¥æ­·ä¸­
                roll_history[rid_clean].append({
                    "process": cat, 
                    "val": val, 
                    "page": p_num, 
                    "title": title
                })
            except: continue

    # 2. å®šç¾©ç‰©ç†ä½éšæ¬Šé‡ (æ•¸å­—è¶Šå¤§ä»£è¡¨è£½ç¨‹è¶Šå¾Œæ®µ)
    # æ¬Šé‡è¦å‰‡ï¼šæœªå†ç”Ÿ(1) < ç ”ç£¨(2) < å†ç”Ÿ(3) < éŠ²è£œ(4)
    weights = {
        "æœªå†ç”Ÿæœ¬é«”": 1, 
        "è»¸é ¸æœªå†ç”Ÿ": 1, 
        "ç²¾åŠ å·¥å†ç”Ÿ": 3, 
        "éŠ²è£œ": 4
    }

    # 3. åŸ·è¡Œã€Œè·¨è£½ç¨‹æ¯”å°ã€
    for rid, records in roll_history.items():
        if len(records) < 2: continue # åªæœ‰ä¸€ç­†ç´€éŒ„ç„¡æ³•æ¯”å°
        
        # æŒ‰é ç¢¼æ’åºï¼Œæ¨¡æ“¬åŠ å·¥å…ˆå¾Œé †åº
        records.sort(key=lambda x: str(x['page']))
        
        for i in range(len(records) - 1):
            curr = records[i] # å‰ä¸€å€‹è£½ç¨‹
            nxt = records[i+1] # å¾Œä¸€å€‹è£½ç¨‹
            
            # ğŸ’¡ [ç´°ç¯€æ ¡æ­£]ï¼šå¦‚æœæ¨™é¡Œå«ã€Œç ”ç£¨ã€ï¼Œä½éšè¨­ç‚º 2
            w_curr = 2 if "ç ”ç£¨" in curr['title'] else weights.get(curr['process'], 3)
            w_nxt = 2 if "ç ”ç£¨" in nxt['title'] else weights.get(nxt['process'], 3)
            
            # ğŸ’¡ æ ¸å¿ƒåˆ¤å®šï¼šå¦‚æœå¾Œä¸€å€‹è£½ç¨‹çš„ä½éšæ¯”è¼ƒé«˜ï¼Œå°ºå¯¸ã€Œä¸æ‡‰ã€è®Šå°
            # (ä¾‹å¦‚ï¼šå†ç”Ÿè»Šä¿®å¾Œçš„å°ºå¯¸ç†è«–ä¸Šæ‡‰å¤§æ–¼æœªå†ç”Ÿæ™‚çš„å°ºå¯¸é–€æª»)
            if w_nxt > w_curr and nxt['val'] < curr['val']:
                process_issues.append({
                    "page": nxt['page'], 
                    "item": f"ç·¨è™Ÿ {rid} è·¨è£½ç¨‹ä½éšæª¢æŸ¥",
                    "issue_type": "ğŸ›‘æµç¨‹ç•°å¸¸(ä½éšè¡çª)",
                    "common_reason": f"å¾Œæ®µè£½ç¨‹å°ºå¯¸({nxt['val']})å°æ–¼å‰æ®µ({curr['val']})",
                    "failures": [{
                        "id": rid, 
                        "val": f"å¾Œæ®µ:{nxt['val']} < å‰æ®µ:{curr['val']}", 
                        "calc": "ä¸ç¬¦ç‰©ç†æ¼”é€²é‚è¼¯"
                    }],
                    "source": "ğŸ ç³»çµ±åˆ¤å®š"
                })
    return process_issues
    
# --- 6. æ‰‹æ©Ÿç‰ˆ UI èˆ‡ æ ¸å¿ƒåŸ·è¡Œé‚è¼¯ ---
st.title("ğŸ­ äº¤è²¨å–®ç¨½æ ¸")

data_source = st.radio(
    "è«‹é¸æ“‡è³‡æ–™ä¾†æºï¼š", 
    ["ğŸ“¸ ä¸Šå‚³ç…§ç‰‡", "ğŸ“‚ ä¸Šå‚³ JSON æª”", "ğŸ“Š ä¸Šå‚³ Excel æª”"], 
    horizontal=True
)

with st.container(border=True):
    # --- æƒ…æ³ A: ä¸Šå‚³ç…§ç‰‡ ---
    if data_source == "ğŸ“¸ ä¸Šå‚³ç…§ç‰‡":
        if st.session_state.get('source_mode') == 'json' or st.session_state.get('source_mode') == 'excel':
            st.session_state.photo_gallery = []
            st.session_state.source_mode = 'image'

        uploaded_files = st.file_uploader(
            "è«‹é¸æ“‡ JPG/PNG ç…§ç‰‡...", 
            type=['jpg', 'png', 'jpeg'], 
            accept_multiple_files=True, 
            key=f"uploader_{st.session_state.uploader_key}"
        )
        
        if uploaded_files:
            for f in uploaded_files: 
                if not any(x['file'].name == f.name for x in st.session_state.photo_gallery if x['file']):
                    st.session_state.photo_gallery.append({
                        'file': f, 
                        'table_md': None, 
                        'header_text': None,
                        'full_text': None,
                        'raw_json': None
                    })
            st.session_state.uploader_key += 1
            if st.session_state.enable_auto_analysis:
                st.session_state.auto_start_analysis = True
            components.html("""<script>window.parent.document.body.scrollTo(0, window.parent.document.body.scrollHeight);</script>""", height=0)
            st.rerun()

    # --- æƒ…æ³ B: ä¸Šå‚³ JSON ---
    elif data_source == "ğŸ“‚ ä¸Šå‚³ JSON æª”":
        st.info("ğŸ’¡ è«‹é»æ“Šä¸‹æ–¹æŒ‰éˆ•ï¼Œå¾ä½ çš„è³‡æ–™å¤¾é¸æ“‡ä¹‹å‰ä¸‹è¼‰çš„ `.json` æª”ã€‚")
        uploaded_json = st.file_uploader("ä¸Šå‚³JSONæª”", type=['json'], key="json_uploader")
        
        if uploaded_json:
            try:
                current_file_name = uploaded_json.name
                if st.session_state.get('last_loaded_json_name') != current_file_name:
                    json_data = json.load(uploaded_json)
                    st.session_state.photo_gallery = []
                    st.session_state.source_mode = 'json'
                    st.session_state.last_loaded_json_name = current_file_name
                    
                    import re
                    for page in json_data:
                        real_page = "Unknown"
                        full_text = page.get('full_text', '')
                        if full_text:
                            match = re.search(r"(?:é …æ¬¡|Page|é æ¬¡|NO\.)[:\s]*(\d+)\s*[/ï¼]\s*\d+", full_text, re.IGNORECASE)
                            if match:
                                real_page = match.group(1)
                        
                        st.session_state.photo_gallery.append({
                            'file': None,
                            'table_md': page.get('table_md'),
                            'header_text': page.get('header_text'),
                            'full_text': full_text,
                            'raw_json': page.get('raw_json'),
                            'real_page': real_page
                        })
                    
                    st.toast(f"âœ… æˆåŠŸè¼‰å…¥ JSON: {current_file_name}", icon="ğŸ“‚")
                    if st.session_state.enable_auto_analysis:
                        st.session_state.auto_start_analysis = True
                    st.rerun()
                else:
                    st.success(f"ğŸ“‚ ç›®å‰è¼‰å…¥ JSONï¼š**{uploaded_json.name}**")
            except Exception as e:
                st.error(f"JSON æª”æ¡ˆæ ¼å¼éŒ¯èª¤: {e}")

    # --- æƒ…æ³ C: ä¸Šå‚³ Excel (æ–°å¢çš„æ”¾åœ¨é€™) ---
    elif data_source == "ğŸ“Š ä¸Šå‚³ Excel æª”":
        st.info("ğŸ’¡ ä¸Šå‚³ Excel æª”å¾Œï¼Œç³»çµ±æœƒå°‡è¡¨æ ¼å…§å®¹è½‰æ›ç‚ºæ–‡å­—ä¾› AI ç¨½æ ¸ã€‚")
        uploaded_xlsx = st.file_uploader("ä¸Šå‚³ Excel æª”", type=['xlsx', 'xls'], key="xlsx_uploader")
        
        if uploaded_xlsx:
            try:
                current_file_name = uploaded_xlsx.name
                if st.session_state.get('last_loaded_xlsx_name') != current_file_name:
                    df_dict = pd.read_excel(uploaded_xlsx, sheet_name=None)
                    st.session_state.photo_gallery = []
                    st.session_state.source_mode = 'excel'
                    st.session_state.last_loaded_xlsx_name = current_file_name
                    
                    for sheet_name, df in df_dict.items():
                        df = df.fillna("")
                        md_table = df.to_markdown(index=False)
                        st.session_state.photo_gallery.append({
                            'file': None,
                            'table_md': md_table,
                            'header_text': f"ä¾†æºåˆ†é : {sheet_name}",
                            'full_text': f"Excel å…§å®¹ - åˆ†é  {sheet_name}\n" + md_table,
                            'raw_json': None,
                            'real_page': sheet_name
                        })
                    st.toast(f"âœ… æˆåŠŸè¼‰å…¥ Excel: {current_file_name}", icon="ğŸ“Š")
                    if st.session_state.enable_auto_analysis:
                        st.session_state.auto_start_analysis = True
                    st.rerun()
                else:
                    st.success(f"ğŸ“Š ç›®å‰è¼‰å…¥ Excelï¼š**{uploaded_xlsx.name}**")
            except Exception as e:
                st.error(f"Excel è®€å–å¤±æ•—: {e}")

if st.session_state.photo_gallery:
    st.caption(f"å·²ç´¯ç© {len(st.session_state.photo_gallery)} é æ–‡ä»¶")
    col_btn1, col_btn2 = st.columns([1, 1], gap="small")
    with col_btn1: start_btn = st.button("ğŸš€ é–‹å§‹åˆ†æ", type="primary", use_container_width=True)
    with col_btn2: 
        clear_btn = st.button("ğŸ—‘ï¸ç…§ç‰‡æ¸…é™¤", help="æ¸…é™¤", use_container_width=True)

    if clear_btn:
        st.session_state.photo_gallery = []
        st.session_state.analysis_result_cache = None
        if 'last_loaded_json_name' in st.session_state:
            del st.session_state.last_loaded_json_name 
        st.rerun()

    is_auto_start = st.session_state.auto_start_analysis
    if is_auto_start:
        st.session_state.auto_start_analysis = False

    if 'analysis_result_cache' not in st.session_state:
        st.session_state.analysis_result_cache = None

    trigger_analysis = start_btn or is_auto_start

    if trigger_analysis:
        st.session_state.auto_start_analysis = False
        total_start = time.time()
        
        # 1. åŸ·è¡Œåˆ†æå€å¡Š
        with st.status("ç¸½ç¨½æ ¸å®˜æ­£åœ¨é€²è¡Œå…¨æ–¹ä½åˆ†æ...", expanded=True) as status_box:
            status_text = st.empty()
            progress_bar = st.progress(0)
            total_imgs = len(st.session_state.photo_gallery)
            ocr_start = time.time()
            
            def process_task(index, item):
                if item.get('full_text'):
                    return index, item.get('header_text',''), item['full_text'], None
                try:
                    item['file'].seek(0)
                    _, h, f, _, _ = extract_layout_with_azure(item['file'], DOC_ENDPOINT, DOC_KEY)
                    return index, h, f, None
                except Exception as e:
                    return index, None, None, str(e)

            # æ•¸æ“šæ”¶é›†
            with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
                futures = [executor.submit(process_task, i, item) for i, item in enumerate(st.session_state.photo_gallery)]
                for future in concurrent.futures.as_completed(futures):
                    idx, h_txt, f_txt, err = future.result()
                    if not err:
                        st.session_state.photo_gallery[idx].update({'header_text': h_txt, 'full_text': f_txt, 'file': None})
                    progress_bar.progress((idx + 1) / total_imgs)

            ocr_duration = time.time() - ocr_start
            combined_input = ""
            for i, p in enumerate(st.session_state.photo_gallery):
                combined_input += f"\n=== Page {i+1} ===\n{p.get('full_text','')}\n"

            res_main = agent_unified_check(combined_input, combined_input, GEMINI_KEY, main_model_name)
            st.write("DEBUG - AI å›å‚³å…§å®¹:", res_main) # âš¡ï¸ è®“éŒ¯èª¤ç¾å½¢
            dim_data = res_main.get("dimension_data", [])
            python_numeric_issues = python_numerical_audit(dim_data)
            python_accounting_issues = python_accounting_audit(dim_data, res_main)
            python_process_issues = python_process_audit(dim_data)
            python_header_issues, python_debug_data = python_header_check(st.session_state.photo_gallery)

            ai_filtered_issues = []
            ai_raw_issues = res_main.get("issues", [])
            if isinstance(ai_raw_issues, list):
                for i in ai_raw_issues:
                    if isinstance(i, dict):
                        i['source'] = 'ğŸ¤– ç¸½ç¨½æ ¸ AI'
                        if any(k in i.get("issue_type", "") for k in ["æµç¨‹", "è¦æ ¼æå–å¤±æ•—", "æœªåŒ¹é…"]):
                            ai_filtered_issues.append(i)

            # æœ€çµ‚åˆä½µæ‰€æœ‰ç±ƒå­
            all_issues = ai_filtered_issues + python_numeric_issues + python_accounting_issues + python_process_issues + python_header_issues
            
            # å­˜å…¥å¿«å–
            usage = res_main.get("_token_usage", {"input": 0, "output": 0})
            st.session_state.analysis_result_cache = {
                "job_no": res_main.get("job_no", "Unknown"),
                "all_issues": all_issues,
                "total_duration": time.time() - total_start,
                "cost_twd": (usage.get("input", 0)*0.5 + usage.get("output", 0)*3.0) / 1000000 * 32.5,
                "total_in": usage.get("input", 0),
                "total_out": usage.get("output", 0),
                "ocr_duration": ocr_duration,
                "time_eng": time.time() - total_start - ocr_duration,
                "ai_extracted_data": dim_data,
                "python_debug_data": python_debug_data,
                "full_text_for_search": combined_input, # è£œå›é€™è¡Œä»¥å…å ±éŒ¯
                "combined_input": combined_input  # âœ… ç¢ºä¿é€™ä¸€è¡Œä¸€å®šè¦åœ¨ï¼
            }
            status_box.update(label="âœ… åˆ†æå®Œæˆï¼", state="complete", expanded=False)
            st.rerun()

    # --- ğŸ’¡ [é‡å¤§ä¿®æ­£] é¡¯ç¤ºçµæœå€å¡Šï¼šå¿…é ˆèˆ‡ if trigger_analysis å¹³ç´š ---
    if st.session_state.analysis_result_cache:
        cache = st.session_state.analysis_result_cache
        all_issues = cache.get('all_issues', [])
        
        st.success(f"å·¥ä»¤: {cache['job_no']} | â±ï¸ {cache['total_duration']:.1f}s")
        st.info(f"ğŸ’° æœ¬æ¬¡æˆæœ¬: NT$ {cache['cost_twd']:.2f} (In: {cache['total_in']:,} / Out: {cache['total_out']:,})")
        st.caption(f"ç´°ç¯€è€—æ™‚: Azure OCR {cache['ocr_duration']:.1f}s | AI åˆ†æ {cache['time_eng']:.1f}s")
        
        # å±•é–‹é é¢
        with st.expander("ğŸ” æŸ¥çœ‹ AI è®€å–åˆ°çš„ Excel è¦å‰‡ (Debug)"):
            rules_text = get_dynamic_rules(cache.get('full_text_for_search',''), debug_mode=True)
            st.markdown(rules_text)
                
        with st.expander("ğŸ”¬ æŸ¥çœ‹ AI æŠ„éŒ„åŸå§‹æ•¸æ“š", expanded=False):
            st.json(cache.get("ai_extracted_data", []))

        with st.expander("ğŸ æŸ¥çœ‹ Python ç¡¬é‚è¼¯åµæ¸¬çµæœ (Debug)", expanded=False):
            if cache.get('python_debug_data'):
                st.dataframe(cache['python_debug_data'], use_container_width=True, hide_index=True)
            else:
                st.caption("ç„¡åµæ¸¬è³‡æ–™")

        # åˆ¤å®šçµè«–é¡¯ç¤º
        real_errors = [i for i in all_issues if "æœªåŒ¹é…" not in i.get('issue_type', '')]
        if not all_issues:
            st.balloons()
            st.success("âœ… å…¨æ•¸åˆæ ¼ï¼")
        elif not real_errors:
            st.success(f"âœ… æ•¸å€¼åˆæ ¼ï¼ (ä½†æœ‰ {len(all_issues)} å€‹é …ç›®æœªåŒ¹é…è¦å‰‡)")
        else:
            st.error(f"ç™¼ç¾ {len(real_errors)} é¡ç•°å¸¸")

        # å¡ç‰‡å¾ªç’°é¡¯ç¤º
        for item in all_issues:
            with st.container(border=True):
                c1, c2 = st.columns([3, 1])
                source_label = item.get('source', '')
                issue_type = item.get('issue_type', 'ç•°å¸¸')
                c1.markdown(f"**P.{item.get('page', '?')} | {item.get('item')}**  `{source_label}`")
                
                if any(kw in issue_type for kw in ["çµ±è¨ˆ", "æ•¸é‡", "æµç¨‹"]):
                    c2.error(f"ğŸ›‘ {issue_type}")
                else:
                    c2.warning(f"âš ï¸ {issue_type}")
                
                st.caption(f"åŸå› : {item.get('common_reason', '')}")
                
                failures = item.get('failures', [])
                if failures:
                    table_data = []
                    for f in failures:
                        if isinstance(f, dict):
                            table_data.append({
                                "é …ç›®/ç·¨è™Ÿ": f.get('id', 'æœªçŸ¥'), 
                                "å¯¦æ¸¬/è¨ˆæ•¸": f.get('val', 'N/A'),
                                "æ¨™æº–/å‚™è¨»": f.get('target', ''),
                                "ç‹€æ…‹": f.get('calc', '')
                            })
                    st.dataframe(table_data, use_container_width=True, hide_index=True)
        
        st.divider()
        # ä¸‹è¼‰æŒ‰éˆ•èˆ‡åŸæ–‡å±•é–‹
        # ... (é€™è£¡æ¥ä½ åŸæœ¬å‰©ä¸‹çš„ä»£ç¢¼å³å¯ï¼Œä¹Ÿè¦è¨˜å¾—ç¸®æ’å¾€å·¦ç§»)
        current_job_no = cache.get('job_no', 'Unknown')
        safe_job_no = current_job_no.replace("/", "_").replace("\\", "_").strip()
        file_name_str = f"{safe_job_no}_cleaned.json"

        # æº–å‚™åŒ¯å‡ºè³‡æ–™
        export_data = []
        for item in st.session_state.photo_gallery:
            export_data.append({
                "table_md": item.get('table_md'),
                "header_text": item.get('header_text'),
                "full_text": item.get('full_text'),
                "raw_json": item.get('raw_json')
            })
        json_str = json.dumps(export_data, indent=2, ensure_ascii=False)

        st.subheader("ğŸ’¾ æ¸¬è©¦è³‡æ–™å­˜æª”")
        st.caption(f"å·²è­˜åˆ¥å·¥ä»¤ï¼š**{current_job_no}**ã€‚ä¸‹è¼‰å¾Œå¯ä¾›ä¸‹æ¬¡æ¸¬è©¦ä½¿ç”¨ã€‚")
        
        st.download_button(
            label=f"â¬‡ï¸ ä¸‹è¼‰æ¸¬è©¦è³‡æ–™ ({file_name_str})",
            data=json_str,
            file_name=file_name_str,
            mime="application/json",
            type="primary"
        )

        # ğŸ’¡ ä½¿ç”¨ .get() å¯ä»¥é˜²æ­¢å› ç‚ºæ‰¾ä¸åˆ°æ¨™ç±¤è€Œç›´æ¥å ±éŒ¯ç•¶æ©Ÿ
        with st.expander("ğŸ‘€ æŸ¥çœ‹å‚³çµ¦ AI çš„æœ€çµ‚æ–‡å­— (Prompt Input)"):
            st.caption("é€™æ‰æ˜¯ AI çœŸæ­£è®€åˆ°çš„å…§å®¹ (å·²éæ¿¾é›œè¨Š)ï¼š")
            st.code(cache.get('combined_input', 'ç„¡è³‡æ–™'), language='markdown')
    
    if st.session_state.photo_gallery and st.session_state.get('source_mode') != 'json':
        st.caption("å·²æ‹æ”ç…§ç‰‡ï¼š")
        cols = st.columns(4)
        for idx, item in enumerate(st.session_state.photo_gallery):
            with cols[idx % 4]:
                if item.get('file'):
                    st.image(item['file'], caption=f"P.{idx+1}", use_container_width=True)
                if st.button("âŒ", key=f"del_{idx}"):
                    st.session_state.photo_gallery.pop(idx)
                    st.session_state.analysis_result_cache = None
                    st.rerun()
else:
    st.info("ğŸ‘† è«‹é»æ“Šä¸Šæ–¹æŒ‰éˆ•é–‹å§‹æ–°å¢ç…§ç‰‡")
