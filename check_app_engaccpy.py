import streamlit as st
import streamlit.components.v1 as components
from azure.core.credentials import AzureKeyCredential
from azure.ai.documentintelligence import DocumentIntelligenceClient
from azure.ai.documentintelligence.models import AnalyzeResult
import google.generativeai as genai
from openai import OpenAI
import json
import time
import concurrent.futures
import pandas as pd
from thefuzz import fuzz
from collections import Counter
import re

# --- 1. é é¢è¨­å®š ---
st.set_page_config(page_title="äº¤è²¨å–®ç¨½æ ¸", page_icon="ğŸ­", layout="centered")

# --- CSS æ¨£å¼ ---
st.markdown("""
<style>
/* 1. æ¨™é¡Œå¤§å°æ§åˆ¶ */
h1 {
    font-size: 1.7rem !important; 
    white-space: nowrap !important;
    overflow: hidden !important; 
    text-overflow: ellipsis !important;
}

/* 2. ä¸»åŠŸèƒ½æŒ‰éˆ• (ç´…è‰² Primary) -> è®Šå¤§ã€è®Šé«˜ */
/* é€™æœƒå½±éŸ¿ã€Œé–‹å§‹åˆ†æã€å’Œã€Œç…§ç‰‡æ¸…é™¤ã€ */
button[kind="primary"] {
    height: 60px;               
    font-size: 20px !important; 
    font-weight: bold !important;
    border-radius: 10px !important;
    margin-top: 0px !important;    
    margin-bottom: 5px !important; 
    width: 100%;                
}

/* 3. æ¬¡è¦æŒ‰éˆ• (ç°è‰² Secondary) -> ä¿æŒåŸç‹€ */
/* é€™æœƒå½±éŸ¿æ¯ä¸€å¼µç…§ç‰‡ä¸‹é¢çš„ã€ŒXã€æŒ‰éˆ•ï¼Œè®“å®ƒç¶­æŒå°å°çš„ */
button[kind="secondary"] {
    height: auto !important;
    font-weight: normal !important;
}
</style>
""", unsafe_allow_html=True)
# --- 2. ç§˜å¯†é‡‘é‘°è®€å– ---
try:
    DOC_ENDPOINT = st.secrets["DOC_ENDPOINT"]
    DOC_KEY = st.secrets["DOC_KEY"]
    GEMINI_KEY = st.secrets["GEMINI_KEY"]
    OPENAI_KEY = st.secrets.get("OPENAI_KEY", "")
except:
    st.error("æ‰¾ä¸åˆ°é‡‘é‘°ï¼è«‹åœ¨ Streamlit Cloud è¨­å®š Secretsã€‚")
    st.stop()

# --- 3. åˆå§‹åŒ– Session State ---
if 'photo_gallery' not in st.session_state: st.session_state.photo_gallery = []
if 'uploader_key' not in st.session_state: st.session_state.uploader_key = 0
if 'auto_start_analysis' not in st.session_state: st.session_state.auto_start_analysis = False

# --- å´é‚Šæ¬„æ¨¡å‹è¨­å®š (åˆä½µç‚ºå–®ä¸€é¸æ“‡) ---
with st.sidebar:
    st.header("æ¨¡å‹è¨­å®š")
    
    # é€™è£¡åŠ å…¥æœ€æ–°çš„ Gemini æ¨¡å‹
    model_options = {
        "Gemini 3 Flash preview": "gemini-3-flash-preview",
        "Gemini 2.5 Flash": "models/gemini-2.5-flash",
        "Gemini 2.5 Pro": "models/gemini-2.5-pro",
        #"GPT-5(ç„¡æ•ˆ)": "models/gpt-5",
        #"GPT-5 Mini(ç„¡æ•ˆ)": "models/gpt-5-mini",
    }
    options_list = list(model_options.keys())
    
    st.subheader("ğŸ¤– ç¸½ç¨½æ ¸ Agent")
    model_selection = st.selectbox(
        "è² è²¬ï¼šè¦æ ¼ã€è£½ç¨‹ã€æ•¸é‡ã€çµ±è¨ˆå…¨åŒ…", 
        options=options_list, 
        index=0, 
        key="main_model"
    )
    main_model_name = model_options[model_selection]
    
    st.divider()
    
    default_auto = st.query_params.get("auto", "true") == "true"
    def update_url_param():
        current_state = "true" if st.session_state.enable_auto_analysis else "false"
        st.query_params["auto"] = current_state

    st.toggle(
        "âš¡ ä¸Šå‚³å¾Œè‡ªå‹•åˆ†æ", 
        value=default_auto, 
        key="enable_auto_analysis", 
        on_change=update_url_param
    )

# --- Excel è¦å‰‡è®€å–å‡½æ•¸ (å–®ä¸€ä»£ç†æ•´åˆç‰ˆ) ---
@st.cache_data
def get_dynamic_rules(ocr_text, debug_mode=False):
    try:
        df = pd.read_excel("rules.xlsx")
        df.columns = [c.strip() for c in df.columns]
        ocr_text_clean = str(ocr_text).upper().replace(" ", "").replace("\n", "")
        specific_rules = []

        for index, row in df.iterrows():
            item_name = str(row.get('Item_Name', '')).strip()
            # ğŸ’¡ è·³éåŸæœ¬çš„ã€Œ(é€šç”¨)ã€é …ç›®ï¼ŒåªæŠ“ç‰¹è¦
            if not item_name or "(é€šç”¨)" in item_name: continue
            
            # ä½¿ç”¨æ¨¡ç³ŠåŒ¹é…åˆ¤æ–·æ˜¯å¦ç‚ºç•¶å‰è™•ç†çš„é …ç›®
            score = fuzz.partial_ratio(item_name.upper().replace(" ", ""), ocr_text_clean)
            if score >= 85:
                # æå–ç‰¹è¦è³‡è¨Š
                spec = str(row.get('Standard_Spec', ''))
                logic = str(row.get('Logic_Prompt', ''))
                u_local = str(row.get('Unit_Rule_Local', ''))
                u_agg = str(row.get('Unit_Rule_Agg', ''))
                u_freight = str(row.get('Unit_Rule_Freight', ''))
                
                desc = f"- **[ç‰¹å®šé …ç›®è¦å‰‡] {item_name}**\n"
                if spec != 'nan' and spec: desc += f"  - [å¼·åˆ¶è¦æ ¼]: {spec}\n"
                if logic != 'nan' and logic: desc += f"  - [ä¾‹å¤–æŒ‡ä»¤]: {logic}\n"
                if u_local != 'nan' and u_local: desc += f"  - [æœƒè¨ˆå–®é …]: {u_local}\n"
                if u_agg != 'nan' and u_agg: desc += f"  - [æœƒè¨ˆèšåˆ]: {u_agg}\n"
                if u_freight != 'nan' and u_freight: desc += f"  - [æœƒè¨ˆé‹è²»]: {u_freight}\n"
                specific_rules.append(desc)
        
        return "\n".join(specific_rules) if specific_rules else "ç„¡ç‰¹å®šå°ˆæ¡ˆè¦å‰‡ï¼Œè«‹ä¾ç…§é€šç”¨æ†²æ³•åŸ·è¡Œã€‚"
    except Exception as e:
        return f"è®€å–è¦å‰‡æª”æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}"
        
# --- 4. æ ¸å¿ƒå‡½æ•¸ï¼šAzure ç¥ä¹‹çœ¼ ---
def extract_layout_with_azure(file_obj, endpoint, key):
    client = DocumentIntelligenceClient(endpoint=endpoint, credential=AzureKeyCredential(key))
    file_content = file_obj.getvalue()
    
    poller = client.begin_analyze_document("prebuilt-layout", file_content, content_type="application/octet-stream")
    result: AnalyzeResult = poller.result()
    
    markdown_output = ""
    full_content_text = ""
    real_page_num = "Unknown"
    
    bottom_stop_keywords = ["æ³¨æ„äº‹é …", "ä¸­æ©Ÿå“æª¢å–®ä½", "ä¿å­˜æœŸé™", "è¡¨å–®ç·¨è™Ÿ", "FORM NO", "ç°½ç« "]
    top_right_noise_keywords = [
        "æª¢é©—é¡åˆ¥", "å°ºå¯¸æª¢é©—", "ä¾åœ–é¢æ¨™è¨˜", "ææ–™æª¢é©—", "æˆä»½åˆ†æ", 
        "éç ´å£æ€§", "æ­£å¸¸åŒ–", "é€€ç«", "æ·¬.å›ç«", "è¡¨é¢ç¡¬åŒ–", "è©¦è»Š",
        "æ€§èƒ½æ¸¬è©¦", "è©¦å£“è©¦æ¼", "å‹•.éœå¹³è¡¡è©¦é©—", ":selected:", ":unselected:",
        "æŠ—æ‹‰", "ç¡¬åº¦è©¦é©—", "UT", "PT", "MT"
    ]
    
    if result.tables:
        for idx, table in enumerate(result.tables):
            page_num = "Unknown"
            if table.bounding_regions: page_num = table.bounding_regions[0].page_number
            markdown_output += f"\n### Table {idx + 1} (Page {page_num}):\n"
            rows = {}
            stop_processing_table = False 
            
            for cell in table.cells:
                if stop_processing_table: break
                content = cell.content.replace("\n", " ").strip()
                
                for kw in bottom_stop_keywords:
                    if kw in content:
                        stop_processing_table = True
                        break
                if stop_processing_table: break
                
                is_noise = False
                for kw in top_right_noise_keywords:
                    if kw in content:
                        is_noise = True
                        break
                if is_noise: content = "" 

                r, c = cell.row_index, cell.column_index
                if r not in rows: rows[r] = {}
                rows[r][c] = content
            
            for r in sorted(rows.keys()):
                row_cells = []
                if rows[r]:
                    max_col = max(rows[r].keys())
                    for c in range(max_col + 1): 
                        row_cells.append(rows[r].get(c, ""))
                    markdown_output += "| " + " | ".join(row_cells) + " |\n"
    
    if result.content:
        match = re.search(r"(?:é …æ¬¡|Page|é æ¬¡|NO\.)[:\s]*(\d+)\s*[/ï¼]\s*\d+", result.content, re.IGNORECASE)
        if match:
            real_page_num = match.group(1)

        cut_index = len(result.content)
        for keyword in bottom_stop_keywords:
            idx = result.content.find(keyword)
            if idx != -1 and idx < cut_index:
                cut_index = idx
        
        temp_text = result.content[:cut_index]
        for noise in top_right_noise_keywords:
            temp_text = temp_text.replace(noise, "")
            
        full_content_text = temp_text
        header_snippet = full_content_text[:800]
    else:
        full_content_text = ""
        header_snippet = ""

    return markdown_output, header_snippet, full_content_text, None, real_page_num

# --- Python ç¡¬é‚è¼¯ï¼šè¡¨é ­ä¸€è‡´æ€§æª¢æŸ¥ (é•·åº¦æ•æ„Ÿç‰ˆ) ---
def python_header_check(photo_gallery):
    issues = []
    if not photo_gallery:
        return issues, []

    # å®šç¾© Regex (é‡å° "å»ç©ºç™½+å»æ›è¡Œ" å¾Œçš„å­—ä¸²è¨­è¨ˆ)
    patterns = {
        # ã€ä¿®æ”¹é» 1ã€‘å·¥ä»¤ Regex æ”¾å¯¬ï¼š
        # åŸæœ¬åªæŠ“ W é–‹é ­ï¼Œç¾åœ¨æ”¹æŠ“ "ç·¨è™Ÿ" å¾Œé¢æ¥çš„ "ä»»ä½•è‹±æ•¸å­—ä¸²"
        # é€™æ¨£å°±ç®—å®ƒå¯« WW363... æˆ–æ˜¯ 12345... éƒ½èƒ½æ•´ä¸²æŠ“å‡ºä¾†æ¯”å°
        "å·¥ä»¤ç·¨è™Ÿ": r"[å·¥åœŸä¸‹][ä»¤å†·ä»Š]ç·¨è™Ÿ[:\.]*([A-Za-z0-9\-\_]+)", 
        
        "é å®šäº¤è²¨": r"[é é¢„é …é ‚][å®šäº¤].*?(\d{2,4}[\.\-/]\d{1,2}[\.\-/]\d{1,2})",
        "å¯¦éš›äº¤è²¨": r"[å¯¦çœŸ][éš›äº¤].*?(\d{2,4}[\.\-/]\d{1,2}[\.\-/]\d{1,2})"
    }

    extracted_data = [] 
    all_values = {key: [] for key in patterns}

    for i, page in enumerate(photo_gallery):
        # æš´åŠ›æ¸…æ´—ï¼šå»æ›è¡Œã€å»ç©ºæ ¼ã€è½‰å¤§å¯«
        raw_text = page.get('header_text', '') + page.get('full_text', '')
        clean_text = raw_text.replace("\n", "").replace(" ", "").replace("\r", "").upper()
        
        # ã€ä¿®æ”¹é» 2ã€‘é ç¢¼é˜²å‘†ï¼šç¢ºä¿ä¸€å®šæœ‰å€¼
        # å„ªå…ˆæŠ“ real_pageï¼ŒæŠ“ä¸åˆ°å°±ç”¨ index
        r_page = page.get('real_page')
        if not r_page or r_page == "Unknown":
            page_label = f"P.{i + 1}"
        else:
            page_label = f"P.{r_page}"
            
        page_result = {"é æ•¸": page_label}
        
        for key, pattern in patterns.items():
            match = re.search(pattern, clean_text)
            if match:
                val = match.group(1).strip()
                
                # ã€ä¿®æ”¹é» 3ã€‘é‡å°å·¥ä»¤çš„ç‰¹æ®Šè™•ç† (å¦‚æœå¤ªé•·å¯èƒ½å°±æ˜¯é‡è¤‡æ‰“å­—)
                if key == "å·¥ä»¤ç·¨è™Ÿ":
                    # å¦‚æœä½ ç¢ºå®šå·¥ä»¤åªæœ‰ 10 ç¢¼ï¼Œä½†æŠ“åˆ°äº† 11 ç¢¼ä»¥ä¸Š (å¦‚ WW...)
                    # æˆ‘å€‘ä¿ç•™é€™å€‹éŒ¯èª¤çš„å€¼ï¼Œè®“å¾Œé¢çš„å¤šæ•¸æ±ºå»æŠŠå®ƒæªå‡ºä¾†
                    pass 
                
                page_result[key] = val
                all_values[key].append(val)
            else:
                page_result[key] = "N/A"
        
        extracted_data.append(page_result)

    # æ­¥é©Ÿ 2: æ±ºå®šã€Œæ­£ç¢ºæ¨™æº–ã€ (ä½¿ç”¨å¤šæ•¸æ±º)
    standard_data = {}
    for key, values in all_values.items():
        if values:
            # æ¿¾æ‰ N/A å¾Œå†æŠ•ç¥¨
            valid_values = [v for v in values if v != "N/A"]
            if valid_values:
                most_common = Counter(valid_values).most_common(1)[0][0]
                standard_data[key] = most_common
            else:
                standard_data[key] = "N/A"
        else:
            standard_data[key] = "N/A"

    # æ­¥é©Ÿ 3: æ¯”å°æ¯ä¸€é 
    for data in extracted_data:
        page_num = data['é æ•¸']
        
        for key, standard_val in standard_data.items():
            current_val = data[key]
            
            if standard_val == "N/A": continue # å…¨å·éƒ½æ²’æŠ“åˆ°å°±ä¸æ¯”äº†

            # é–‹å§‹æ¯”å° (å­—ä¸²ä¸ç›¸ç­‰)
            if current_val != standard_val:
                
                # åˆ¤æ–·æ˜¯å¦ç‚ºé•·åº¦ç•°å¸¸ (é‡å°å·¥ä»¤)
                reason = "èˆ‡å…¨å·å¤šæ•¸é é¢ä¸ä¸€è‡´"
                if key == "å·¥ä»¤ç·¨è™Ÿ" and len(current_val) != len(standard_val):
                    reason += f" (é•·åº¦ç•°å¸¸: {len(current_val)}ç¢¼ vs æ¨™æº–{len(standard_val)}ç¢¼)"

                issue = {
                    "page": page_num.replace("P.", ""),
                    "item": f"è¡¨é ­æª¢æŸ¥-{key}",
                    "rule_used": "Pythonç¡¬é‚è¼¯æª¢æŸ¥",
                    "issue_type": "è·¨é è³‡è¨Šä¸ç¬¦",
                    "spec_logic": f"æ‡‰ç‚º {standard_val}",
                    "common_reason": reason,
                    "failures": [
                        {"id": "å…¨å·åŸºæº–", "val": standard_val, "calc": "å¤šæ•¸æ±ºæ¨™æº–"},
                        {"id": f"æœ¬é ({page_num})", "val": current_val, "calc": "ç•°å¸¸/æ¼æŠ“"}
                    ],
                    "source": "ğŸ¤– ç³»çµ±è‡ªå‹•"
                }
                issues.append(issue)
                
    return issues, extracted_data

    # --- 5. ç¸½ç¨½æ ¸ Agent (æ•´åˆç‰ˆ - å¼·é‚è¼¯å„ªåŒ–) ---
def agent_unified_check(combined_input, full_text_for_search, api_key, model_name):
    # è®€å– Excel è¦å‰‡
    dynamic_rules = get_dynamic_rules(full_text_for_search)

    system_prompt = f"""
    ä½ æ˜¯ä¸€ä½æ¥µåº¦åš´è¬¹çš„ä¸­é‹¼æ©Ÿæ¢°å“ç®¡ã€ç¸½ç¨½æ ¸å®˜ã€‘ã€‚ä½ å¿…é ˆåƒã€Œé›»è…¦ç¨‹å¼ã€ä¸€æ¨£åŸ·è¡Œä»¥ä¸‹é›™æ¨¡çµ„ç¨½æ ¸ï¼Œç¦æ­¢ä»»ä½•ä¸»è§€è§£é‡‹ã€‚
    
    {dynamic_rules}

    ---

    #### âš”ï¸ æ¨¡çµ„ Aï¼šå·¥ç¨‹å°ºå¯¸æ•¸æ“šæå– (AI ç¿»è­¯å®˜ä»»å‹™)
    
    1. **è¦æ ¼æå–è­¦å ±èˆ‡å®Œæ•´æ€§ (é‡è¦)**ï¼š
       - **å¤šé‡ç›®æ¨™è™•ç†**ï¼šè‹¥è¦æ ¼å…§åŒ…å«å¤šå€‹ã€Œç›®æ¨™æ•¸å€¼ã€ï¼ˆå¦‚ï¼šé©…å‹•ç«¯ 157mm / éé©…å‹•ç«¯ 127mmï¼‰ï¼Œè«‹å°‡æ‰€æœ‰ç›®æ¨™æ•¸å­—å…¨éƒ¨å¡«å…¥ `threshold_list`ï¼Œä¸¦å–æœ€å¤§å€¼å¡«å…¥ `threshold`ã€‚é€™ **ä¸æ˜¯** æå–å¤±æ•—ã€‚
       - **å ±è­¦æ©Ÿåˆ¶**ï¼šè‹¥è¦æ ¼æ–‡å­—ä¸­æœ‰æ•¸å­—ï¼Œä½†ä½ å®Œå…¨ç„¡æ³•è§£æï¼ˆæå–å¾Œ threshold ç‚º 0 æˆ– nullï¼‰ï¼Œä½ ã€Œå¿…é ˆã€åœ¨ `issues` æ¸…å–®å›å ± `ğŸ›‘è¦æ ¼æå–å¤±æ•—`ã€‚åªè¦æœ‰æŠ“åˆ°ä»»ä½•ä¸€å€‹ç›®æ¨™æ•¸å­—ï¼Œåš´ç¦å ±éŒ¯ã€‚
       
    2. **ç›®æ¨™è¦æ ¼è§£æ (mm å®šä½èˆ‡é›œè¨Šéæ¿¾)**ï¼š
       - **âœ… å¿…æŠ“ (ç›®æ¨™å°ºå¯¸)**ï¼šå„ªå…ˆå°‹æ‰¾èˆ‡ã€Œè‡³...ã€ã€ã€Œä»¥ä¸Šã€ã€ã€ŒÂ±ã€ã€ã€Œ~ã€ã€ã€Œç›´å¾‘ã€ç›´æ¥é—œè¯çš„æ•¸å­—ã€‚
       - **âŒ æ’é™¤ (åŠ å·¥é‡é›œè¨Š)**ï¼šåš´ç¦æå–ã€Œæ¯æ¬¡è»Šä¿®...ã€ã€ã€Œé€²åˆ€é‡...ã€ã€ã€ŒåŠ å·¥é‡...ã€å¾Œé¢çš„å°æ•¸å­—ï¼ˆå¦‚ 0.5~5mmï¼‰ã€‚
       - **ğŸ“ æœ¬é«”æœªå†ç”Ÿåº•ç·š**ï¼šé‡å°ã€Œæœ¬é«”ã€æœªå†ç”Ÿé …ç›®ï¼Œå…¶ç›®æ¨™é–€æª»ï¼ˆthresholdï¼‰**çµ•å°ä¸æœƒå°æ–¼ 120mm**ã€‚è«‹è‡ªå‹•å¿½ç•¥æ¨™é¡Œæˆ–è¦æ ¼ä¸­ä»»ä½•å°æ–¼ 120 çš„æ•¸å­—ï¼ˆå¦‚ #1æ©Ÿã€é …æ¬¡2ã€è»Šä¿®3mmï¼‰ã€‚
       - **å€é–“è¨ˆç®—**ï¼šè‹¥æœ‰ `Â±` æˆ–åå·®ï¼Œå¿…é ˆå…ˆç®—å‡ºæœ€çµ‚ç¯„åœã€‚å¦‚ `300Â±0.1` -> `[[299.9, 300.1]]`ã€‚
    
    3. **é …ç›®åˆ†é¡æ±ºç­–æµç¨‹ (ç”±ä¸Šè‡³ä¸‹åŸ·è¡Œï¼Œå‘½ä¸­å³åœæ­¢)**ï¼š
       - **LEVEL 1 (æœ€é«˜å„ªå…ˆ)ï¼šéŠ²è£œåˆ¤å®š**
         * æ¨™é¡Œå«ã€ŒéŠ²è£œã€ã€ã€ŒéŠ²æ¥ã€ -> åˆ†é¡å¿…ç‚º `min_limit`ã€‚
         * (è¨»ï¼šå³ä¾¿æ¨™é¡Œå«è»¸é ¸æˆ–æœªå†ç”Ÿï¼Œåªè¦æœ‰éŠ²è£œï¼Œä»¥æ­¤ç‚ºæº–)ã€‚
         
       - **LEVEL 2ï¼šæœªå†ç”Ÿåˆ¤å®š**
         * æ¨™é¡Œå«ã€Œæœªå†ç”Ÿã€æ™‚ï¼Œé€²è¡ŒäºŒé¸ä¸€ï¼š
           a. å«ã€Œè»¸é ¸ã€ -> åˆ†é¡å¿…ç‚º `max_limit`ã€‚ (ğŸ’¡ æç¤ºï¼šå³ä¾¿æœ‰é©…å‹•/éé©…å‹•å¤šå€‹æ•¸å­—ï¼Œä¹Ÿè«‹å…¨éƒ¨æ”¾å…¥ threshold_listï¼Œä¸å‡†è®Šæ›´ç‚º range)ã€‚
           b. ä¸å«ã€Œè»¸é ¸ã€(æœ¬é«”) -> åˆ†é¡å¿…ç‚º `un_regen`ã€‚
         * (âš ï¸ è­¦å‘Šï¼šåš´ç¦å› è¦æ ¼æ–‡å­—å«ã€Œå†ç”Ÿã€è€Œå°‡å…¶æ­¸é¡ç‚º range)ã€‚
         
       - **LEVEL 3ï¼šç²¾åŠ å·¥èˆ‡è£é…åˆ¤å®š**
         * æ¨™é¡Œã€Œä¸å«æœªå†ç”Ÿã€ï¼Œä¸”åŒ…å«ã€Œå†ç”Ÿã€ã€ã€Œç ”ç£¨ã€ã€ã€Œç²¾åŠ å·¥ã€ã€ã€Œè»Šä¿®åŠ å·¥ã€ã€ã€Œçµ„è£ã€ã€ã€Œæ‹†è£ã€ã€ã€ŒçœŸåœ“åº¦ã€ã€ã€ŒKEYWAYã€-> åˆ†é¡å¿…ç‚º `range`ã€‚
         * (ğŸ’¡ æç¤ºï¼šé€™é¡é …ç›®è¦æ±‚å…©ä½å°æ•¸ï¼Œè¦æ ¼å¤šä»¥ã€Œå€é–“ã€(å¦‚ 129~135) æˆ–ã€ŒÂ± å…¬å·®ã€å‘ˆç¾ï¼Œè«‹å‹™å¿…ç²¾ç¢ºç®—å‡º std_ranges)ã€‚
    
    4. **æ•¸æ“šæŠ„éŒ„ (å­—ä¸²ä¿è­·æ¨¡å¼)**ï¼š
       - **ç¦æ­¢ç°¡åŒ–**ï¼šå¯¦æ¸¬å€¼è‹¥é¡¯ç¤º `349.90`ï¼Œå¿…é ˆè¼¸å‡º `"349.90"`ã€‚ç¦æ­¢å¯«æˆ `349.9`ã€‚
       - **æ ¼å¼**ï¼šæ‰€æœ‰å¯¦æ¸¬å€¼å¿…é ˆåŒ…è£¹æˆé›™å¼•è™Ÿå­—ä¸²ã€‚`["RollID", "å¯¦æ¸¬å€¼å­—ä¸²"]`ã€‚
    #### ğŸš« æ•¸æ“šæŠ„éŒ„ç´”æ·¨åŒ–æŒ‡ä»¤ (æ ¸å¿ƒç¦ä»¤)ï¼š
       - **å­—é«”è¾¨è­˜å„ªå…ˆ**ï¼šåˆ©ç”¨è¦–è¦ºèƒ½åŠ›å€åˆ†ã€ŒåŸå§‹æ‰“å°å­—é«”ã€èˆ‡ã€Œæ‰‹å¯«ç­†è·¡ã€ã€‚
       - **çµ•å°å¿½ç•¥æ‰‹å¯«**ï¼šåš´ç¦æŠ„éŒ„ä»»ä½•æ‰‹å¯«çš„æ•¸å­—ã€ç®­é ­ç¬¦è™Ÿ (->)ã€åˆªé™¤ç·šã€åœ“åœˆæ¨™è¨˜ã€å‹¾é¸ç¬¦è™Ÿã€ç°½åæˆ–æ—¥æœŸã€‚
       - **å”¯ä¸€æ•¸æ“šä¾†æº**ï¼šåƒ…æå–å„²å­˜æ ¼å…§ã€ŒåŸå§‹æ‰“å°ã€çš„æ•¸å€¼ã€‚è‹¥å„²å­˜æ ¼å› æ‰‹å¯«æ¨™è¨»ç”¢ç”Ÿæ··äº‚å­—ä¸²ï¼ˆå¦‚ "129.93 -> 129.94"ï¼‰ï¼Œä½ å¿…é ˆç„¡è¦–æ‰‹å¯«éƒ¨åˆ†ï¼Œåƒ…è¼¸å‡ºæ‰“å°çš„å­—ä¸² `"129.93"`ã€‚
       - **ç¦æ­¢æè¿°é›œè¨Š**ï¼šä¸è¦åœ¨ JSON å…§å®¹ä¸­å˜—è©¦è§£é‡‹æˆ–æè¿°æ‰‹å¯«çš„æ›´æ­£å…§å®¹ã€‚
    
    5. **å°ºå¯¸å¤§å°é‚è¼¯æª¢æŸ¥**ï¼š
       - **ç‰©ç†ä½éšæº–å‰‡**ï¼š`æœªå†ç”Ÿè»Šä¿® < ç ”ç£¨ < å†ç”Ÿè»Šä¿® < éŠ²è£œ`ã€‚
       - **åˆ¤å®šè¦æ±‚**ï¼šåˆ¤å®šè¦æ±‚ï¼šé‡å°åŒä¸€ Roll IDï¼Œè·¨è£½ç¨‹ä¹‹å°ºå¯¸å¤§å°å¿…é ˆç¬¦åˆä¸Šè¿°ä½éšé‚è¼¯ã€‚æ³¨æ„ï¼šåŒä¸€ç·¨è™Ÿå‡ºç¾åœ¨ä¸åŒé …ç›®è¡¨æ ¼ä¸­ä»£è¡¨ã€Œå…¨æµç¨‹ç´€éŒ„ã€ï¼Œå±¬æ–¼æ­£å¸¸ç¾è±¡ï¼Œåš´ç¦åˆ¤å®šç‚ºè¡çªã€‚ åƒ…åœ¨ä½éšä¸ç¬¦ï¼ˆä¾‹å¦‚ï¼šç ”ç£¨å°ºå¯¸å¤§æ–¼å†ç”Ÿè»Šä¿®ï¼‰æ™‚ï¼Œæ‰å›å ± ğŸ›‘æµç¨‹ç•°å¸¸ã€‚
        
    #### ğŸ’° æ¨¡çµ„ Bï¼šæœƒè¨ˆèˆ‡æµç¨‹æ•¸æ“šæå– (AI ä»»å‹™ï¼šæŠ„éŒ„å‚³ç¥¨)
    **ã€é‡è¦ç¦ä»¤ã€‘ï¼šåš´ç¦åœ¨æ­¤åˆ¤æ–·ã€Œæ•¸é‡æ˜¯å¦æ­£ç¢ºã€ï¼Œè¨ˆç®—å·¥ä½œç”±ç³»çµ±å¾Œå°åŸ·è¡Œã€‚**
    **æå–å·¦ä¸Šè§’ã€çµ±è¨ˆè¡¨ã€‘**ï¼šå¿…é ˆæŠ„éŒ„çµ±è¨ˆè¡¨æ¯ä¸€è¡Œï¼ˆåŒ…å«ç†±è™•ç†ã€æ‹†è£ã€è»Šä¿®ç­‰ï¼‰ã€‚
    **æŠ„éŒ„å‚³ç¥¨ (æ ¸å¿ƒè¦æ±‚)**ï¼š
       - **åš´ç¦éºæ¼**ï¼šé é¢ä¸­è‹¥æœ‰ã€Œå¤šå€‹ç¨ç«‹æ¨™é¡Œã€çš„è¡¨æ ¼ï¼ˆä¾‹å¦‚å…ˆæ‹†è£ 170ã€å†æ‹†è£ 200ï¼‰ï¼Œä½ å¿…é ˆå°‡å®ƒå€‘è¦–ç‚ºã€Œä¸åŒçš„é …ç›®ã€åˆ†åˆ¥æŠ„éŒ„åˆ° `dimension_data`ã€‚
       - **ç¦æ­¢åˆä½µ**ï¼šå³ä¾¿é …ç›®åç¨±ç›¸ä¼¼ï¼Œåªè¦ä½ç½®ä¸åŒï¼Œå°±å¿…é ˆåˆ†æˆå¤šå€‹ç‰©ä»¶å›å‚³ã€‚

    1. **æå–å·¦ä¸Šè§’ã€çµ±è¨ˆè¡¨ã€‘(Summary Table)**ï¼š
       - è«‹å°‡çµ±è¨ˆè¡¨ï¼ˆå·¦ä¸Šè§’ï¼‰ä¸­æ¯ä¸€è¡ŒåŒ…å«ã€Œå¯¦äº¤æ•¸é‡ã€çš„é …ç›®æå–å‡ºä¾†ã€‚
       - **æ ¼å¼**ï¼š`summary_rows: [ {{ "title": "é …ç›®åç¨±", "target": æ•¸å­— }}, ... ]`
       - **æå–é‹è²»**ï¼šå–®ç¨æå–å·¦ä¸Šè§’é‹è²»é …æ¬¡çš„æ•¸å­—åˆ° `freight_target`ã€‚

    2. **å…§æ–‡é …ç›®å±¬æ€§æŠ„éŒ„**ï¼š
       - **item_pc_target**: æå–é …ç›®æ‹¬è™Ÿå…§çš„æ•¸å­—ï¼ˆå¦‚ 12PC æå– 12ï¼‰ã€‚
       - **accounting_rules**: å¿…é ˆç²¾ç¢ºæŠ„éŒ„ Excel çŸ¥è­˜åº«ä¸­çš„ `Unit_Rule_Local` (å–®é …)ã€`Unit_Rule_Agg` (èšåˆ)ã€`Unit_Rule_Freight` (é‹è²») æ–‡å­—ã€‚
       - **ç‰¹åˆ¥è¦æ±‚**ï¼šè‹¥ `Unit_Rule_Agg` åŒ…å«å¤šå€‹è³‡è¨Šï¼ˆå¦‚ã€Œè±å…, 2SET=1PCã€ï¼‰ï¼Œå¿…é ˆã€ŒåŸå°ä¸å‹•ã€å…¨éƒ¨æŠ„éŒ„ä¸¦ä»¥é€—è™Ÿéš”é–‹ã€‚ç¦æ­¢è‡ªè¡Œåˆªæ¸›æ–‡å­—ã€‚

    3. **å·¥ä»¶æµç¨‹èˆ‡å°ºå¯¸ä½éšæª¢æŸ¥ (ç”± AI åˆ¤å®šä¸¦å ±æ–¼ issues)**ï¼š
       - **ä½éš**ï¼š`æœªå†ç”Ÿ < ç ”ç£¨ < å†ç”Ÿ < éŠ²è£œ`ã€‚è‹¥å¾Œæ®µå°ºå¯¸å°æ–¼å‰æ®µï¼ˆéŠ²è£œé™¤å¤–ï¼‰ï¼Œå ± `ğŸ›‘æµç¨‹ç•°å¸¸`ã€‚
       - **æº¯æºèˆ‡é‡è¤‡æ€§**ï¼šå‡ºç¾ã€Œç ”ç£¨/å†ç”Ÿã€å¿…é ˆå¾€å‰æª¢æŸ¥æ˜¯å¦æœ‰å‰æ®µç´€éŒ„ã€‚
       - **ç‰¹åˆ¥æ³¨æ„**ï¼šåŒä¸€ç·¨è™Ÿåœ¨ä¸åŒé …ç›®ä¸­å¤šæ¬¡å‡ºç¾æ˜¯ã€Œå…¨è£½ç¨‹ç´€éŒ„ã€ï¼Œå®Œå…¨åˆæ³•ï¼Œ**ä¸å‡†å›å ±ã€ŒåŒæ™‚å­˜åœ¨ã€æˆ–ã€Œç‰©ç†æµç¨‹è¡çªã€**ã€‚
    
    4. **âš–ï¸ æµç¨‹ç¨½æ ¸ç´”æ·¨åŒ–æŒ‡ä»¤ï¼š
       - **ç„¡è¦–æ‰‹å¯«æ„è¦‹**ï¼šåœ¨åˆ¤æ–·ã€Œç‰©ç†ä½éšã€èˆ‡ã€Œå·¥ä»¶æº¯æºã€æ™‚ï¼Œåƒ…ä¾æ“šè¡¨æ ¼å…§çš„æ‰“å°æ•¸æ“šã€‚
       - **å¿½ç•¥æ¨™è¨˜é›œè¨Š**ï¼šåš´ç¦å› ç‚ºæ•¸æ“šæ—é‚Šæœ‰æ‰‹å¯«çš„ã€ŒOKã€ã€ã€Œåˆæ ¼ã€æˆ–ã€Œç®­é ­ã€è€Œå½±éŸ¿åˆ¤å®šã€‚
       - **é–å®šæ‰“å°äº‹å¯¦**ï¼šå³ä½¿æ‰‹å¯«æ›´æ­£å¾Œçš„æ•¸å­—çœ‹èµ·ä¾†æ›´åˆç†ï¼Œä½ ä¹Ÿå¿…é ˆã€Œä»¥åŸå§‹æ‰“å°æ•¸å€¼ã€ä½œç‚ºåˆ¤å®šç‰©ç†é‚è¼¯çš„å”¯ä¸€ä¾æ“šã€‚

    ---

    ### ğŸ“ è¼¸å‡ºè¦ç¯„ (Output Format)
    å¿…é ˆå›å‚³å–®ä¸€ JSONã€‚`issues` åƒ…å­˜æ”¾ï¼šæµç¨‹ç•°å¸¸ã€è¦æ ¼æå–å¤±æ•—ã€è¡¨é ­ä¸ä¸€ã€‚æ•¸é‡èˆ‡çµ±è¨ˆç•°å¸¸ç”±ç³»çµ±è‡ªå‹•ç”¢å‡ºï¼Œä¸å‡†å¡«å…¥ `issues`ã€‚

    {{
      "job_no": "å·¥ä»¤ç·¨è™Ÿ",
      "summary_rows": [
         {{ "title": "çµ±è¨ˆè¡¨é …ç›®åç¨±", "target": "å¯¦äº¤æ•¸é‡æ•¸å­—" }} 
      ], // ğŸ’¡ å¿…é ˆæŠ„éŒ„å·¦ä¸Šè§’çµ±è¨ˆè¡¨çš„ã€Œæ¯ä¸€è¡Œã€æ•¸æ“š
      "freight_target": 0, // ğŸ’¡ å·¦ä¸Šè§’é‹è²»é …æ¬¡çš„æ•¸å­—
      "issues": [ 
         {{
           "page": "é ç¢¼", "item": "é …ç›®", "issue_type": "çµ±è¨ˆä¸ç¬¦ / ğŸ›‘æµç¨‹ç•°å¸¸ / ğŸ›‘è¦æ ¼æå–å¤±æ•—",
           "common_reason": "åŸå› ",
           "failures": [
              {{ "id": "ğŸ” çµ±è¨ˆç¸½å¸³åŸºæº–", "val": "æ•¸", "calc": "ç›®æ¨™" }},
              {{ "id": "é …ç›® (P.é ç¢¼)", "val": "æ•¸", "calc": "è¨ˆå…¥" }},
              {{ "id": "ğŸ§® å…§æ–‡å¯¦éš›åŠ ç¸½", "val": "æ•¸", "calc": "è¨ˆç®—" }}
           ]
         }}
      ],
      "dimension_data": [
         {{
           "page": "æ•¸å­—",
           "item_title": "åç¨±",
           "category": "åˆ†é¡",
           "item_pc_target": 0, // é …ç›®æ‹¬è™Ÿå…§çš„ PC æ•¸
           "accounting_rules": {{ "local": "", "agg": "", "freight": "" }}, // ğŸ’¡ å¾Excelç²¾ç¢ºæŠ„éŒ„
           "standard_logic": {{
              "logic_type": "å¿…é ˆå¾ [range, un_regen, min_limit, max_limit] é¸ä¸€å¡«å…¥", 
              "threshold_list": [], // è¦æ ¼ä¸­å‡ºç¾çš„æ‰€æœ‰æ•¸å­—
              "ranges_list": [],    // AI é ç®—å¥½çš„ [[min, max]]
              "threshold": 0        // ä¸»è¦çš„é–€æª»æ•¸å­—ï¼Œåš´ç¦å¡« 0 (è‹¥æ¨™é¡Œæœ‰æ•¸å­—)
           }},
           "std_spec": "å« mm çš„åŸå§‹è¦æ ¼æ–‡å­—",
           "data": [ ["RollID", "å¯¦æ¸¬å€¼å­—ä¸²"] ] // ğŸ’¡ å‹™å¿…ä¿ç•™æœ«å°¾çš„ 0ï¼Œå¦‚ "349.90"
         }}
      ]
    }}

    #### ğŸ’¡ AI ç¿»è­¯å®˜ç¯„ä¾‹ (ç¦æ­¢æŠ„è¥²æ•¸å­—ï¼Œé ˆæŠ“å–ç•¶å‰æ¨™é¡ŒçœŸå¯¦æ•¸å­—)ï¼š
    1. range: å¦‚ `XXXÂ±YYY` -> {{ "logic_type": "range", "min": XXX-YYY, "max": XXX+YYY }}
    2. un_regen: å¦‚ `è‡³ XXXmm å†ç”Ÿ` -> {{ "logic_type": "un_regen", "threshold": XXX }}
    3. min_limit: å¦‚ `XXXmm ä»¥ä¸Š` -> {{ "logic_type": "min_limit", "min": XXX }}
    4. max_limit: å¦‚ `XXXmm ä»¥ä¸‹` -> {{ "logic_type": "max_limit", "max": XXX }}
    """
    
    generation_config = {"response_mime_type": "application/json", "temperature": 0.0, "top_k": 1, "top_p": 0.95}
    
    try:
        if "gemini" in model_name.lower():
            genai.configure(api_key=api_key)
            model = genai.GenerativeModel(model_name)
            response = model.generate_content([system_prompt, combined_input], generation_config=generation_config)
            raw_content = response.text
            usage_meta = response.usage_metadata
            usage_in = usage_meta.prompt_token_count if usage_meta else 0
            usage_out = usage_meta.candidates_token_count if usage_meta else 0
        else:
            client = OpenAI(api_key=OPENAI_KEY)
            response = client.chat.completions.create(
                model=model_name,
                messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": combined_input}],
                temperature=0.0
            )
            raw_content = response.choices[0].message.content
            usage_in = response.usage.prompt_tokens
            usage_out = response.usage.completion_tokens

        # JSON æ¸…æ´—
        if "```json" in raw_content:
            raw_content = raw_content.replace("```json", "").replace("```", "")
        elif "```" in raw_content:
            raw_content = raw_content.replace("```", "")
            
        try:
            parsed_data = json.loads(raw_content)
        except:
            parsed_data = {"job_no": "JSON Error", "issues": []}

        final_response = parsed_data if isinstance(parsed_data, dict) else {"job_no": "Unknown", "issues": []}
        if "issues" not in final_response: final_response["issues"] = []
        if "job_no" not in final_response: final_response["job_no"] = "Unknown"

        valid_issues = []
        for i in final_response["issues"]:
            if isinstance(i, dict) and i.get("item"):
                reason = i.get("common_reason", "")
                i_type = i.get("issue_type", "")
                if "åˆæ ¼" in reason and "æœªåŒ¹é…" not in i_type: continue
                if "åˆæ ¼" in reason and "æœªåŒ¹é…" in i_type: i["issue_type"] = "âš ï¸æœªåŒ¹é…è¦å‰‡"
                valid_issues.append(i)
        
        final_response["issues"] = valid_issues
        final_response["_token_usage"] = {"input": usage_in, "output": usage_out}
        
        return final_response

    except Exception as e:
        return {"job_no": "Error", "issues": [{"item": "System Error", "common_reason": str(e)}], "_token_usage": {"input": 0, "output": 0}}
        
# --- é‡é»ï¼šPython å¼•æ“ç¨ç«‹æ–¼ agent å‡½å¼ä¹‹å¤– ---
def python_numerical_audit(dimension_data):
    grouped_errors = {} # æ”¹ç”¨å­—å…¸ä¾†é€²è¡Œåˆ†é¡æ”¶é›†
    import re
    if not dimension_data: return [] # ä¿®æ­£ï¼šè‹¥ç„¡è³‡æ–™å›å‚³ç©ºæ¸…å–®

    for item in dimension_data:
        raw_data_list = item.get("data", [])
        title = item.get("item_title", "")
        cat = str(item.get("category", "")).strip()
        page_num = item.get("page", "?")
        raw_spec = str(item.get("std_spec", ""))
        
        # --- ğŸ›¡ï¸ æ•¸æ“šæ¸…æ´—èˆ‡ã€Œæ¨¡å¼å„ªå…ˆã€é è§£æ (ä¿ç•™æ‚¨çš„å®Œæ•´é‚è¼¯) ---
        trusted_stds = [] 
        logic = item.get("standard_logic", {})
        s_ranges = logic.get("ranges_list", []) if logic.get("ranges_list") else item.get("std_ranges", [])
        
        # 1. æŠ“å–ç·Šè²¼ "mm" çš„æ•¸å­—
        mm_nums = [float(n) for n in re.findall(r"(\d+\.?\d*)\s*mm", raw_spec)]
        trusted_stds.extend(mm_nums)

        # 2. è§£æ Â± æˆ–åå·®çµæ§‹
        pm_match = re.findall(r"(\d+\.?\d*)\s*[Â±]\s*(\d+\.?\d*)", raw_spec)
        for base, offset in pm_match:
            b, o = float(base), float(offset)
            s_ranges.append([b - o, b + o])
            trusted_stds.extend([b, b-o, b+o])

        # 3. åŸ·è¡Œé›œè¨Šéæ¿¾
        all_nums = [float(n) for n in re.findall(r"(\d+\.?\d*)", raw_spec)]
        noise = [350.0, 300.0, 200.0, 145.0, 130.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0]
        clean_std = [n for n in all_nums if (n in trusted_stds) or (n not in noise and n > 5)]

        # ç²å– AI å‚³ä¾†çš„é‚è¼¯åƒæ•¸
        l_type = logic.get("logic_type")
        s_list = logic.get("threshold_list", [])
        s_threshold = logic.get("threshold")

        for entry in raw_data_list:
            if not isinstance(entry, list) or len(entry) < 2: continue
            
            # 1. å…ˆæŠ“å– AI æŠ„éŒ„ä¸‹ä¾†çš„åŸå§‹å­—ä¸²ï¼ˆå¯èƒ½åŒ…å«æ‰‹å¯«é›œè¨Šå¦‚ "129.93 -> 129.94"ï¼‰
            rid, val_raw = str(entry[0]).strip(), str(entry[1]).strip()
            if not val_raw or val_raw in ["N/A", "nan", "M10"]: continue

            try:
                # ğŸ’¡ [æ ¸å¿ƒä¿®æ”¹é»]ï¼šåªæŠ“å–å­—ä¸²ä¸­çš„ç¬¬ä¸€å€‹æ•¸å­—ï¼Œç„¡è¦–å¾Œé¢çš„å¡—æ”¹
                # ä½¿ç”¨ re.findall æ‰¾å‡ºæ‰€æœ‰ç¬¦åˆæ•¸å­—æ ¼å¼çš„å…§å®¹ï¼Œå–ç´¢å¼• [0] çš„é‚£ä¸€å€‹
                val_match = re.findall(r"\d+\.?\d*", val_raw)
                val_str = val_match[0] if val_match else val_raw 

                # 2. æ¥ä¸‹ä¾†çš„åˆ¤å®šéƒ½ä½¿ç”¨é€™å€‹ä¹¾æ·¨çš„ val_str
                val = float(val_str)
                # ğŸ’¡ ç²¾ç¢ºæª¢æŸ¥ï¼šå¿…é ˆå«å°æ•¸é»ä¸”å¾Œç¶´é•·åº¦ç‚º 2 (ä»æœƒæª¢æŸ¥ 349.90 çš„çµå°¾ 0)
                is_two_dec = "." in val_str and len(val_str.split(".")[-1]) == 2
                is_pure_int = "." not in val_str
                is_passed, reason, t_used, engine_label = True, "", "N/A", "æœªçŸ¥"
                
                # ... (ä¸‹æ–¹å¾ŒçºŒé‚è¼¯å®Œå…¨ä¸ç”¨å‹•) ...
                # --- ğŸ’¡ [æ ¸å¿ƒä¿®æ­£]ï¼šé‡æ–°æ’åˆ—åˆ¤å®šå„ªå…ˆåºï¼Œè§£æ±ºé—œéµå­—ç¢°æ’ ---

                # 1. ã€éŠ²è£œæ¨¡å¼ã€‘å„ªå…ˆæ¬Šæœ€é«˜
                if l_type == "min_limit" or "éŠ²è£œ" in (cat + title):
                    engine_label = "éŠ²è£œ(ä¸‹é™)"
                    if not is_pure_int:
                        is_passed, reason = False, "éŠ²è£œæ ¼å¼éŒ¯èª¤: æ‡‰ç‚ºç´”æ•´æ•¸"
                    elif clean_std:
                        t_used = min(clean_std, key=lambda x: abs(x - val))
                        if val < t_used: is_passed, reason = False, f"éŠ²è£œä¸è¶³: å¯¦æ¸¬ {val} < åŸºæº– {t_used}"

                # 2. ã€æœªå†ç”Ÿæ¨¡å¼ã€‘(åŒ…å«æœ¬é«”èˆ‡è»¸é ¸) å„ªå…ˆæ–¼ç²¾åŠ å·¥
                elif l_type in ["un_regen", "max_limit"] or "æœªå†ç”Ÿ" in (cat + title):
                    # åˆ†æ”¯ A: è»¸é ¸æœªå†ç”Ÿ (max_limit)
                    if "è»¸é ¸" in (cat + title):
                        engine_label = "è»¸é ¸(ä¸Šé™)"
                        # 1. æ”¶é›†æ‰€æœ‰å¯èƒ½çš„æ•¸å­—æ¨™æº–
                        candidates = [float(n) for n in (clean_std + s_list)]
                        if s_threshold: candidates.append(float(s_threshold))
                        
                        # 2. ğŸ›¡ï¸ å®‰å…¨é–ï¼šå¦‚æœå®Œå…¨æ²’æŠ“åˆ°åŸºæº–æ•¸å­—ï¼Œç›´æ¥è·³éåˆ¤å®šï¼Œä¸å‡†ç”¨ 0 åˆ¤æ–·
                        if not candidates or max(candidates) == 0:
                            continue 

                        target = max(candidates)
                        t_used = target
                        
                        # 3. åŸ·è¡Œåˆ¤å®šé‚è¼¯
                        if not is_pure_int: 
                            is_passed, reason = False, "è»¸é ¸æ ¼å¼éŒ¯èª¤: æ‡‰ç‚ºç´”æ•´æ•¸"
                        elif val > target: 
                            is_passed, reason = False, f"è¶…éä¸Šé™ {target}"
                    
                    # åˆ†æ”¯ B: æœ¬é«”æœªå†ç”Ÿ (un_regen)
                    else:
                        engine_label = "æœªå†ç”Ÿ(æœ¬é«”)"
                        candidates = [float(n) for n in (clean_std + s_list) if float(n) >= 120.0]
                        if s_threshold and float(s_threshold) >= 120.0: candidates.append(float(s_threshold))
                        
                        if candidates:
                            if not candidates: continue
                            target = max(candidates)
                            t_used = target
                            if val <= target:
                                if not is_pure_int: is_passed, reason = False, f"æœªå†ç”Ÿ(<=æ¨™æº–{target}): æ‡‰ç‚ºæ•´æ•¸"
                            else:
                                if not is_two_dec: is_passed, reason = False, f"æœªå†ç”Ÿ(>æ¨™æº–{target}): æ‡‰å¡«å…©ä½å°æ•¸(å«æœ«å°¾0)"
                        else:
                            is_passed = True # æ²’æŠ“åˆ°120ä»¥ä¸Šæ¨™æº–å‰‡ä¸åˆ¤å®š

                # 3. ã€ç²¾åŠ å·¥/å†ç”Ÿ/è»Šä¿®/çµ„è£æ¨¡å¼ã€‘æœ€å¾Œåˆ¤å®š
                elif l_type == "range" or any(x in (cat + title) for x in ["å†ç”Ÿ", "ç²¾åŠ å·¥", "ç ”ç£¨", "è»Šä¿®", "çµ„è£", "æ‹†è£", "çœŸåœ“åº¦"]):
                    engine_label = "ç²¾åŠ å·¥(å€é–“)"
                    if not is_two_dec:
                        is_passed, reason = False, "ç²¾åŠ å·¥æ ¼å¼éŒ¯èª¤: æ‡‰å¡«å…©ä½å°æ•¸(å¦‚.90)"
                    elif s_ranges:
                        t_used = str(s_ranges)
                        is_passed = any(r[0] <= val <= r[1] for r in s_ranges if len(r)==2)
                        if not is_passed: reason = f"å°ºå¯¸ä¸åœ¨å€é–“ {t_used} å…§"
                    elif clean_std:
                        s_min, s_max = min(clean_std), max(clean_std)
                        t_used = f"{s_min}~{s_max}"
                        if not (s_min <= val <= s_max): is_passed, reason = False, f"ä¸åœ¨ç¯„åœå…§ {t_used}"

                # ğŸ’¡ [åˆä½µå¡ç‰‡èˆ‡æ¨¡å¼é¡¯ç¤º]
                if not is_passed:
                    # ä½¿ç”¨ engine_label è®“ç•«é¢é¡¯ç¤ºæ›´æ¸…æ¥š
                    error_key = (page_num, title, reason)
                    if error_key not in grouped_errors:
                        grouped_errors[error_key] = {
                            "page": page_num,
                            "item": title,
                            "issue_type": f"æ•¸å€¼ç•°å¸¸({engine_label})",
                            "rule_used": f"Excel: {raw_spec}",
                            "common_reason": reason,
                            "failures": [],
                            "source": "ğŸ ç³»çµ±åˆ¤å®š"
                        }
                    grouped_errors[error_key]["failures"].append({
                        "id": rid, 
                        "val": val_str, 
                        "target": f"åŸºæº–:{t_used}", 
                        "calc": f"âš–ï¸ {engine_label} å¼•æ“"
                    })
            except: continue
            
    return list(grouped_errors.values())
    
def python_accounting_audit(dimension_data, res_main):
    
    #Python æœƒè¨ˆå®˜ï¼š
    #1. å…¨é …ç›®å–®é …æ ¸å° (æœ¬é«”å»é‡/è»¸é ¸è¨ˆè¡Œ)
    #2. è»¸é ¸ç·¨è™Ÿé‡è¤‡æ€§ç›£æ§ (é™2æ¬¡)
    #3. ç¸½è¡¨å°å¸³ (Aèšåˆ/Bä¸€èˆ¬é›™æ¨¡å¼)
    #4. é‹è²»å‹•æ…‹ç²¾ç®— (æ”¯æ´ XPC=1 æ›ç®—)
    #5. æ”¯æ´ Agg Rule æ··åˆæŒ‡ä»¤ (è±å…ç±ƒå­, å–®ä½æ›ç®—)
    
    accounting_issues = []
    from thefuzz import fuzz
    from collections import Counter
    import re
    
    # --- 1. å–å¾—å°å¸³åŸºæº– (ä¾†è‡ªå·¦ä¸Šè§’çµ±è¨ˆè¡¨) ---
    summary_rows = res_main.get("summary_rows", [])
    # ğŸ’¡ é—œéµä¿®æ­£ï¼šå»ºç«‹ç¸½è¡¨è¿½è¹¤å™¨ï¼Œä¸¦åŸ·è¡Œã€Œå­—ä¸²è½‰æ•¸å­—ã€å®‰å…¨éæ¿¾
    global_sum_tracker = {}
    for s in summary_rows:
        s_title = s.get('title', 'Unknown')
        s_target_raw = s.get('target', 0)
        try:
            # è™•ç†å¯èƒ½å«é€—è™Ÿçš„å­—ä¸²å¦‚ "4,524"
            s_target = float(str(s_target_raw).replace(',', '').strip())
        except:
            s_target = 0
        global_sum_tracker[s_title] = {"target": s_target, "actual": 0, "details": []}

    # ğŸ’¡ å–å¾—é‹è²»åŸºæº–æ•¸å­—
    freight_target_raw = res_main.get("freight_target", 0)
    try:
        freight_target = float(str(freight_target_raw).replace(',', '').strip())
    except:
        freight_target = 0

    # --- 2. é–‹å§‹é€é …éæ­·å…§æ–‡æ•¸æ“š ---
    for item in dimension_data:
        title = item.get("item_title", "")
        page = item.get("page", "?")
        rules = item.get("accounting_rules", {})
        data_list = item.get("data", []) # æ ¼å¼: [["ID", "Val"], ...]
        
        # å–å¾—æ‰€æœ‰ ID çš„æ¸…å–® (æ¸…æ´—)
        ids = [str(e[0]).strip() for e in data_list if e and len(e) > 0]
        id_counts = Counter(ids)

        # ğŸ’¡ [2.1 å–®é … PC æ•¸æ ¸å°] 
        try:
            target_pc = float(str(item.get("item_pc_target", 0)))
        except:
            target_pc = 0
            
        u_local = str(rules.get("local", "")) if rules.get("local") else ""
        is_body = "æœ¬é«”" in title
        is_journal = any(k in title for k in ["è»¸é ¸", "å…§å­”", "Journal"])
        
        # è¨ˆç®—å¯¦éš›æ•¸é‡ï¼š1SET=4PCS, 1SET=2PCS, æœ¬é«”å»é‡, å…¶é¤˜è¨ˆè¡Œæ•¸
        if "1SET=4PCS" in u_local: 
            actual_item_qty = len(data_list) / 4
        elif "1SET=2PCS" in u_local: 
            actual_item_qty = len(data_list) / 2
        elif is_body or "PC=PC" in u_local: 
            actual_item_qty = len(set(ids)) # å»é‡
        else: 
            actual_item_qty = len(data_list) # è¨ˆè¡Œ

        if actual_item_qty != target_pc and target_pc > 0:
            accounting_issues.append({
                "page": page, "item": title, "issue_type": "çµ±è¨ˆä¸ç¬¦(å–®é …)",
                "common_reason": f"æ¨™é¡Œè¦æ±‚ {target_pc}PCï¼Œå…§æ–‡æ ¸ç®—ç‚º {actual_item_qty}",
                "failures": [
                    {"id": f"é …ç›®æ¨™é¡Œç›®æ¨™", "val": target_pc, "calc": "ç›®æ¨™"},
                    {"id": "å…§æ–‡å¯¦éš›è¨ˆæ•¸", "val": actual_item_qty, "calc": "å¯¦éš›"}
                ],
                "source": "ğŸ æœƒè¨ˆå¼•æ“"
            })

        # ğŸ’¡ [2.2 è»¸é ¸ä¸‰æ”¯ç¦ä»¤]
        if is_journal:
            for rid, count in id_counts.items():
                if count >= 3:
                    accounting_issues.append({
                        "page": page, "item": title, "issue_type": "ğŸ›‘ç·¨è™Ÿé‡è¤‡ç•°å¸¸",
                        "common_reason": f"ç·¨è™Ÿ {rid} å‡ºç¾ {count} æ¬¡ï¼Œé•åè»¸é ¸é™2æ¬¡è¦å®š",
                        "failures": [{"id": rid, "val": f"{count} æ¬¡", "calc": "ç¦æ­¢è¶…é2æ¬¡"}],
                        "source": "ğŸ æœƒè¨ˆå¼•æ“"
                    })

        # ğŸ’¡ [2.3 ç¸½è¡¨èˆ‡é‹è²»å°å¸³]
        # è§£æ Agg è¦å‰‡ (æ”¯æ´ è±å…, 2SET=1PC æ··åˆæ ¼å¼)
        u_agg_raw = str(rules.get("agg", "")).strip()
        agg_parts = [p.strip() for p in u_agg_raw.split(",")]
        is_exempt_from_baskets = "è±å…" in agg_parts
        
        agg_multiplier = 1.0
        for p in agg_parts:
            conv_match = re.search(r"(\d+)SET=1PC", p)
            if conv_match: agg_multiplier = 1.0 / float(conv_match.group(1))

        for s_title, data in global_sum_tracker.items():
            u_freight = str(rules.get("freight", "")) if rules.get("freight") else ""
            is_freight_row = "é‹è²»" in s_title
            
            match = False
            current_add_val = actual_item_qty # é è¨­

            if is_freight_row:
                # ğŸšš é‹è²»æ¨¡å¼
                if "è±å…" in u_freight: continue
                elif "è¨ˆå…¥" in u_freight: match = True
                elif is_body and "æœªå†ç”Ÿ" in title: match = True
                
                if match:
                    # å‹•æ…‹æ›ç®—ï¼šæ”¯æ´ 2PC=1, 3PC=1...
                    conv = re.search(r"(\d+)PC=1", u_freight)
                    if conv: current_add_val = actual_item_qty / int(conv.group(1))
            else:
                # ğŸ“¦ ç¸½è¡¨æ ¸å° (A/Bé›™æ¨¡å¼)
                is_repair = any(k in s_title for k in ["ROLLè»Šä¿®", "å†ç”Ÿ"])
                is_weld   = "éŠ²è£œ" in s_title
                is_assem  = any(k in s_title for k in ["æ‹†è£", "çµ„è£", "è£é…"])
                is_basket_row = is_repair or is_weld or is_assem

                if is_basket_row:
                    # Aæ¨¡å¼ (èšåˆç±ƒå­)ï¼šå—ã€Œè±å…ã€æ¨™ç±¤å½±éŸ¿
                    if is_exempt_from_baskets:
                        match = False
                    else:
                        if is_repair and any(k in title for k in ["æœªå†ç”Ÿ", "å†ç”Ÿ", "ç ”ç£¨", "è»Šä¿®"]): match = True
                        elif is_weld and "éŠ²è£œ" in title: match = True
                        elif is_assem and any(k in title for k in ["æ‹†è£", "çµ„è£", "çœŸåœ“åº¦"]): match = True
                
                # Bæ¨¡å¼ (ä¸€èˆ¬æ ¸å°)ï¼šåå­—å°ä¸Šå°±é»è²¨ï¼Œä¸å—ã€Œè±å…ã€å½±éŸ¿
                if not match and fuzz.partial_ratio(s_title, title) > 85:
                    match = True

                if match:
                    current_add_val = actual_item_qty * agg_multiplier

            if match:
                data["actual"] += current_add_val
                label = "è¨ˆå…¥é‹è²»" if is_freight_row else "è¨ˆå…¥ç¸½å¸³"
                data["details"].append({"id": f"{title} (P.{page})", "val": current_add_val, "calc": label})

    # --- 3. çµç®—ç•°å¸¸å ±å‘Š ---
    for s_title, data in global_sum_tracker.items():
        if abs(data["actual"] - data["target"]) > 0.01 and data["target"] > 0:
            icon = "ğŸšš" if "é‹è²»" in s_title else "ğŸ”"
            accounting_issues.append({
                "page": "ç¸½è¡¨", "item": s_title, "issue_type": "çµ±è¨ˆä¸ç¬¦",
                "common_reason": f"æ¨™è¨» {data['target']} != å…§æ–‡åŠ ç¸½ {data['actual']}",
                "failures": [{"id": f"{icon} çµ±è¨ˆåŸºæº–", "val": data["target"], "calc": "ç›®æ¨™"}] + data["details"] + [{"id": "ğŸ§® å¯¦éš›ç¸½è¨ˆ", "val": data["actual"], "calc": "è¨ˆç®—"}],
                "source": "ğŸ æœƒè¨ˆå¼•æ“"
            })
        
    return accounting_issues
    
# --- 6. æ‰‹æ©Ÿç‰ˆ UI èˆ‡ æ ¸å¿ƒåŸ·è¡Œé‚è¼¯ ---
st.title("ğŸ­ äº¤è²¨å–®ç¨½æ ¸")

data_source = st.radio(
    "è«‹é¸æ“‡è³‡æ–™ä¾†æºï¼š", 
    ["ğŸ“¸ ä¸Šå‚³ç…§ç‰‡", "ğŸ“‚ ä¸Šå‚³ JSON æª”", "ğŸ“Š ä¸Šå‚³ Excel æª”"], 
    horizontal=True
)

with st.container(border=True):
    # --- æƒ…æ³ A: ä¸Šå‚³ç…§ç‰‡ ---
    if data_source == "ğŸ“¸ ä¸Šå‚³ç…§ç‰‡":
        if st.session_state.get('source_mode') == 'json' or st.session_state.get('source_mode') == 'excel':
            st.session_state.photo_gallery = []
            st.session_state.source_mode = 'image'

        uploaded_files = st.file_uploader(
            "è«‹é¸æ“‡ JPG/PNG ç…§ç‰‡...", 
            type=['jpg', 'png', 'jpeg'], 
            accept_multiple_files=True, 
            key=f"uploader_{st.session_state.uploader_key}"
        )
        
        if uploaded_files:
            for f in uploaded_files: 
                if not any(x['file'].name == f.name for x in st.session_state.photo_gallery if x['file']):
                    st.session_state.photo_gallery.append({
                        'file': f, 
                        'table_md': None, 
                        'header_text': None,
                        'full_text': None,
                        'raw_json': None
                    })
            st.session_state.uploader_key += 1
            if st.session_state.enable_auto_analysis:
                st.session_state.auto_start_analysis = True
            components.html("""<script>window.parent.document.body.scrollTo(0, window.parent.document.body.scrollHeight);</script>""", height=0)
            st.rerun()

    # --- æƒ…æ³ B: ä¸Šå‚³ JSON ---
    elif data_source == "ğŸ“‚ ä¸Šå‚³ JSON æª”":
        st.info("ğŸ’¡ è«‹é»æ“Šä¸‹æ–¹æŒ‰éˆ•ï¼Œå¾ä½ çš„è³‡æ–™å¤¾é¸æ“‡ä¹‹å‰ä¸‹è¼‰çš„ `.json` æª”ã€‚")
        uploaded_json = st.file_uploader("ä¸Šå‚³JSONæª”", type=['json'], key="json_uploader")
        
        if uploaded_json:
            try:
                current_file_name = uploaded_json.name
                if st.session_state.get('last_loaded_json_name') != current_file_name:
                    json_data = json.load(uploaded_json)
                    st.session_state.photo_gallery = []
                    st.session_state.source_mode = 'json'
                    st.session_state.last_loaded_json_name = current_file_name
                    
                    import re
                    for page in json_data:
                        real_page = "Unknown"
                        full_text = page.get('full_text', '')
                        if full_text:
                            match = re.search(r"(?:é …æ¬¡|Page|é æ¬¡|NO\.)[:\s]*(\d+)\s*[/ï¼]\s*\d+", full_text, re.IGNORECASE)
                            if match:
                                real_page = match.group(1)
                        
                        st.session_state.photo_gallery.append({
                            'file': None,
                            'table_md': page.get('table_md'),
                            'header_text': page.get('header_text'),
                            'full_text': full_text,
                            'raw_json': page.get('raw_json'),
                            'real_page': real_page
                        })
                    
                    st.toast(f"âœ… æˆåŠŸè¼‰å…¥ JSON: {current_file_name}", icon="ğŸ“‚")
                    if st.session_state.enable_auto_analysis:
                        st.session_state.auto_start_analysis = True
                    st.rerun()
                else:
                    st.success(f"ğŸ“‚ ç›®å‰è¼‰å…¥ JSONï¼š**{uploaded_json.name}**")
            except Exception as e:
                st.error(f"JSON æª”æ¡ˆæ ¼å¼éŒ¯èª¤: {e}")

    # --- æƒ…æ³ C: ä¸Šå‚³ Excel (æ–°å¢çš„æ”¾åœ¨é€™) ---
    elif data_source == "ğŸ“Š ä¸Šå‚³ Excel æª”":
        st.info("ğŸ’¡ ä¸Šå‚³ Excel æª”å¾Œï¼Œç³»çµ±æœƒå°‡è¡¨æ ¼å…§å®¹è½‰æ›ç‚ºæ–‡å­—ä¾› AI ç¨½æ ¸ã€‚")
        uploaded_xlsx = st.file_uploader("ä¸Šå‚³ Excel æª”", type=['xlsx', 'xls'], key="xlsx_uploader")
        
        if uploaded_xlsx:
            try:
                current_file_name = uploaded_xlsx.name
                if st.session_state.get('last_loaded_xlsx_name') != current_file_name:
                    df_dict = pd.read_excel(uploaded_xlsx, sheet_name=None)
                    st.session_state.photo_gallery = []
                    st.session_state.source_mode = 'excel'
                    st.session_state.last_loaded_xlsx_name = current_file_name
                    
                    for sheet_name, df in df_dict.items():
                        df = df.fillna("")
                        md_table = df.to_markdown(index=False)
                        st.session_state.photo_gallery.append({
                            'file': None,
                            'table_md': md_table,
                            'header_text': f"ä¾†æºåˆ†é : {sheet_name}",
                            'full_text': f"Excel å…§å®¹ - åˆ†é  {sheet_name}\n" + md_table,
                            'raw_json': None,
                            'real_page': sheet_name
                        })
                    st.toast(f"âœ… æˆåŠŸè¼‰å…¥ Excel: {current_file_name}", icon="ğŸ“Š")
                    if st.session_state.enable_auto_analysis:
                        st.session_state.auto_start_analysis = True
                    st.rerun()
                else:
                    st.success(f"ğŸ“Š ç›®å‰è¼‰å…¥ Excelï¼š**{uploaded_xlsx.name}**")
            except Exception as e:
                st.error(f"Excel è®€å–å¤±æ•—: {e}")

if st.session_state.photo_gallery:
    st.caption(f"å·²ç´¯ç© {len(st.session_state.photo_gallery)} é æ–‡ä»¶")
    col_btn1, col_btn2 = st.columns([1, 1], gap="small")
    with col_btn1: start_btn = st.button("ğŸš€ é–‹å§‹åˆ†æ", type="primary", use_container_width=True)
    with col_btn2: 
        clear_btn = st.button("ğŸ—‘ï¸ç…§ç‰‡æ¸…é™¤", help="æ¸…é™¤", use_container_width=True)

    if clear_btn:
        st.session_state.photo_gallery = []
        st.session_state.analysis_result_cache = None
        if 'last_loaded_json_name' in st.session_state:
            del st.session_state.last_loaded_json_name 
        st.rerun()

    is_auto_start = st.session_state.auto_start_analysis
    if is_auto_start:
        st.session_state.auto_start_analysis = False

    if 'analysis_result_cache' not in st.session_state:
        st.session_state.analysis_result_cache = None

    trigger_analysis = start_btn or is_auto_start

    if trigger_analysis:
        total_start = time.time()
        status = st.empty()
        progress_bar = st.progress(0)
            
        extracted_data_list = [None] * len(st.session_state.photo_gallery)
        full_text_for_search = ""
        total_imgs = len(st.session_state.photo_gallery)
            
        ocr_start = time.time()

        def process_image_task(index, item):
            index = int(index)
            # å¦‚æœå·²ç¶“æœ‰è³‡æ–™äº†å°±ä¸é‡è¤‡æƒæ
            if item.get('table_md') and item.get('header_text') and item.get('full_text'):
                real_page = item.get('real_page', str(index + 1))
                return index, item['table_md'], item['header_text'], item['full_text'], None, real_page, None
    
            try:
                if item.get('file') is None:
                    return index, None, None, None, None, None, "ç„¡åœ–ç‰‡æª”æ¡ˆ"
                
                item['file'].seek(0)
                # é€™è£¡æœƒæ¥åˆ°æˆ‘å€‘å‰›æ‰ä¿®æ”¹å¾Œå›å‚³çš„ None
                table_md, header, full, _, real_page = extract_layout_with_azure(item['file'], DOC_ENDPOINT, DOC_KEY)
                return index, table_md, header, full, None, real_page, None
            except Exception as e:
                return index, None, None, None, None, None, f"OCRå¤±æ•—: {str(e)}"

        status.text(f"Azure æ­£åœ¨å¹³è¡Œæƒæ {total_imgs} é æ–‡ä»¶...")

        with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
            futures = []
            for i, item in enumerate(st.session_state.photo_gallery):
                futures.append(executor.submit(process_image_task, i, item))
            
            completed_count = 0
            for future in concurrent.futures.as_completed(futures):
                idx, t_md, h_txt, f_txt, raw_j, r_page, err = future.result()
                idx = int(idx)
                
                if err:
                    st.error(f"ç¬¬ {idx+1} é è®€å–å¤±æ•—: {err}")
                    extracted_data_list[idx] = None
                else:
                    st.session_state.photo_gallery[idx]['table_md'] = t_md
                    st.session_state.photo_gallery[idx]['header_text'] = h_txt
                    st.session_state.photo_gallery[idx]['full_text'] = f_txt
                    st.session_state.photo_gallery[idx]['raw_json'] = raw_j
                    st.session_state.photo_gallery[idx]['real_page'] = r_page
                    st.session_state.photo_gallery[idx]['file'] = None
                    
                    extracted_data_list[idx] = {
                        "page": r_page,
                        "table": t_md or "", 
                        "header_text": h_txt or ""
                    }
                
                completed_count += 1
                progress_bar.progress(completed_count / (total_imgs + 1))
        
        for i, data in enumerate(extracted_data_list):
            if data and isinstance(data, dict):
                page_idx = i
                if 0 <= page_idx < len(st.session_state.photo_gallery):
                    full_text_for_search += st.session_state.photo_gallery[page_idx].get('full_text', '')

        ocr_end = time.time()
        ocr_duration = ocr_end - ocr_start

        combined_input = "ä»¥ä¸‹æ˜¯å„é è³‡æ–™ï¼š\n"
        for i, data in enumerate(extracted_data_list):
            if data is None: continue
            page_num = data.get('page', i+1)
            table_text = data.get('table', '')
            header_text = data.get('header_text', '')
            combined_input += f"\n=== Page {page_num} ===\nã€é é¦–ã€‘:\n{header_text}\nã€è¡¨æ ¼ã€‘:\n{table_text}\n"
            
        status.text("ç¸½ç¨½æ ¸ Agent æ­£åœ¨é€²è¡Œå…¨æ–¹ä½åˆ†æ...")
        
        # 1. åŸ·è¡Œ AI åˆ†æ
        t0 = time.time()
        # ğŸ’¡ [ä¿®æ­£]ï¼šä¸å†é‡è¤‡å‚³é€ full_text_for_search
        # æ—¢ç„¶ full_text_for_search åªæ˜¯ç”¨ä¾†æ‰¾è¦å‰‡ï¼Œé‚£å°±ä¸è¦æŠŠå®ƒç•¶æˆåƒæ•¸å‚³çµ¦ agent
        res_main = agent_unified_check(combined_input, combined_input, GEMINI_KEY, main_model_name)
        time_main = time.time() - t0
        
        progress_bar.progress(100)
        status.empty()
        
        total_end = time.time()
        
        # --- 1. æˆæœ¬è¨ˆç®— (å®Œå…¨ä¾ç…§æ‚¨çš„ç‰ˆæœ¬ï¼ŒåŸå°ä¸å‹•) ---
        usage_main = res_main.get("_token_usage", {"input": 0, "output": 0})
        
        def get_model_rate(model_name):
            name = model_name.lower()
            if "gpt" in name:
                if "mini" in name: return 0.15, 0.60
                elif "3.5" in name: return 0.50, 1.50
                else: return 2.50, 10.00
            else:
                if "flash" in name: return 0.5, 3.00
                else: return 1.25, 10.00 # Pro

        rate_in, rate_out = get_model_rate(main_model_name)
        
        cost_usd = (usage_main["input"] / 1_000_000 * rate_in) + (usage_main["output"] / 1_000_000 * rate_out)
        cost_twd = cost_usd * 32.5
        
        # --- 2. å•Ÿå‹• Python ç¡¬æ ¸æ•¸å€¼ç¨½æ ¸ (æ”¹åœ¨é€™è£¡åŸ·è¡Œä¸€æ¬¡å³å¯) ---
        dim_data = res_main.get("dimension_data", [])
        python_numeric_issues = python_numerical_audit(dim_data)
        
        # --- ğŸ’¡ [æ–°å¢æ’å…¥] å•Ÿå‹• Python æœƒè¨ˆå¼•æ“ (è§£æ±º NameError) ---
        # é€™è£¡æœƒåŸ·è¡Œæ‚¨æœ€çœ‹é‡çš„èšåˆæ¨¡å¼ã€æœ¬é«”å»é‡èˆ‡é‹è²»æ ¸å°
        python_accounting_issues = python_accounting_audit(dim_data, res_main)
        
        # --- 3. Python è¡¨é ­æª¢æŸ¥ ---
        python_header_issues, python_debug_data = python_header_check(st.session_state.photo_gallery)
        
        # --- 4. åˆä½µçµæœ (æ­£å¼ç§»äº¤æ¬Šé™) ---
        ai_raw_issues = res_main.get("issues", [])
        ai_filtered_issues = []

        for i in ai_raw_issues:
            i['source'] = 'ğŸ¤– ç¸½ç¨½æ ¸ AI'
            i_type = i.get("issue_type", "")
            
            # åªæœ‰æµç¨‹ç•°å¸¸ã€è¦æ ¼æå–å¤±æ•—ã€è¡¨é ­ã€æœªåŒ¹é…è½ AI çš„
            # çµ±è¨ˆèˆ‡æ•¸é‡ä¸ç¬¦ç¾åœ¨äº¤çµ¦ Python å¼•æ“äº†ï¼Œæ‰€ä»¥æ’é™¤ AI åŸæœ¬å ±çš„
            ai_only_tasks = ["æµç¨‹", "è¦æ ¼æå–å¤±æ•—", "è¡¨é ­", "æœªåŒ¹é…"]
            
            if any(k in i_type for k in ai_only_tasks):
                ai_filtered_issues.append(i)
        
        # æœ€çµ‚åˆä½µæ‰€æœ‰ç¨½æ ¸ç±ƒå­
        all_issues = ai_filtered_issues + python_numeric_issues + python_accounting_issues + python_header_issues
        
        st.session_state.analysis_result_cache = {
            "job_no": res_main.get("job_no", "Unknown"),
            "all_issues": all_issues,
            "total_duration": total_end - total_start,
            "cost_twd": cost_twd,
            "total_in": usage_main["input"],
            "total_out": usage_main["output"],
            "ocr_duration": ocr_duration,
            "time_eng": time_main, # é€™è£¡å€Ÿç”¨è®Šæ•¸åï¼Œå¯¦ç‚ºç¸½æ™‚é–“
            "time_acc": 0,         # å–®ä¸€ä»£ç†ç„¡ç¬¬äºŒæ™‚é–“
            "full_text_for_search": full_text_for_search,
            "combined_input": combined_input,
            "python_debug_data": python_debug_data,
            "ai_extracted_data": dim_data
        }

    if st.session_state.analysis_result_cache:
        cache = st.session_state.analysis_result_cache
        all_issues = cache['all_issues']
        
        st.success(f"å·¥ä»¤: {cache['job_no']} | â±ï¸ {cache['total_duration']:.1f}s")
        st.info(f"ğŸ’° æœ¬æ¬¡æˆæœ¬: NT$ {cache['cost_twd']:.2f} (In: {cache['total_in']:,} / Out: {cache['total_out']:,})")
        st.caption(f"ç´°ç¯€è€—æ™‚: Azure OCR {cache['ocr_duration']:.1f}s | AI åˆ†æ {cache['time_eng']:.1f}s")
        
        with st.expander("ğŸ” æŸ¥çœ‹ AI è®€å–åˆ°çš„ Excel è¦å‰‡ (Debug)"):
            rules_text = get_dynamic_rules(cache['full_text_for_search'], debug_mode=True)
            if "ç„¡ç‰¹å®šè¦å‰‡" in rules_text:
                st.caption("ç„¡åŒ¹é…è¦å‰‡")
            else:
                st.markdown(rules_text)
                
        # --- æ–°å¢çš„ Debug å±•é–‹é  ---
        with st.expander("ğŸ”¬ æŸ¥çœ‹ AI æŠ„éŒ„çµ¦ Python çš„åŸå§‹æ•¸æ“š (æª¢æŸ¥æ‰‹å¯«éæ¿¾)", expanded=False):
            raw_dim_data = cache.get("ai_extracted_data", [])
            if raw_dim_data:
                st.write("é€™æ˜¯ AI æŠ„éŒ„ä¸¦ç¿»è­¯å¾Œçš„ JSONï¼ˆåŒ…å«æ ¼å¼æ˜¯å¦æ­£ç¢ºã€æ•¸å­—æ˜¯å¦è¢«ç°¡åŒ–ï¼‰ï¼š")
                st.json(raw_dim_data)
            else:
                st.caption("ç„¡æ•¸æ“šæå–è³‡æ–™ã€‚")

        with st.expander("ğŸ æŸ¥çœ‹ Python ç¡¬é‚è¼¯åµæ¸¬çµæœ (Debug)", expanded=False):
            if cache.get('python_debug_data'):
                p_data = cache['python_debug_data']
                standard_data = {}
                all_values = {"å·¥ä»¤ç·¨è™Ÿ": [], "é å®šäº¤è²¨": [], "å¯¦éš›äº¤è²¨": []}
                for page in p_data:
                    for k in all_values.keys():
                        if page.get(k) and page[k] != "N/A":
                            all_values[k].append(page[k])
                
                standard_row = {"é ç¢¼": "ğŸ† åˆ¤å®šæ¨™æº–"}
                for k, v in all_values.items():
                    if v:
                        standard_row[k] = Counter(v).most_common(1)[0][0]
                    else:
                        standard_row[k] = "N/A"
                
                final_df_data = [standard_row] + p_data
                st.dataframe(final_df_data, use_container_width=True, hide_index=True)
                st.info("ğŸ’¡ ã€Œåˆ¤å®šæ¨™æº–ã€æ˜¯ä¾æ“šå¤šæ•¸æ±ºç”¢ç”Ÿçš„ã€‚")
            else:
                st.caption("ç„¡åµæ¸¬è³‡æ–™")

        real_errors = [i for i in all_issues if "æœªåŒ¹é…" not in i.get('issue_type', '')]
        
        if not real_errors:
            st.balloons()
            if not all_issues:
                st.success("âœ… å…¨æ•¸åˆæ ¼ï¼")
            else:
                st.success(f"âœ… æ•¸å€¼å…¨æ•¸åˆæ ¼ï¼ (ä½†æœ‰ {len(all_issues)} å€‹é …ç›®æœªåŒ¹é…è¦å‰‡ï¼Œè«‹æª¢æŸ¥)")
        else:
            st.error(f"ç™¼ç¾ {len(real_errors)} é¡æ•¸å€¼ç•°å¸¸ï¼Œå¦æœ‰ {len(all_issues) - len(real_errors)} å€‹é …ç›®æœªåŒ¹é…è¦å‰‡")

        for item in all_issues:
            with st.container(border=True):
                c1, c2 = st.columns([3, 1])
                
                source_label = item.get('source', '')
                issue_type = item.get('issue_type', 'ç•°å¸¸')
                
                c1.markdown(f"**P.{item.get('page', '?')} | {item.get('item')}**  `{source_label}`")
                
                # é¡è‰²æ§åˆ¶ï¼šæœƒè¨ˆçµ±è¨ˆé¡ç”¨ç´…è‰²ï¼Œè¦æ ¼é¡ç”¨é»ƒè‰²
                if "çµ±è¨ˆ" in issue_type or "æ•¸é‡" in issue_type or "æµç¨‹" in issue_type:
                    c2.error(f"ğŸ›‘ {issue_type}")
                else:
                    c2.warning(f"âš ï¸ {issue_type}")
                
                st.caption(f"åŸå› : {item.get('common_reason', '')}")
                
                # --- æ¸²æŸ“è¡¨æ ¼ (æœƒè¨ˆå°å¸³å–®) ---
                failures = item.get('failures', [])
                if failures:
                    table_data = []
                    for f in failures:
                        if isinstance(f, dict):
                            # æˆ‘å€‘çµ±ä¸€ä½¿ç”¨é€™å››å€‹æ¬„ä½æ¨™é¡Œï¼Œæœƒè¨ˆèˆ‡å·¥ç¨‹å…±ç”¨
                            row = {
                                "é …ç›®/æ»¾è¼ªç·¨è™Ÿ": f.get('id', 'æœªçŸ¥'), 
                                "å¯¦æ¸¬/è¨ˆæ•¸": f.get('val', 'N/A'),
                                "æ¨™æº–/å‚™è¨»": f.get('target', ''), # å·¥ç¨‹ç”¨
                                "åˆ¤å®šç®—å¼/ç‹€æ…‹": f.get('calc', '') # æœƒè¨ˆç”¨
                            }
                            # å¦‚æœæ˜¯æœƒè¨ˆæ¨¡å¼ï¼ŒæŠŠ target ç•™ç©ºï¼Œè³‡è¨Šä¸»è¦åœ¨ id å’Œ val
                            table_data.append(row)
                    
                    if table_data:
                        st.dataframe(table_data, use_container_width=True, hide_index=True)
                else:
                    # å¦‚æœæ²’æœ‰ failuresï¼Œè‡³å°‘é¡¯ç¤ºä¸€å€‹æ•¸æ“šæç¤º
                    st.info(f"è©³ç´°æ•¸æ“šè¦‹ä¸Šè¿°åŸå› èªªæ˜")
        
        st.divider()

        current_job_no = cache.get('job_no', 'Unknown')
        safe_job_no = current_job_no.replace("/", "_").replace("\\", "_").strip()
        file_name_str = f"{safe_job_no}_cleaned.json"

        # æº–å‚™åŒ¯å‡ºè³‡æ–™
        export_data = []
        for item in st.session_state.photo_gallery:
            export_data.append({
                "table_md": item.get('table_md'),
                "header_text": item.get('header_text'),
                "full_text": item.get('full_text'),
                "raw_json": item.get('raw_json')
            })
        json_str = json.dumps(export_data, indent=2, ensure_ascii=False)

        st.subheader("ğŸ’¾ æ¸¬è©¦è³‡æ–™å­˜æª”")
        st.caption(f"å·²è­˜åˆ¥å·¥ä»¤ï¼š**{current_job_no}**ã€‚ä¸‹è¼‰å¾Œå¯ä¾›ä¸‹æ¬¡æ¸¬è©¦ä½¿ç”¨ã€‚")
        
        st.download_button(
            label=f"â¬‡ï¸ ä¸‹è¼‰æ¸¬è©¦è³‡æ–™ ({file_name_str})",
            data=json_str,
            file_name=file_name_str,
            mime="application/json",
            type="primary"
        )

        with st.expander("ğŸ‘€ æŸ¥çœ‹å‚³çµ¦ AI çš„æœ€çµ‚æ–‡å­— (Prompt Input)"):
            st.caption("é€™æ‰æ˜¯ AI çœŸæ­£è®€åˆ°çš„å…§å®¹ (å·²éæ¿¾é›œè¨Š)ï¼š")
            st.code(cache['combined_input'], language='markdown')
    
    if st.session_state.photo_gallery and st.session_state.get('source_mode') != 'json':
        st.caption("å·²æ‹æ”ç…§ç‰‡ï¼š")
        cols = st.columns(4)
        for idx, item in enumerate(st.session_state.photo_gallery):
            with cols[idx % 4]:
                if item.get('file'):
                    st.image(item['file'], caption=f"P.{idx+1}", use_container_width=True)
                if st.button("âŒ", key=f"del_{idx}"):
                    st.session_state.photo_gallery.pop(idx)
                    st.session_state.analysis_result_cache = None
                    st.rerun()
else:
    st.info("ğŸ‘† è«‹é»æ“Šä¸Šæ–¹æŒ‰éˆ•é–‹å§‹æ–°å¢ç…§ç‰‡")
