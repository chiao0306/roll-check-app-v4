import streamlit as st
import streamlit.components.v1 as components
from azure.core.credentials import AzureKeyCredential
from azure.ai.documentintelligence import DocumentIntelligenceClient
from azure.ai.documentintelligence.models import AnalyzeResult
import google.generativeai as genai
from openai import OpenAI
import json
import time
import concurrent.futures
import pandas as pd
from thefuzz import fuzz
from collections import Counter
import re

# --- 1. é é¢è¨­å®š ---
st.set_page_config(page_title="äº¤è²¨å–®ç¨½æ ¸", page_icon="ğŸ­", layout="centered")

# --- CSS æ¨£å¼ ---
st.markdown("""
<style>
/* 1. æ¨™é¡Œå¤§å°æ§åˆ¶ */
h1 {
    font-size: 1.7rem !important; 
    white-space: nowrap !important;
    overflow: hidden !important; 
    text-overflow: ellipsis !important;
}

/* 2. ä¸»åŠŸèƒ½æŒ‰éˆ• (ç´…è‰² Primary) -> è®Šå¤§ã€è®Šé«˜ */
/* é€™æœƒå½±éŸ¿ã€Œé–‹å§‹åˆ†æã€å’Œã€Œç…§ç‰‡æ¸…é™¤ã€ */
button[kind="primary"] {
    height: 60px;               
    font-size: 20px !important; 
    font-weight: bold !important;
    border-radius: 10px !important;
    margin-top: 0px !important;    
    margin-bottom: 5px !important; 
    width: 100%;                
}

/* 3. æ¬¡è¦æŒ‰éˆ• (ç°è‰² Secondary) -> ä¿æŒåŸç‹€ */
/* é€™æœƒå½±éŸ¿æ¯ä¸€å¼µç…§ç‰‡ä¸‹é¢çš„ã€ŒXã€æŒ‰éˆ•ï¼Œè®“å®ƒç¶­æŒå°å°çš„ */
button[kind="secondary"] {
    height: auto !important;
    font-weight: normal !important;
}
</style>
""", unsafe_allow_html=True)
# --- 2. ç§˜å¯†é‡‘é‘°è®€å– ---
try:
    DOC_ENDPOINT = st.secrets["DOC_ENDPOINT"]
    DOC_KEY = st.secrets["DOC_KEY"]
    GEMINI_KEY = st.secrets["GEMINI_KEY"]
    OPENAI_KEY = st.secrets.get("OPENAI_KEY", "")
except:
    st.error("æ‰¾ä¸åˆ°é‡‘é‘°ï¼è«‹åœ¨ Streamlit Cloud è¨­å®š Secretsã€‚")
    st.stop()

# --- 3. åˆå§‹åŒ– Session State ---
if 'photo_gallery' not in st.session_state: st.session_state.photo_gallery = []
if 'uploader_key' not in st.session_state: st.session_state.uploader_key = 0
if 'auto_start_analysis' not in st.session_state: st.session_state.auto_start_analysis = False

# --- å´é‚Šæ¬„æ¨¡å‹è¨­å®š (åˆä½µç‚ºå–®ä¸€é¸æ“‡) ---
with st.sidebar:
    st.header("æ¨¡å‹è¨­å®š")
    
    # é€™è£¡åŠ å…¥æœ€æ–°çš„ Gemini æ¨¡å‹
    model_options = {
        "Gemini 3 Flash preview": "gemini-3-flash-preview",
        "Gemini 2.5 Flash": "models/gemini-2.5-flash",
        "Gemini 2.5 Pro": "models/gemini-2.5-pro",
        #"GPT-5(ç„¡æ•ˆ)": "models/gpt-5",
        #"GPT-5 Mini(ç„¡æ•ˆ)": "models/gpt-5-mini",
    }
    options_list = list(model_options.keys())
    
    st.subheader("ğŸ¤– ç¸½ç¨½æ ¸ Agent")
    model_selection = st.selectbox(
        "è² è²¬ï¼šè¦æ ¼ã€è£½ç¨‹ã€æ•¸é‡ã€çµ±è¨ˆå…¨åŒ…", 
        options=options_list, 
        index=0, 
        key="main_model"
    )
    main_model_name = model_options[model_selection]
    
    st.divider()
    
    default_auto = st.query_params.get("auto", "true") == "true"
    def update_url_param():
        current_state = "true" if st.session_state.enable_auto_analysis else "false"
        st.query_params["auto"] = current_state

    st.toggle(
        "âš¡ ä¸Šå‚³å¾Œè‡ªå‹•åˆ†æ", 
        value=default_auto, 
        key="enable_auto_analysis", 
        on_change=update_url_param
    )

# --- Excel è¦å‰‡è®€å–å‡½æ•¸ (å–®ä¸€ä»£ç†æ•´åˆç‰ˆ) ---
@st.cache_data
def get_dynamic_rules(ocr_text, debug_mode=False):
    try:
        df = pd.read_excel("rules.xlsx")
        df.columns = [c.strip() for c in df.columns]
        ocr_text_clean = str(ocr_text).upper().replace(" ", "").replace("\n", "")
        specific_rules = []

        for index, row in df.iterrows():
            item_name = str(row.get('Item_Name', '')).strip()
            # ğŸ’¡ è·³éåŸæœ¬çš„ã€Œ(é€šç”¨)ã€é …ç›®ï¼ŒåªæŠ“ç‰¹è¦
            if not item_name or "(é€šç”¨)" in item_name: continue
            
            # ä½¿ç”¨æ¨¡ç³ŠåŒ¹é…åˆ¤æ–·æ˜¯å¦ç‚ºç•¶å‰è™•ç†çš„é …ç›®
            score = fuzz.partial_ratio(item_name.upper().replace(" ", ""), ocr_text_clean)
            if score >= 85:
                # æå–ç‰¹è¦è³‡è¨Š
                spec = str(row.get('Standard_Spec', ''))
                logic = str(row.get('Logic_Prompt', ''))
                u_local = str(row.get('Unit_Rule_Local', ''))
                u_agg = str(row.get('Unit_Rule_Agg', ''))
                u_freight = str(row.get('Unit_Rule_Freight', ''))
                
                desc = f"- **[ç‰¹å®šé …ç›®è¦å‰‡] {item_name}**\n"
                if spec != 'nan' and spec: desc += f"  - [å¼·åˆ¶è¦æ ¼]: {spec}\n"
                if logic != 'nan' and logic: desc += f"  - [ä¾‹å¤–æŒ‡ä»¤]: {logic}\n"
                if u_local != 'nan' and u_local: desc += f"  - [æœƒè¨ˆå–®é …]: {u_local}\n"
                if u_agg != 'nan' and u_agg: desc += f"  - [æœƒè¨ˆèšåˆ]: {u_agg}\n"
                if u_freight != 'nan' and u_freight: desc += f"  - [æœƒè¨ˆé‹è²»]: {u_freight}\n"
                specific_rules.append(desc)
        
        return "\n".join(specific_rules) if specific_rules else "ç„¡ç‰¹å®šå°ˆæ¡ˆè¦å‰‡ï¼Œè«‹ä¾ç…§é€šç”¨æ†²æ³•åŸ·è¡Œã€‚"
    except Exception as e:
        return f"è®€å–è¦å‰‡æª”æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}"
        
# --- 4. æ ¸å¿ƒå‡½æ•¸ï¼šAzure ç¥ä¹‹çœ¼ ---
def extract_layout_with_azure(file_obj, endpoint, key):
    client = DocumentIntelligenceClient(endpoint=endpoint, credential=AzureKeyCredential(key))
    file_content = file_obj.getvalue()
    
    poller = client.begin_analyze_document("prebuilt-layout", file_content, content_type="application/octet-stream")
    result: AnalyzeResult = poller.result()
    
    markdown_output = ""
    full_content_text = ""
    real_page_num = "Unknown"
    
    bottom_stop_keywords = ["æ³¨æ„äº‹é …", "ä¸­æ©Ÿå“æª¢å–®ä½", "ä¿å­˜æœŸé™", "è¡¨å–®ç·¨è™Ÿ", "FORM NO", "ç°½ç« "]
    top_right_noise_keywords = [
        "æª¢é©—é¡åˆ¥", "å°ºå¯¸æª¢é©—", "ä¾åœ–é¢æ¨™è¨˜", "ææ–™æª¢é©—", "æˆä»½åˆ†æ", 
        "éç ´å£æ€§", "æ­£å¸¸åŒ–", "é€€ç«", "æ·¬.å›ç«", "è¡¨é¢ç¡¬åŒ–", "è©¦è»Š",
        "æ€§èƒ½æ¸¬è©¦", "è©¦å£“è©¦æ¼", "å‹•.éœå¹³è¡¡è©¦é©—", ":selected:", ":unselected:",
        "æŠ—æ‹‰", "ç¡¬åº¦è©¦é©—", "UT", "PT", "MT"
    ]
    
    if result.tables:
        for idx, table in enumerate(result.tables):
            page_num = "Unknown"
            if table.bounding_regions: page_num = table.bounding_regions[0].page_number
            markdown_output += f"\n### Table {idx + 1} (Page {page_num}):\n"
            rows = {}
            stop_processing_table = False 
            
            for cell in table.cells:
                if stop_processing_table: break
                content = cell.content.replace("\n", " ").strip()
                
                for kw in bottom_stop_keywords:
                    if kw in content:
                        stop_processing_table = True
                        break
                if stop_processing_table: break
                
                is_noise = False
                for kw in top_right_noise_keywords:
                    if kw in content:
                        is_noise = True
                        break
                if is_noise: content = "" 

                r, c = cell.row_index, cell.column_index
                if r not in rows: rows[r] = {}
                rows[r][c] = content
            
            for r in sorted(rows.keys()):
                row_cells = []
                if rows[r]:
                    max_col = max(rows[r].keys())
                    for c in range(max_col + 1): 
                        row_cells.append(rows[r].get(c, ""))
                    markdown_output += "| " + " | ".join(row_cells) + " |\n"
    
    if result.content:
        match = re.search(r"(?:é …æ¬¡|Page|é æ¬¡|NO\.)[:\s]*(\d+)\s*[/ï¼]\s*\d+", result.content, re.IGNORECASE)
        if match:
            real_page_num = match.group(1)

        cut_index = len(result.content)
        for keyword in bottom_stop_keywords:
            idx = result.content.find(keyword)
            if idx != -1 and idx < cut_index:
                cut_index = idx
        
        temp_text = result.content[:cut_index]
        for noise in top_right_noise_keywords:
            temp_text = temp_text.replace(noise, "")
            
        full_content_text = temp_text
        header_snippet = full_content_text[:800]
    else:
        full_content_text = ""
        header_snippet = ""

    return markdown_output, header_snippet, full_content_text, None, real_page_num

# --- Python ç¡¬é‚è¼¯ï¼šè¡¨é ­ä¸€è‡´æ€§æª¢æŸ¥ (é•·åº¦æ•æ„Ÿç‰ˆ) ---
def python_header_check(photo_gallery):
    issues = []
    if not photo_gallery:
        return issues, []

    # å®šç¾© Regex (é‡å° "å»ç©ºç™½+å»æ›è¡Œ" å¾Œçš„å­—ä¸²è¨­è¨ˆ)
    patterns = {
        # ã€ä¿®æ”¹é» 1ã€‘å·¥ä»¤ Regex æ”¾å¯¬ï¼š
        # åŸæœ¬åªæŠ“ W é–‹é ­ï¼Œç¾åœ¨æ”¹æŠ“ "ç·¨è™Ÿ" å¾Œé¢æ¥çš„ "ä»»ä½•è‹±æ•¸å­—ä¸²"
        # é€™æ¨£å°±ç®—å®ƒå¯« WW363... æˆ–æ˜¯ 12345... éƒ½èƒ½æ•´ä¸²æŠ“å‡ºä¾†æ¯”å°
        "å·¥ä»¤ç·¨è™Ÿ": r"[å·¥åœŸä¸‹][ä»¤å†·ä»Š]ç·¨è™Ÿ[:\.]*([A-Za-z0-9\-\_]+)", 
        
        "é å®šäº¤è²¨": r"[é é¢„é …é ‚][å®šäº¤].*?(\d{2,4}[\.\-/]\d{1,2}[\.\-/]\d{1,2})",
        "å¯¦éš›äº¤è²¨": r"[å¯¦çœŸ][éš›äº¤].*?(\d{2,4}[\.\-/]\d{1,2}[\.\-/]\d{1,2})"
    }

    extracted_data = [] 
    all_values = {key: [] for key in patterns}

    for i, page in enumerate(photo_gallery):
        # æš´åŠ›æ¸…æ´—ï¼šå»æ›è¡Œã€å»ç©ºæ ¼ã€è½‰å¤§å¯«
        raw_text = page.get('header_text', '') + page.get('full_text', '')
        clean_text = raw_text.replace("\n", "").replace(" ", "").replace("\r", "").upper()
        
        # ã€ä¿®æ”¹é» 2ã€‘é ç¢¼é˜²å‘†ï¼šç¢ºä¿ä¸€å®šæœ‰å€¼
        # å„ªå…ˆæŠ“ real_pageï¼ŒæŠ“ä¸åˆ°å°±ç”¨ index
        r_page = page.get('real_page')
        if not r_page or r_page == "Unknown":
            page_label = f"P.{i + 1}"
        else:
            page_label = f"P.{r_page}"
            
        page_result = {"é æ•¸": page_label}
        
        for key, pattern in patterns.items():
            match = re.search(pattern, clean_text)
            if match:
                val = match.group(1).strip()
                
                # ã€ä¿®æ”¹é» 3ã€‘é‡å°å·¥ä»¤çš„ç‰¹æ®Šè™•ç† (å¦‚æœå¤ªé•·å¯èƒ½å°±æ˜¯é‡è¤‡æ‰“å­—)
                if key == "å·¥ä»¤ç·¨è™Ÿ":
                    # å¦‚æœä½ ç¢ºå®šå·¥ä»¤åªæœ‰ 10 ç¢¼ï¼Œä½†æŠ“åˆ°äº† 11 ç¢¼ä»¥ä¸Š (å¦‚ WW...)
                    # æˆ‘å€‘ä¿ç•™é€™å€‹éŒ¯èª¤çš„å€¼ï¼Œè®“å¾Œé¢çš„å¤šæ•¸æ±ºå»æŠŠå®ƒæªå‡ºä¾†
                    pass 
                
                page_result[key] = val
                all_values[key].append(val)
            else:
                page_result[key] = "N/A"
        
        extracted_data.append(page_result)

    # æ­¥é©Ÿ 2: æ±ºå®šã€Œæ­£ç¢ºæ¨™æº–ã€ (ä½¿ç”¨å¤šæ•¸æ±º)
    standard_data = {}
    for key, values in all_values.items():
        if values:
            # æ¿¾æ‰ N/A å¾Œå†æŠ•ç¥¨
            valid_values = [v for v in values if v != "N/A"]
            if valid_values:
                most_common = Counter(valid_values).most_common(1)[0][0]
                standard_data[key] = most_common
            else:
                standard_data[key] = "N/A"
        else:
            standard_data[key] = "N/A"

    # æ­¥é©Ÿ 3: æ¯”å°æ¯ä¸€é 
    for data in extracted_data:
        page_num = data['é æ•¸']
        
        for key, standard_val in standard_data.items():
            current_val = data[key]
            
            if standard_val == "N/A": continue # å…¨å·éƒ½æ²’æŠ“åˆ°å°±ä¸æ¯”äº†

            # é–‹å§‹æ¯”å° (å­—ä¸²ä¸ç›¸ç­‰)
            if current_val != standard_val:
                
                # åˆ¤æ–·æ˜¯å¦ç‚ºé•·åº¦ç•°å¸¸ (é‡å°å·¥ä»¤)
                reason = "èˆ‡å…¨å·å¤šæ•¸é é¢ä¸ä¸€è‡´"
                if key == "å·¥ä»¤ç·¨è™Ÿ" and len(current_val) != len(standard_val):
                    reason += f" (é•·åº¦ç•°å¸¸: {len(current_val)}ç¢¼ vs æ¨™æº–{len(standard_val)}ç¢¼)"

                issue = {
                    "page": page_num.replace("P.", ""),
                    "item": f"è¡¨é ­æª¢æŸ¥-{key}",
                    "rule_used": "Pythonç¡¬é‚è¼¯æª¢æŸ¥",
                    "issue_type": "è·¨é è³‡è¨Šä¸ç¬¦",
                    "spec_logic": f"æ‡‰ç‚º {standard_val}",
                    "common_reason": reason,
                    "failures": [
                        {"id": "å…¨å·åŸºæº–", "val": standard_val, "calc": "å¤šæ•¸æ±ºæ¨™æº–"},
                        {"id": f"æœ¬é ({page_num})", "val": current_val, "calc": "ç•°å¸¸/æ¼æŠ“"}
                    ],
                    "source": "ğŸ¤– ç³»çµ±è‡ªå‹•"
                }
                issues.append(issue)
                
    return issues, extracted_data

    # --- 5. ç¸½ç¨½æ ¸ Agent (æ•´åˆç‰ˆ - å¼·é‚è¼¯å„ªåŒ–) ---
def agent_unified_check(combined_input, full_text_for_search, api_key, model_name):
    dynamic_rules = get_dynamic_rules(full_text_for_search)

    system_prompt = f"""
    ä½ æ˜¯ä¸€ä½æ¥µåº¦åš´è¬¹çš„ä¸­é‹¼æ©Ÿæ¢°å“ç®¡ã€æ•¸æ“šæŠ„éŒ„å“¡ã€‘ã€‚ä½ å¿…é ˆåƒã€Œé›»è…¦ç¨‹å¼ã€ä¸€æ¨£åŸ·è¡Œä»»å‹™ã€‚
    
    {dynamic_rules}

    ---

    ### ğŸš€ åŸ·è¡Œç¨‹åº (Execution Procedure)

    #### âš”ï¸ æ¨¡çµ„ Aï¼šæ•¸æ“šæŠ„éŒ„èˆ‡è¦æ ¼ç¿»è­¯ (AI ä»»å‹™)
    1. **è¦æ ¼æŠ„éŒ„ (std_spec)**ï¼šç²¾ç¢ºæŠ„éŒ„æ¨™é¡Œä¸­å« `mm`ã€`Â±`ã€`+`ã€`-` çš„è¦æ ¼æ–‡å­—ã€‚
    2. **æ•¸æ“šæŠ„éŒ„ (ds - æ¥µé€Ÿå£“ç¸®æ ¼å¼)**ï¼š
       - **æ ¼å¼**ï¼šä½¿ç”¨ä¸€å€‹å­—ä¸²ä»£è¡¨å…¨é æ•¸æ“šï¼Œæ ¼å¼ç‚º `"ID:å€¼|ID:å€¼|ID:å€¼"`ã€‚
       - **å­—ä¸²ä¿è­·**ï¼šç¦æ­¢ç°¡åŒ–æ•¸å­—ã€‚`349.90` å¿…å¯« `"349.90"`ã€‚ç¦æ­¢å¯«æˆ `349.9`ã€‚
    3. **åˆ†é¡è­˜åˆ¥ (category)**ï¼š[æœªå†ç”Ÿæœ¬é«”, è»¸é ¸æœªå†ç”Ÿ, éŠ²è£œ, ç²¾åŠ å·¥å†ç”Ÿ] (è«‹æŒ‰ LEVEL 1-3 é †åºåˆ¤å®š)ã€‚
    4. **è¦æ ¼ç·¨è­¯ (sl)**ï¼šæå–æ¨™é¡Œä¸­çœ‹åˆ°çš„ `threshold` (é–€æª»ï¼Œæœ¬é«”é–€æª»çµ•å° >= 120)ã€‚

    #### ğŸ’° æ¨¡çµ„ Bï¼šæœƒè¨ˆæŒ‡æ¨™æå– (AI ä»»å‹™)
    1. **å‚³ç¥¨æå–**ï¼šæŠ„éŒ„çµ±è¨ˆè¡¨æ¯ä¸€è¡Œçš„åç¨±èˆ‡æ•¸é‡åˆ° `summary_rows`ã€‚æå–é‹è²»é …æ¬¡åˆ° `freight_target`ã€‚
    2. **é …ç›® PC æ•¸**ï¼šæå–é …ç›®æ¨™é¡Œæ‹¬è™Ÿå…§çš„æ•¸å­—ï¼ˆå¦‚ 12PCï¼‰åˆ° `item_pc_target`ã€‚
    3. **ğŸ›‘ ç¦ä»¤**ï¼šä½ ä¸éœ€æŠ„éŒ„ Excel è¦å‰‡æ–‡å­—ï¼Œä¹Ÿä¸å‡†åœ¨ `issues` å ±æ•¸å€¼å¤§å°å•é¡Œã€‚

    #### âš–ï¸ æ¨¡çµ„ Cï¼šè¶¨å‹¢ç¨½æ ¸ (AI åˆ¤å®š)
    1. **ç‰©ç†ä½éšæ¼”é€²**ï¼š`æœªå†ç”Ÿ < ç ”ç£¨ < å†ç”Ÿ < éŠ²è£œ`ã€‚è·¨é é¢å¾Œæ®µå°ºå¯¸å°æ–¼å‰æ®µï¼ˆéŠ²è£œé™¤å¤–ï¼‰ï¼Œå ± `ğŸ›‘æµç¨‹ç•°å¸¸`ã€‚

    ---

    ### ğŸ“ è¼¸å‡ºè¦ç¯„ (Output Format)
    å¿…é ˆå›å‚³å–®ä¸€ JSONã€‚æ•¸æ“šéƒ¨åˆ†è«‹åš´æ ¼éµå®ˆ `ds` èˆ‡ `sl` å£“ç¸®æ¨™ç±¤ã€‚
    - **ç¦æ­¢é›™å¼•è™Ÿ**ï¼šåœ¨ `item_title` æˆ– `std_spec` ä¸­ï¼Œåš´ç¦å‡ºç¾é›™å¼•è™Ÿ `"`ã€‚
    - **æ›¿æ›è¦å‰‡**ï¼šè‹¥é‡åˆ°è‹±å‹ç¬¦è™Ÿï¼Œè«‹ä¸€å¾‹æ”¹å¯«ç‚º `'` (å–®å¼•è™Ÿ) æˆ– `inch`ã€‚
    - **ç¯„ä¾‹**ï¼š`8" ROLL` å¿…é ˆå¯«æˆ `8' ROLL`ã€‚

    {{
      "job_no": "å·¥ä»¤",
      "summary_rows": [ {{ "title": "åç¨±", "target": æ•¸å­— }} ],
      "freight_target": 0,
      "issues": [ 
         {{ "page": "é ç¢¼", "item": "é …ç›®", "issue_type": "ğŸ›‘æµç¨‹ç•°å¸¸ / ğŸ›‘è¦æ ¼æå–å¤±æ•—", "common_reason": "åŸå› ", "failures": [] }}
      ],
      "dimension_data": [
         {{
           "page": æ•¸å­—,
           "item_title": "æ¨™é¡Œ",
           "category": "åˆ†é¡æ¨™ç±¤",
           "item_pc_target": æ•¸å­—,
           "sl": {{ "lt": "åˆ†é¡æ¨™ç±¤", "t": 0 }}, # lt: logic_type, t: threshold
           "std_spec": "åŸå§‹è¦æ ¼æ–‡å­—",
           "ds": "ID:å€¼|ID:å€¼|ID:å€¼" 
         }}
      ]
    }}
    """
    
    generation_config = {"response_mime_type": "application/json", "temperature": 0.0}
    
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel(model_name)
        response = model.generate_content([system_prompt, combined_input], generation_config=generation_config)
        
        raw_content = response.text
        # ğŸ›¡ï¸ è¶…ç´šè§£æå™¨ï¼šé˜²æ­¢ AI è¼¸å‡ºå¸¶æœ‰ Markdown æ¨™ç±¤æˆ–å»¢è©±
        import re
        json_match = re.search(r"\{.*\}", raw_content, re.DOTALL)
        if json_match:
            raw_content = json_match.group()
            
        parsed_data = json.loads(raw_content)
        parsed_data["_token_usage"] = {
            "input": response.usage_metadata.prompt_token_count, 
            "output": response.usage_metadata.candidates_token_count
        }
        return parsed_data

    except Exception as e:
        return {"job_no": f"JSON Error: {str(e)}", "issues": [], "dimension_data": []}
        
# --- é‡é»ï¼šPython å¼•æ“ç¨ç«‹æ–¼ agent å‡½å¼ä¹‹å¤– ---

def python_process_audit(dimension_data):
    process_issues = []
    roll_history = {} # { "ID": [{"p": "cat", "v": 190, "page": 1}, ...] }
    if not dimension_data: return []

    for item in dimension_data:
        p_num, ds, cat = item.get("page", "?"), item.get("ds", ""), str(item.get("category", "")).strip()
        pairs = [p.split(":") for p in ds.split("|") if ":" in p]
        for rid, val_str in pairs:
            try:
                val = float(re.findall(r"\d+\.?\d*", val_str)[0])
                rid_clean = rid.strip()
                if rid_clean not in roll_history: roll_history[rid_clean] = []
                roll_history[rid_clean].append({"p": cat, "v": val, "page": p_num, "title": item.get("item_title")})
            except: continue

    weights = {"un_regen": 1, "max_limit": 1, "range": 3, "min_limit": 4}
    for rid, records in roll_history.items():
        if len(records) < 2: continue
        records.sort(key=lambda x: str(x['page']))
        for i in range(len(records) - 1):
            curr, nxt = records[i], records[i+1]
            w_curr = weights.get(curr['p'], 2)
            if "ç ”ç£¨" in curr['title']: w_curr = 2
            w_nxt = weights.get(nxt['p'], 2)
            if "ç ”ç£¨" in nxt['title']: w_nxt = 2
            
            # ğŸ’¡ é—œéµåˆ¤å®šï¼šå¾Œæ®µä½éšå¤§ï¼Œæ•¸å€¼å°±ä¸æ‡‰è©²è®Šå°
            if w_nxt > w_curr and nxt['v'] < curr['v']:
                process_issues.append({
                    "page": nxt['page'], "item": f"ç·¨è™Ÿ {rid} å°ºå¯¸ä½éšæª¢æŸ¥",
                    "issue_type": "ğŸ›‘æµç¨‹ç•°å¸¸(å°ºå¯¸å€’ç½®)",
                    "common_reason": f"å¾Œæ®µ{nxt['p']}å°ºå¯¸å°æ–¼å‰æ®µ{curr['p']}",
                    "failures": [{"id": rid, "val": f"å¾Œ:{nxt['v']} < å‰:{curr['v']}", "calc": "å°ºå¯¸ä¸ç¬¦ä½éšé‚è¼¯"}]
                })
    return process_issues
    
def python_numerical_audit(dimension_data):
    grouped_errors = {}
    import re
    if not dimension_data: return []

    for item in dimension_data:
        raw_data_list = item.get("data", [])
        title = item.get("item_title", "")
        cat = str(item.get("category", "")).strip()
        page_num = item.get("page", "?")
        raw_spec = str(item.get("std_spec", ""))
        
        logic = item.get("sl", {})
        l_type = str(logic.get("lt", "")).lower()
        s_list = [float(n) for n in logic.get("tl", []) if n is not None]
        s_threshold = logic.get("t", 0)

        # --- ğŸ›¡ï¸ æ•¸æ“šæ¸…æ´—ï¼šè¬ç”¨å…¬å·®/å€é–“è§£æ (è§£æ±º 203.22 +0.3, +0.8 å•é¡Œ) ---
        s_ranges = []
        # 1. æŠ“å–åŸºåº•æ•¸å­— (mmä¹‹å‰çš„æ•¸å­—)
        mm_match = re.search(r"(\d+\.?\d*)\s*mm", raw_spec)
        base_val = float(mm_match.group(1)) if mm_match else None
        
        # 2. æŠ“å–æ‰€æœ‰åç§»é‡ (å¦‚ +0.3, +0.8 æˆ– +0, -0.14)
        offsets = re.findall(r"([+-]\s*\d+\.?\d*)", raw_spec)
        
        if base_val and len(offsets) >= 2:
            # ğŸ’¡ [è¨ˆç®—æ ¸å¿ƒ]ï¼šåŸºåº•åŠ åç§»ï¼Œå–æœ€å¤§æœ€å°å€¼
            calc_nums = [base_val + float(o.replace(" ", "")) for o in offsets]
            s_ranges = [[min(calc_nums), max(calc_nums)]]
        elif base_val and len(offsets) == 1:
            val_off = base_val + float(offsets[0].replace(" ", ""))
            s_ranges = [[val_off, 9999.0]] if "+" in offsets[0] else [[0.0, val_off]]
        
        # 3. é›œè¨Šéæ¿¾
        all_nums = [float(n) for n in re.findall(r"(\d+\.?\d*)", raw_spec)]
        noise = [350.0, 300.0, 200.0, 145.0, 130.0]
        clean_std = [n for n in all_nums if (base_val and n == base_val) or (n not in noise and n > 5)]

        for entry in raw_data_list:
            if not isinstance(entry, list) or len(entry) < 2: continue
            rid, val_raw = str(entry[0]).strip(), str(entry[1]).strip()
            if not val_raw or val_raw in ["N/A", "nan", "M10"]: continue

            try:
                # åªå–ç¬¬ä¸€å€‹æ•¸å­—ï¼Œéæ¿¾æ‰‹å¯«
                v_match = re.findall(r"\d+\.?\d*", val_raw)
                val_str = v_match[0] if v_match else val_raw
                val = float(val_str)
                is_two_dec = "." in val_str and len(val_str.split(".")[-1]) == 2
                is_pure_int = "." not in val_str
                is_passed, reason, t_used, e_label = True, "", "N/A", "æœªçŸ¥"

                # --- ğŸ’¡ åˆ¤å®šå„ªå…ˆåºæ•´åˆ ---
                
                # A. éŠ²è£œ
                if "min_limit" in l_type or "éŠ²è£œ" in (cat + title):
                    e_label = "éŠ²è£œ(ä¸‹é™)"
                    if not is_pure_int: is_passed, reason = False, "éŠ²è£œæ ¼å¼éŒ¯èª¤: æ‡‰ç‚ºç´”æ•´æ•¸"
                    elif clean_std:
                        t_used = min(clean_std, key=lambda x: abs(x - val))
                        if val < t_used: is_passed, reason = False, f"éŠ²è£œä¸è¶³: å¯¦æ¸¬ {val} < {t_used}"

                # B. æœªå†ç”Ÿ (æœ¬é«”/è»¸é ¸)
                elif "un_regen" in l_type or "max_limit" in l_type or "æœªå†ç”Ÿ" in (cat + title):
                    if "è»¸é ¸" in (cat + title):
                        e_label = "è»¸é ¸(ä¸Šé™)"
                        candidates = [float(n) for n in (clean_std + s_list)]
                        if s_threshold: candidates.append(float(s_threshold))
                        target = max(candidates) if candidates else 0
                        t_used = target
                        if target > 0:
                            if not is_pure_int: is_passed, reason = False, "æ‡‰ç‚ºç´”æ•´æ•¸"
                            elif val > target: is_passed, reason = False, f"è¶…éä¸Šé™ {target}"
                    else:
                        e_label = "æœªå†ç”Ÿ(æœ¬é«”)"
                        candidates = [n for n in clean_std if n >= 120.0]
                        target = max(candidates) if candidates else 196.0
                        t_used = target
                        if val <= target:
                            if not is_pure_int: is_passed, reason = False, "æ‡‰ç‚ºæ•´æ•¸"
                        elif not is_two_dec: is_passed, reason = False, "æ‡‰å¡«å…©ä½å°æ•¸"

                # C. ç²¾åŠ å·¥å†ç”Ÿé¡ / å†ç”Ÿè»Šä¿® (Page 2 é …ç›® 5 èµ°é€™è£¡)
                elif "range" in l_type or "ç²¾åŠ å·¥" in cat or any(x in (cat + title) for x in ["å†ç”Ÿ", "ç ”ç£¨", "è»Šä¿®", "çµ„è£", "æ‹†è£", "çœŸåœ“åº¦"]):
                    e_label = "ç²¾åŠ å·¥(å€é–“)"
                    if not is_two_dec:
                        is_passed, reason = False, "æ ¼å¼éŒ¯èª¤: æ‡‰å¡«å…©ä½å°æ•¸(å¦‚.90)"
                    elif s_ranges:
                        # ğŸ’¡ é—œéµï¼šé€™è£¡æœƒç”¨åˆ° [203.52, 204.02]
                        t_used = str(s_ranges)
                        is_passed = any(r[0] <= val <= r[1] for r in s_ranges)
                        if not is_passed: reason = f"å°ºå¯¸ä¸åœ¨å€é–“ {t_used} å…§"

                if not is_passed:
                    key = (page_num, title, reason)
                    if error_key := key: # ä¿®æ­£ key åç¨±
                        if error_key not in grouped_errors:
                            grouped_errors[error_key] = {"page": page_num, "item": title, "issue_type": f"æ•¸å€¼ç•°å¸¸({e_label})", "common_reason": reason, "failures": []}
                        grouped_errors[error_key]["failures"].append({"id": rid, "val": val_str, "target": f"åŸºæº–:{t_used}", "calc": f"âš–ï¸ {e_label} å¼•æ“"})
            except: continue
    return list(grouped_errors.values())
    
def python_accounting_audit(dimension_data, res_main):
    """
    Python æœƒè¨ˆå®˜ï¼šåŸ·è¡Œå–®é …æ ¸å°ã€è»¸é ¸é™æ¬¡æª¢æŸ¥ã€é›™æ¨¡å¼å°å¸³ã€é‹è²»ç²¾ç®—ã€‚
    æ”¯æ´è‡ªå‹•æŸ¥è¡¨èˆ‡ Agg Rule æ··åˆæŒ‡ä»¤ã€‚
    """
    accounting_issues = []
    from thefuzz import fuzz
    from collections import Counter
    import re
    import pandas as pd

    # 1. å»ºç«‹ Excel è¦å‰‡å¿«å– (ä¾›è‡ªå‹•æŸ¥è¡¨)
    try:
        df_rules = pd.read_excel("rules.xlsx")
        df_rules.columns = [c.strip() for c in df_rules.columns]
    except:
        df_rules = None

    def safe_float(value):
        if value is None or str(value).upper() == 'NULL': return 0.0
        cleaned = "".join(re.findall(r"[\d\.]+", str(value).replace(',', '')))
        try: return float(cleaned) if cleaned else 0.0
        except: return 0.0

    # 2. å–å¾—å°å¸³åŸºæº– (ä¾†è‡ªå·¦ä¸Šè§’çµ±è¨ˆè¡¨)
    summary_rows = res_main.get("summary_rows", [])
    global_sum_tracker = {}
    for s in summary_rows:
        s_title = s.get('title', 'Unknown')
        if not s_title or len(str(s_title).strip()) < 2: continue
        s_target = safe_float(s.get('target', 0))
        global_sum_tracker[s_title] = {"target": s_target, "actual": 0, "details": []}

    freight_target = safe_float(res_main.get("freight_target", 0))
    freight_actual_sum = 0
    freight_details = []

    # 3. é–‹å§‹é€é …éæ­·
    for item in dimension_data:
        title = item.get("item_title", "")
        page = item.get("page", "?")
        target_pc = safe_float(item.get("item_pc_target", 0))
        
        # ğŸ’¡ [è§£å£“ç¸®æ•¸æ“š]
        ds = item.get("ds", "")
        data_list = [pair.split(":") for pair in ds.split("|") if ":" in pair]
        
        # ğŸ’¡ [åˆå§‹åŒ–æœ¬é …æ•¸é‡] é˜²æ­¢å ±éŒ¯
        actual_item_qty = 0 
        
        # ğŸ’¡ [è‡ªå‹•æŸ¥è¡¨]
        matched_rule = {"local": "", "agg": "", "freight": ""}
        if df_rules is not None:
            for _, row in df_rules.iterrows():
                if fuzz.partial_ratio(str(row.get('Item_Name', '')), title) >= 85:
                    matched_rule = {
                        "local": str(row.get('Unit_Rule_Local', '')),
                        "agg": str(row.get('Unit_Rule_Agg', '')),
                        "freight": str(row.get('Unit_Rule_Freight', ''))
                    }
                    break

        # --- 3.1 å–®é …æ ¸å° ---
        ids = [str(e[0]).strip() for e in data_list if len(e) > 0]
        id_counts = Counter(ids)
        u_local = matched_rule["local"]
        
        if "1SET=4PCS" in u_local: 
            actual_item_qty = len(data_list) / 4
        elif "1SET=2PCS" in u_local: 
            actual_item_qty = len(data_list) / 2
        elif "æœ¬é«”" in title or "PC=PC" in u_local: 
            actual_item_qty = len(set(ids)) # æœ¬é«”å»é‡
        else: 
            actual_item_qty = len(data_list) # è»¸é ¸/å…¶ä»–è¨ˆè¡Œæ•¸

        if actual_item_qty != target_pc and target_pc > 0:
            accounting_issues.append({
                "page": page, "item": title, "issue_type": "çµ±è¨ˆä¸ç¬¦(å–®é …)",
                "common_reason": f"è¦æ±‚ {target_pc}PCï¼Œå…§æ–‡æ ¸ç®—ç‚º {actual_item_qty}",
                "failures": [{"id": "æ¨™é¡Œç›®æ¨™", "val": target_pc}, {"id": "å…§æ–‡å¯¦éš›", "val": actual_item_qty}],
                "source": "ğŸ æœƒè¨ˆå¼•æ“"
            })

        # è»¸é ¸é‡è¤‡æª¢æŸ¥
        if any(k in title for k in ["è»¸é ¸", "å…§å­”", "Journal"]):
            for rid, count in id_counts.items():
                if count >= 3:
                    accounting_issues.append({
                        "page": page, "item": title, "issue_type": "ğŸ›‘ç·¨è™Ÿé‡è¤‡ç•°å¸¸",
                        "common_reason": f"ç·¨è™Ÿ {rid} å‡ºç¾ {count} æ¬¡ï¼Œé•åé™2æ¬¡è¦å®š",
                        "failures": [{"id": rid, "val": f"{count} æ¬¡", "calc": "ç¦æ­¢è¶…é2æ¬¡"}]
                    })

        # --- 3.2 ç¸½è¡¨å°å¸³ (æ”¯æ´ è±å…, 2SET=1PC æ··åˆæŒ‡ä»¤) ---
        u_agg_raw = matched_rule["agg"]
        agg_parts = [p.strip() for p in u_agg_raw.split(",")]
        is_exempt_from_basket = "è±å…" in agg_parts
        
        agg_multiplier = 1.0
        for p in agg_parts:
            conv = re.search(r"(\d+)SET=1PC", p)
            if conv: agg_multiplier = 1.0 / float(conv.group(1))

        for s_title, data in global_sum_tracker.items():
            is_rep = any(k in s_title for k in ["ROLLè»Šä¿®", "å†ç”Ÿ"])
            is_weld = "éŠ²è£œ" in s_title
            is_assem = any(k in s_title for k in ["æ‹†è£", "çµ„è£", "è£é…"])
            
            match = False
            # A æ¨¡å¼ï¼šèšåˆç±ƒå­ (å—è±å…æ¨™ç±¤å½±éŸ¿)
            if (is_rep or is_weld or is_assem) and not is_exempt_from_basket:
                if is_rep and any(k in title for k in ["æœªå†ç”Ÿ", "å†ç”Ÿ", "ç ”ç£¨", "è»Šä¿®"]): match = True
                elif is_weld and "éŠ²è£œ" in title: match = True
                elif is_assem and any(k in title for k in ["æ‹†è£", "çµ„è£", "çœŸåœ“åº¦"]): match = True
            
            # B æ¨¡å¼ï¼šä¸€èˆ¬å°å¸³ (åå­—å°ä¸Šå°±åŠ ç¸½ï¼Œç„¡è¦–è±å…)
            if not match and fuzz.partial_ratio(s_title, title) > 85: match = True

            if match:
                # ä½¿ç”¨å®šç¾©å¥½çš„ actual_item_qty
                item_val_for_summary = actual_item_qty * agg_multiplier
                data["actual"] += item_val_for_summary
                data["details"].append({"id": f"{title} (P.{page})", "val": item_val_for_summary, "calc": "è¨ˆå…¥ç¸½å¸³"})

        # --- 3.3 é‹è²»æ ¸å° ---
        u_fr = matched_rule["freight"]
        if "è¨ˆå…¥" in u_fr or ("æœªå†ç”Ÿ" in title and "æœ¬é«”" in title):
            # é‹è²»ä¹Ÿæ”¯æ´å‹•æ…‹è§£æ XPC=1
            fr_multiplier = 1.0
            conv_fr = re.search(r"(\d+)PC=1", u_fr)
            if conv_fr: fr_multiplier = 1.0 / float(conv_fr.group(1))
            
            val_for_freight = actual_item_qty * fr_multiplier
            freight_actual_sum += val_for_freight
            freight_details.append({"id": f"{title} (P.{page})", "val": val_for_freight, "calc": "è¨ˆå…¥é‹è²»"})

    # 4. çµç®—ç•°å¸¸å ±å‘Š
    for s_title, data in global_sum_tracker.items():
        if abs(data["actual"] - data["target"]) > 0.01 and data["target"] > 0:
            accounting_issues.append({
                "page": "ç¸½è¡¨", "item": s_title, "issue_type": "çµ±è¨ˆä¸ç¬¦(ç¸½å¸³)",
                "common_reason": f"æ¨™è¨» {data['target']} != å¯¦éš› {data['actual']}",
                "failures": [{"id": "ğŸ” çµ±è¨ˆåŸºæº–", "val": data["target"]}] + data["details"] + [{"id": "ğŸ§® å¯¦éš›ç¸½è¨ˆ", "val": data["actual"]}],
                "source": "ğŸ æœƒè¨ˆå¼•æ“"
            })

    if abs(freight_actual_sum - freight_target) > 0.01 and freight_target > 0:
        accounting_issues.append({
            "page": "ç¸½è¡¨", "item": "é‹è²»æ ¸å°", "issue_type": "çµ±è¨ˆä¸ç¬¦(é‹è²»)",
            "common_reason": f"åŸºæº– {freight_target} != å¯¦éš› {freight_actual_sum}",
            "failures": [{"id": "ğŸšš é‹è²»åŸºæº–", "val": freight_target}] + freight_details + [{"id": "ğŸ§® é‹è²»ç¸½è¨ˆ", "val": freight_actual_sum}],
            "source": "ğŸ æœƒè¨ˆå¼•æ“"
        })
        
    return accounting_issues
    
# --- 6. æ‰‹æ©Ÿç‰ˆ UI èˆ‡ æ ¸å¿ƒåŸ·è¡Œé‚è¼¯ ---
st.title("ğŸ­ äº¤è²¨å–®ç¨½æ ¸")

data_source = st.radio(
    "è«‹é¸æ“‡è³‡æ–™ä¾†æºï¼š", 
    ["ğŸ“¸ ä¸Šå‚³ç…§ç‰‡", "ğŸ“‚ ä¸Šå‚³ JSON æª”", "ğŸ“Š ä¸Šå‚³ Excel æª”"], 
    horizontal=True
)

with st.container(border=True):
    # --- æƒ…æ³ A: ä¸Šå‚³ç…§ç‰‡ ---
    if data_source == "ğŸ“¸ ä¸Šå‚³ç…§ç‰‡":
        if st.session_state.get('source_mode') == 'json' or st.session_state.get('source_mode') == 'excel':
            st.session_state.photo_gallery = []
            st.session_state.source_mode = 'image'

        uploaded_files = st.file_uploader(
            "è«‹é¸æ“‡ JPG/PNG ç…§ç‰‡...", 
            type=['jpg', 'png', 'jpeg'], 
            accept_multiple_files=True, 
            key=f"uploader_{st.session_state.uploader_key}"
        )
        
        if uploaded_files:
            for f in uploaded_files: 
                if not any(x['file'].name == f.name for x in st.session_state.photo_gallery if x['file']):
                    st.session_state.photo_gallery.append({
                        'file': f, 
                        'table_md': None, 
                        'header_text': None,
                        'full_text': None,
                        'raw_json': None
                    })
            st.session_state.uploader_key += 1
            if st.session_state.enable_auto_analysis:
                st.session_state.auto_start_analysis = True
            components.html("""<script>window.parent.document.body.scrollTo(0, window.parent.document.body.scrollHeight);</script>""", height=0)
            st.rerun()

    # --- æƒ…æ³ B: ä¸Šå‚³ JSON ---
    elif data_source == "ğŸ“‚ ä¸Šå‚³ JSON æª”":
        st.info("ğŸ’¡ è«‹é»æ“Šä¸‹æ–¹æŒ‰éˆ•ï¼Œå¾ä½ çš„è³‡æ–™å¤¾é¸æ“‡ä¹‹å‰ä¸‹è¼‰çš„ `.json` æª”ã€‚")
        uploaded_json = st.file_uploader("ä¸Šå‚³JSONæª”", type=['json'], key="json_uploader")
        
        if uploaded_json:
            try:
                current_file_name = uploaded_json.name
                if st.session_state.get('last_loaded_json_name') != current_file_name:
                    json_data = json.load(uploaded_json)
                    st.session_state.photo_gallery = []
                    st.session_state.source_mode = 'json'
                    st.session_state.last_loaded_json_name = current_file_name
                    
                    import re
                    for page in json_data:
                        real_page = "Unknown"
                        full_text = page.get('full_text', '')
                        if full_text:
                            match = re.search(r"(?:é …æ¬¡|Page|é æ¬¡|NO\.)[:\s]*(\d+)\s*[/ï¼]\s*\d+", full_text, re.IGNORECASE)
                            if match:
                                real_page = match.group(1)
                        
                        st.session_state.photo_gallery.append({
                            'file': None,
                            'table_md': page.get('table_md'),
                            'header_text': page.get('header_text'),
                            'full_text': full_text,
                            'raw_json': page.get('raw_json'),
                            'real_page': real_page
                        })
                    
                    st.toast(f"âœ… æˆåŠŸè¼‰å…¥ JSON: {current_file_name}", icon="ğŸ“‚")
                    if st.session_state.enable_auto_analysis:
                        st.session_state.auto_start_analysis = True
                    st.rerun()
                else:
                    st.success(f"ğŸ“‚ ç›®å‰è¼‰å…¥ JSONï¼š**{uploaded_json.name}**")
            except Exception as e:
                st.error(f"JSON æª”æ¡ˆæ ¼å¼éŒ¯èª¤: {e}")

    # --- æƒ…æ³ C: ä¸Šå‚³ Excel (æ–°å¢çš„æ”¾åœ¨é€™) ---
    elif data_source == "ğŸ“Š ä¸Šå‚³ Excel æª”":
        st.info("ğŸ’¡ ä¸Šå‚³ Excel æª”å¾Œï¼Œç³»çµ±æœƒå°‡è¡¨æ ¼å…§å®¹è½‰æ›ç‚ºæ–‡å­—ä¾› AI ç¨½æ ¸ã€‚")
        uploaded_xlsx = st.file_uploader("ä¸Šå‚³ Excel æª”", type=['xlsx', 'xls'], key="xlsx_uploader")
        
        if uploaded_xlsx:
            try:
                current_file_name = uploaded_xlsx.name
                if st.session_state.get('last_loaded_xlsx_name') != current_file_name:
                    df_dict = pd.read_excel(uploaded_xlsx, sheet_name=None)
                    st.session_state.photo_gallery = []
                    st.session_state.source_mode = 'excel'
                    st.session_state.last_loaded_xlsx_name = current_file_name
                    
                    for sheet_name, df in df_dict.items():
                        df = df.fillna("")
                        md_table = df.to_markdown(index=False)
                        st.session_state.photo_gallery.append({
                            'file': None,
                            'table_md': md_table,
                            'header_text': f"ä¾†æºåˆ†é : {sheet_name}",
                            'full_text': f"Excel å…§å®¹ - åˆ†é  {sheet_name}\n" + md_table,
                            'raw_json': None,
                            'real_page': sheet_name
                        })
                    st.toast(f"âœ… æˆåŠŸè¼‰å…¥ Excel: {current_file_name}", icon="ğŸ“Š")
                    if st.session_state.enable_auto_analysis:
                        st.session_state.auto_start_analysis = True
                    st.rerun()
                else:
                    st.success(f"ğŸ“Š ç›®å‰è¼‰å…¥ Excelï¼š**{uploaded_xlsx.name}**")
            except Exception as e:
                st.error(f"Excel è®€å–å¤±æ•—: {e}")

if st.session_state.photo_gallery:
    st.caption(f"å·²ç´¯ç© {len(st.session_state.photo_gallery)} é æ–‡ä»¶")
    col_btn1, col_btn2 = st.columns([1, 1], gap="small")
    with col_btn1: start_btn = st.button("ğŸš€ é–‹å§‹åˆ†æ", type="primary", use_container_width=True)
    with col_btn2: 
        clear_btn = st.button("ğŸ—‘ï¸ç…§ç‰‡æ¸…é™¤", help="æ¸…é™¤", use_container_width=True)

    if clear_btn:
        st.session_state.photo_gallery = []
        st.session_state.analysis_result_cache = None
        if 'last_loaded_json_name' in st.session_state:
            del st.session_state.last_loaded_json_name 
        st.rerun()

    is_auto_start = st.session_state.auto_start_analysis
    if is_auto_start:
        st.session_state.auto_start_analysis = False

    if 'analysis_result_cache' not in st.session_state:
        st.session_state.analysis_result_cache = None

    trigger_analysis = start_btn or is_auto_start

    if trigger_analysis:
        total_start = time.time()
        status = st.empty()
        progress_bar = st.progress(0)
            
        extracted_data_list = [None] * len(st.session_state.photo_gallery)
        full_text_for_search = ""
        total_imgs = len(st.session_state.photo_gallery)
            
        ocr_start = time.time()

        def process_image_task(index, item):
            index = int(index)
            # å¦‚æœå·²ç¶“æœ‰è³‡æ–™äº†å°±ä¸é‡è¤‡æƒæ
            if item.get('table_md') and item.get('header_text') and item.get('full_text'):
                real_page = item.get('real_page', str(index + 1))
                return index, item['table_md'], item['header_text'], item['full_text'], None, real_page, None
    
            try:
                if item.get('file') is None:
                    return index, None, None, None, None, None, "ç„¡åœ–ç‰‡æª”æ¡ˆ"
                
                item['file'].seek(0)
                # é€™è£¡æœƒæ¥åˆ°æˆ‘å€‘å‰›æ‰ä¿®æ”¹å¾Œå›å‚³çš„ None
                table_md, header, full, _, real_page = extract_layout_with_azure(item['file'], DOC_ENDPOINT, DOC_KEY)
                return index, table_md, header, full, None, real_page, None
            except Exception as e:
                return index, None, None, None, None, None, f"OCRå¤±æ•—: {str(e)}"

        status.text(f"Azure æ­£åœ¨å¹³è¡Œæƒæ {total_imgs} é æ–‡ä»¶...")

        with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
            futures = []
            for i, item in enumerate(st.session_state.photo_gallery):
                futures.append(executor.submit(process_image_task, i, item))
            
            completed_count = 0
            for future in concurrent.futures.as_completed(futures):
                idx, t_md, h_txt, f_txt, raw_j, r_page, err = future.result()
                idx = int(idx)
                
                if err:
                    st.error(f"ç¬¬ {idx+1} é è®€å–å¤±æ•—: {err}")
                    extracted_data_list[idx] = None
                else:
                    st.session_state.photo_gallery[idx]['table_md'] = t_md
                    st.session_state.photo_gallery[idx]['header_text'] = h_txt
                    st.session_state.photo_gallery[idx]['full_text'] = f_txt
                    st.session_state.photo_gallery[idx]['raw_json'] = raw_j
                    st.session_state.photo_gallery[idx]['real_page'] = r_page
                    st.session_state.photo_gallery[idx]['file'] = None
                    
                    extracted_data_list[idx] = {
                        "page": r_page,
                        "table": t_md or "", 
                        "header_text": h_txt or ""
                    }
                
                completed_count += 1
                progress_bar.progress(completed_count / (total_imgs + 1))
        
        for i, data in enumerate(extracted_data_list):
            if data and isinstance(data, dict):
                page_idx = i
                if 0 <= page_idx < len(st.session_state.photo_gallery):
                    full_text_for_search += st.session_state.photo_gallery[page_idx].get('full_text', '')

        ocr_end = time.time()
        ocr_duration = ocr_end - ocr_start

        combined_input = "ä»¥ä¸‹æ˜¯å„é è³‡æ–™ï¼š\n"
        for i, data in enumerate(extracted_data_list):
            if data is None: continue
            page_num = data.get('page', i+1)
            table_text = data.get('table', '')
            header_text = data.get('header_text', '')
            combined_input += f"\n=== Page {page_num} ===\nã€é é¦–ã€‘:\n{header_text}\nã€è¡¨æ ¼ã€‘:\n{table_text}\n"
            
        status.text("ç¸½ç¨½æ ¸ Agent æ­£åœ¨é€²è¡Œå…¨æ–¹ä½åˆ†æ...")
        
        # 1. åŸ·è¡Œ AI 
        res_main = agent_unified_check(combined_input, combined_input, GEMINI_KEY, main_model_name)
        
        # ğŸ’¡ [é‡å¤§ä¿®æ­£]ï¼šå¾ AI å›å‚³ä¸­æŠ“å–ç¶­åº¦æ•¸æ“š
        dim_data = res_main.get("dimension_data", [])
        
        # 2. åŸ·è¡Œä¸‰å€‹ Python å¼•æ“ (æ•¸å€¼ã€æœƒè¨ˆã€æµç¨‹)
        python_numeric_issues = python_numerical_audit(dim_data)
        python_accounting_issues = python_accounting_audit(dim_data, res_main)
        
        # ğŸ’¡ [æ–°å¢]ï¼šå•Ÿå‹• Python æµç¨‹ç¨½æ ¸å¼•æ“
        python_process_issues = python_process_audit(dim_data)
        
        # 3. åˆä½µçµæœ (å¸¶æœ‰é˜²å‘†æª¢æŸ¥ï¼Œä¸¦ç¢ºä¿æ¬ŠåŠ›å¾¹åº•ç§»äº¤) ---
        ai_raw_issues = res_main.get("issues", [])
        ai_filtered_issues = []

        if isinstance(ai_raw_issues, list):
            for i in ai_raw_issues:
                if isinstance(i, dict):
                    i['source'] = 'ğŸ¤– ç¸½ç¨½æ ¸ AI'
                    i_type = str(i.get("issue_type", ""))
                    
                    # ğŸ’¡ [é—œéµä¿®æ­£]ï¼š
                    # æˆ‘å€‘åªä¿ç•™ AI ç™¼ç¾çš„ï¼šè¦æ ¼æå–å¤±æ•—ã€æœªåŒ¹é…è¦å‰‡ã€é‚„æœ‰è¡¨é ­è³‡è¨Šä¸ç¬¦ã€‚
                    # ã€Œæµç¨‹ã€å’Œã€Œçµ±è¨ˆã€å·²ç¶“å®Œå…¨äº¤çµ¦ Python å¼•æ“äº†ï¼Œæ‰€ä»¥é€™è£¡çµ•å°ä¸ç•™ AI å ±çš„ã€‚
                    ai_tasks_to_keep = ["è¦æ ¼æå–å¤±æ•—", "æœªåŒ¹é…", "è¡¨é ­"]
                    if any(k in i_type for k in ai_tasks_to_keep):
                        ai_filtered_issues.append(i)
                else:
                    # å¦‚æœ AI å›å‚³æ ¼å¼å´©æ½°ï¼Œè‡³å°‘ä¿ç•™åŸå§‹æ–‡å­—ä¾›æª¢æŸ¥
                    ai_filtered_issues.append({
                        "page": "?", "item": "AI å›å‚³è§£æç•°å¸¸", "issue_type": "âš ï¸æ ¼å¼éŒ¯èª¤",
                        "common_reason": f"åŸå§‹å…§å®¹: {str(i)}", "source": "ğŸ¤– ç¸½ç¨½æ ¸ AI"
                    })

        # 4. å–å¾— Python è¡¨é ­æª¢æŸ¥ (æ—¥æœŸã€å·¥ä»¤ç­‰)
        python_header_issues, python_debug_data = python_header_check(st.session_state.photo_gallery)
        
        # æœ€çµ‚åˆä½µï¼šAI(æå–è­¦å‘Š) + Python(æ•¸å€¼) + Python(æœƒè¨ˆ) + Python(æµç¨‹) + Python(è¡¨é ­)
        all_issues = ai_filtered_issues + python_numeric_issues + python_accounting_issues + python_process_issues + python_header_issues
        
        # 5. å­˜å…¥å¿«å– (é€™æ˜¯ Debug é é¢èƒ½é¡¯ç¤ºæ•¸æ“šçš„å”¯ä¸€é—œéµ)
        st.session_state.analysis_result_cache = {
            "job_no": res_main.get("job_no", "Unknown"),
            "all_issues": all_issues,
            "total_duration": time.time() - total_start,
            "cost_twd": (res_main.get("_token_usage",{}).get("input",0)*0.5 + res_main.get("_token_usage",{}).get("output",0)*3.0)/1000000*32.5,
            "total_in": res_main.get("_token_usage",{}).get("input", 0),
            "total_out": res_main.get("_token_usage",{}).get("output", 0),
            "ocr_duration": ocr_duration,
            "time_eng": time.time() - total_start - ocr_duration,
            "full_text_for_search": combined_input,
            "combined_input": combined_input,
            "python_debug_data": python_debug_data,
            # âœ… é€™è¡Œæ²’åŠ ï¼ŒDebug é é¢å°±æ˜¯ç©ºçš„ï¼
            "ai_extracted_data": dim_data 
        }
        
    if st.session_state.analysis_result_cache:
        cache = st.session_state.analysis_result_cache
        all_issues = cache['all_issues']
        
        st.success(f"å·¥ä»¤: {cache['job_no']} | â±ï¸ {cache['total_duration']:.1f}s")
        st.info(f"ğŸ’° æœ¬æ¬¡æˆæœ¬: NT$ {cache['cost_twd']:.2f} (In: {cache['total_in']:,} / Out: {cache['total_out']:,})")
        st.caption(f"ç´°ç¯€è€—æ™‚: Azure OCR {cache['ocr_duration']:.1f}s | AI åˆ†æ {cache['time_eng']:.1f}s")
        
        with st.expander("ğŸ” æŸ¥çœ‹ AI è®€å–åˆ°çš„ Excel è¦å‰‡ (Debug)"):
            rules_text = get_dynamic_rules(cache['full_text_for_search'], debug_mode=True)
            if "ç„¡ç‰¹å®šè¦å‰‡" in rules_text:
                st.caption("ç„¡åŒ¹é…è¦å‰‡")
            else:
                st.markdown(rules_text)
                
        # --- æ–°å¢çš„ Debug å±•é–‹é  ---
        with st.expander("ğŸ”¬ æŸ¥çœ‹ AI æŠ„éŒ„çµ¦ Python çš„åŸå§‹æ•¸æ“š (æª¢æŸ¥æ‰‹å¯«éæ¿¾)", expanded=False):
            raw_dim_data = cache.get("ai_extracted_data", [])
            if raw_dim_data:
                st.write("é€™æ˜¯ AI æŠ„éŒ„ä¸¦ç¿»è­¯å¾Œçš„ JSONï¼ˆåŒ…å«æ ¼å¼æ˜¯å¦æ­£ç¢ºã€æ•¸å­—æ˜¯å¦è¢«ç°¡åŒ–ï¼‰ï¼š")
                st.json(raw_dim_data)
            else:
                st.caption("ç„¡æ•¸æ“šæå–è³‡æ–™ã€‚")

        with st.expander("ğŸ æŸ¥çœ‹ Python ç¡¬é‚è¼¯åµæ¸¬çµæœ (Debug)", expanded=False):
            if cache.get('python_debug_data'):
                p_data = cache['python_debug_data']
                standard_data = {}
                all_values = {"å·¥ä»¤ç·¨è™Ÿ": [], "é å®šäº¤è²¨": [], "å¯¦éš›äº¤è²¨": []}
                for page in p_data:
                    for k in all_values.keys():
                        if page.get(k) and page[k] != "N/A":
                            all_values[k].append(page[k])
                
                standard_row = {"é ç¢¼": "ğŸ† åˆ¤å®šæ¨™æº–"}
                for k, v in all_values.items():
                    if v:
                        standard_row[k] = Counter(v).most_common(1)[0][0]
                    else:
                        standard_row[k] = "N/A"
                
                final_df_data = [standard_row] + p_data
                st.dataframe(final_df_data, use_container_width=True, hide_index=True)
                st.info("ğŸ’¡ ã€Œåˆ¤å®šæ¨™æº–ã€æ˜¯ä¾æ“šå¤šæ•¸æ±ºç”¢ç”Ÿçš„ã€‚")
            else:
                st.caption("ç„¡åµæ¸¬è³‡æ–™")

        real_errors = [i for i in all_issues if "æœªåŒ¹é…" not in i.get('issue_type', '')]
        
        if not real_errors:
            st.balloons()
            if not all_issues:
                st.success("âœ… å…¨æ•¸åˆæ ¼ï¼")
            else:
                st.success(f"âœ… æ•¸å€¼å…¨æ•¸åˆæ ¼ï¼ (ä½†æœ‰ {len(all_issues)} å€‹é …ç›®æœªåŒ¹é…è¦å‰‡ï¼Œè«‹æª¢æŸ¥)")
        else:
            st.error(f"ç™¼ç¾ {len(real_errors)} é¡æ•¸å€¼ç•°å¸¸ï¼Œå¦æœ‰ {len(all_issues) - len(real_errors)} å€‹é …ç›®æœªåŒ¹é…è¦å‰‡")

        for item in all_issues:
            with st.container(border=True):
                c1, c2 = st.columns([3, 1])
                
                source_label = item.get('source', '')
                issue_type = item.get('issue_type', 'ç•°å¸¸')
                
                c1.markdown(f"**P.{item.get('page', '?')} | {item.get('item')}**  `{source_label}`")
                
                # é¡è‰²æ§åˆ¶ï¼šæœƒè¨ˆçµ±è¨ˆé¡ç”¨ç´…è‰²ï¼Œè¦æ ¼é¡ç”¨é»ƒè‰²
                if "çµ±è¨ˆ" in issue_type or "æ•¸é‡" in issue_type or "æµç¨‹" in issue_type:
                    c2.error(f"ğŸ›‘ {issue_type}")
                else:
                    c2.warning(f"âš ï¸ {issue_type}")
                
                st.caption(f"åŸå› : {item.get('common_reason', '')}")
                
                # --- æ¸²æŸ“è¡¨æ ¼ (æœƒè¨ˆå°å¸³å–®) ---
                failures = item.get('failures', [])
                if failures:
                    table_data = []
                    for f in failures:
                        if isinstance(f, dict):
                            # æˆ‘å€‘çµ±ä¸€ä½¿ç”¨é€™å››å€‹æ¬„ä½æ¨™é¡Œï¼Œæœƒè¨ˆèˆ‡å·¥ç¨‹å…±ç”¨
                            row = {
                                "é …ç›®/æ»¾è¼ªç·¨è™Ÿ": f.get('id', 'æœªçŸ¥'), 
                                "å¯¦æ¸¬/è¨ˆæ•¸": f.get('val', 'N/A'),
                                "æ¨™æº–/å‚™è¨»": f.get('target', ''), # å·¥ç¨‹ç”¨
                                "åˆ¤å®šç®—å¼/ç‹€æ…‹": f.get('calc', '') # æœƒè¨ˆç”¨
                            }
                            # å¦‚æœæ˜¯æœƒè¨ˆæ¨¡å¼ï¼ŒæŠŠ target ç•™ç©ºï¼Œè³‡è¨Šä¸»è¦åœ¨ id å’Œ val
                            table_data.append(row)
                    
                    if table_data:
                        st.dataframe(table_data, use_container_width=True, hide_index=True)
                else:
                    # å¦‚æœæ²’æœ‰ failuresï¼Œè‡³å°‘é¡¯ç¤ºä¸€å€‹æ•¸æ“šæç¤º
                    st.info(f"è©³ç´°æ•¸æ“šè¦‹ä¸Šè¿°åŸå› èªªæ˜")
        
        st.divider()

        current_job_no = cache.get('job_no', 'Unknown')
        safe_job_no = current_job_no.replace("/", "_").replace("\\", "_").strip()
        file_name_str = f"{safe_job_no}_cleaned.json"

        # æº–å‚™åŒ¯å‡ºè³‡æ–™
        export_data = []
        for item in st.session_state.photo_gallery:
            export_data.append({
                "table_md": item.get('table_md'),
                "header_text": item.get('header_text'),
                "full_text": item.get('full_text'),
                "raw_json": item.get('raw_json')
            })
        json_str = json.dumps(export_data, indent=2, ensure_ascii=False)

        st.subheader("ğŸ’¾ æ¸¬è©¦è³‡æ–™å­˜æª”")
        st.caption(f"å·²è­˜åˆ¥å·¥ä»¤ï¼š**{current_job_no}**ã€‚ä¸‹è¼‰å¾Œå¯ä¾›ä¸‹æ¬¡æ¸¬è©¦ä½¿ç”¨ã€‚")
        
        st.download_button(
            label=f"â¬‡ï¸ ä¸‹è¼‰æ¸¬è©¦è³‡æ–™ ({file_name_str})",
            data=json_str,
            file_name=file_name_str,
            mime="application/json",
            type="primary"
        )

        with st.expander("ğŸ‘€ æŸ¥çœ‹å‚³çµ¦ AI çš„æœ€çµ‚æ–‡å­— (Prompt Input)"):
            st.caption("é€™æ‰æ˜¯ AI çœŸæ­£è®€åˆ°çš„å…§å®¹ (å·²éæ¿¾é›œè¨Š)ï¼š")
            st.code(cache['combined_input'], language='markdown')
    
    if st.session_state.photo_gallery and st.session_state.get('source_mode') != 'json':
        st.caption("å·²æ‹æ”ç…§ç‰‡ï¼š")
        cols = st.columns(4)
        for idx, item in enumerate(st.session_state.photo_gallery):
            with cols[idx % 4]:
                if item.get('file'):
                    st.image(item['file'], caption=f"P.{idx+1}", use_container_width=True)
                if st.button("âŒ", key=f"del_{idx}"):
                    st.session_state.photo_gallery.pop(idx)
                    st.session_state.analysis_result_cache = None
                    st.rerun()
else:
    st.info("ğŸ‘† è«‹é»æ“Šä¸Šæ–¹æŒ‰éˆ•é–‹å§‹æ–°å¢ç…§ç‰‡")
