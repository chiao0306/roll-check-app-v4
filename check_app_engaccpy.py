import streamlit as st
import streamlit.components.v1 as components
from azure.core.credentials import AzureKeyCredential
from azure.ai.documentintelligence import DocumentIntelligenceClient
from azure.ai.documentintelligence.models import AnalyzeResult
import google.generativeai as genai
from openai import OpenAI
import json
import time
import concurrent.futures
import pandas as pd
from thefuzz import fuzz
from collections import Counter
import re

# --- 1. é é¢è¨­å®š ---
st.set_page_config(page_title="äº¤è²¨å–®ç¨½æ ¸", page_icon="ğŸ­", layout="centered")

# --- CSS æ¨£å¼ ---
st.markdown("""
<style>
/* 1. æ¨™é¡Œå¤§å°æ§åˆ¶ */
h1 {
    font-size: 1.7rem !important; 
    white-space: nowrap !important;
    overflow: hidden !important; 
    text-overflow: ellipsis !important;
}

/* 2. ä¸»åŠŸèƒ½æŒ‰éˆ• (ç´…è‰² Primary) -> è®Šå¤§ã€è®Šé«˜ */
/* é€™æœƒå½±éŸ¿ã€Œé–‹å§‹åˆ†æã€å’Œã€Œç…§ç‰‡æ¸…é™¤ã€ */
button[kind="primary"] {
    height: 60px;               
    font-size: 20px !important; 
    font-weight: bold !important;
    border-radius: 10px !important;
    margin-top: 0px !important;    
    margin-bottom: 5px !important; 
    width: 100%;                
}

/* 3. æ¬¡è¦æŒ‰éˆ• (ç°è‰² Secondary) -> ä¿æŒåŸç‹€ */
/* é€™æœƒå½±éŸ¿æ¯ä¸€å¼µç…§ç‰‡ä¸‹é¢çš„ã€ŒXã€æŒ‰éˆ•ï¼Œè®“å®ƒç¶­æŒå°å°çš„ */
button[kind="secondary"] {
    height: auto !important;
    font-weight: normal !important;
}
</style>
""", unsafe_allow_html=True)
# --- 2. ç§˜å¯†é‡‘é‘°è®€å– ---
try:
    DOC_ENDPOINT = st.secrets["DOC_ENDPOINT"]
    DOC_KEY = st.secrets["DOC_KEY"]
    GEMINI_KEY = st.secrets["GEMINI_KEY"]
    OPENAI_KEY = st.secrets.get("OPENAI_KEY", "")
except:
    st.error("æ‰¾ä¸åˆ°é‡‘é‘°ï¼è«‹åœ¨ Streamlit Cloud è¨­å®š Secretsã€‚")
    st.stop()

# --- 3. åˆå§‹åŒ– Session State ---
if 'photo_gallery' not in st.session_state: st.session_state.photo_gallery = []
if 'uploader_key' not in st.session_state: st.session_state.uploader_key = 0
if 'auto_start_analysis' not in st.session_state: st.session_state.auto_start_analysis = False

# --- å´é‚Šæ¬„æ¨¡å‹è¨­å®š (åˆä½µç‚ºå–®ä¸€é¸æ“‡) ---
with st.sidebar:
    st.header("æ¨¡å‹è¨­å®š")
    
    # é€™è£¡åŠ å…¥æœ€æ–°çš„ Gemini æ¨¡å‹
    model_options = {
        "Gemini 3 Flash preview": "gemini-3-flash-preview",
        "Gemini 2.5 Flash": "models/gemini-2.5-flash",
        "Gemini 2.5 Flash Lite": "gemini-2.5-flash-lite",
        "Gemini 2.5 Pro": "models/gemini-2.5-pro",
        "GPT-5 Mini": "models/gpt-5-mini-2025-08-07",
        "GPT-5 Nano": "models/gpt-5-nano-2025-08-07",
        
    }
    options_list = list(model_options.keys())
    
    st.subheader("ğŸ¤– ç¸½ç¨½æ ¸ Agent")
    model_selection = st.selectbox(
        "è² è²¬ï¼šè¦æ ¼ã€è£½ç¨‹ã€æ•¸é‡ã€çµ±è¨ˆå…¨åŒ…", 
        options=options_list, 
        index=1, 
        key="main_model"
    )
    main_model_name = model_options[model_selection]
    
    st.divider()
    
    default_auto = st.query_params.get("auto", "true") == "true"
    def update_url_param():
        current_state = "true" if st.session_state.enable_auto_analysis else "false"
        st.query_params["auto"] = current_state

    st.toggle(
        "âš¡ ä¸Šå‚³å¾Œè‡ªå‹•åˆ†æ", 
        value=default_auto, 
        key="enable_auto_analysis", 
        on_change=update_url_param
    )

# --- Excel è¦å‰‡è®€å–å‡½æ•¸ (å°ˆæ¥­æ¥µç°¡ç‰ˆ) ---
@st.cache_data
def get_dynamic_rules(ocr_text, debug_mode=False):
    try:
        df = pd.read_excel("rules.xlsx")
        df.columns = [c.strip() for c in df.columns]
        ocr_text_clean = str(ocr_text).upper().replace(" ", "").replace("\n", "")
        
        ai_prompt_list = []    # çµ¦ AI çš„ (ç´”æ–‡å­—)
        debug_view_list = []   # çµ¦äººçœ‹çš„ (æ’ç‰ˆæ¸…æ½”)

        for index, row in df.iterrows():
            item_name = str(row.get('Item_Name', '')).strip()
            if not item_name or "(é€šç”¨)" in item_name: continue
            
            score = fuzz.partial_ratio(item_name.upper().replace(" ", ""), ocr_text_clean)
            if score >= 85:
                # å–å€¼èˆ‡æ¸…æ´—
                def clean(v): return str(v).strip() if v and str(v) != 'nan' else None
                
                spec = clean(row.get('Standard_Spec', ''))
                logic = clean(row.get('Logic_Prompt', ''))
                u_fr = clean(row.get('Unit_Rule_Freight', ''))
                u_loc = clean(row.get('Unit_Rule_Local', ''))
                u_agg = clean(row.get('Unit_Rule_Agg', ''))

                # --- A. å»ºæ§‹ AI Prompt (ç¶­æŒä¸è®Š) ---
                if not debug_mode:
                    if spec or logic:
                        desc = f"- [åƒè€ƒè³‡è¨Š] {item_name}\n"
                        if spec: desc += f"  - æ¨™æº–è¦æ ¼: {spec}\n"
                        if logic: desc += f"  - æ³¨æ„äº‹é …: {logic}\n"
                        ai_prompt_list.append(desc)
                
                # --- B. å»ºæ§‹ Debug é¡¯ç¤º (å»é™¤åœ–æ¡ˆï¼Œæ”¹ç”¨è¡¨æ ¼æ„Ÿæ’ç‰ˆ) ---
                else:
                    # ä½¿ç”¨ Markdown çš„å¼•ç”¨å€å¡Š (>) ä¾†åšå±¤ç´šå€åˆ†ï¼Œçœ‹èµ·ä¾†å¾ˆä¹¾æ·¨
                    block = f"#### â–  {item_name} (åŒ¹é…åº¦ {score}%)\n"
                    
                    # AI å€å¡Š
                    block += "**[ AI Prompt è¼¸å…¥ ]**\n"
                    if spec or logic:
                        if spec: block += f"- è¦æ ¼æ¨™æº– : `{spec}`\n"
                        if logic: block += f"- æ³¨æ„äº‹é … : `{logic}`\n"
                    else:
                        block += "- (ç„¡ç‰¹å®šè¼¸å…¥)\n"

                    # Python å€å¡Š
                    block += "\n**[ Python ç¡¬é‚è¼¯è¨­å®š ]**\n"
                    has_py = False
                    if u_fr: 
                        block += f"- é‹è²»é‚è¼¯ : `{u_fr}`\n"
                        has_py = True
                    if u_loc:
                        block += f"- å–®é …è¦å‰‡ : `{u_loc}`\n"
                        has_py = True
                    if u_agg:
                        block += f"- èšåˆè¦å‰‡ : `{u_agg}`\n"
                        has_py = True
                    
                    if not has_py:
                        block += "- (ä½¿ç”¨é è¨­é‚è¼¯)\n"
                    
                    block += "\n---\n"
                    debug_view_list.append(block)

        if debug_mode:
            if not debug_view_list: return "ç„¡ç‰¹å®šè¦å‰‡å‘½ä¸­ã€‚"
            return "\n".join(debug_view_list)
        else:
            return "\n".join(ai_prompt_list) if ai_prompt_list else ""

    except Exception as e:
        return f"è®€å–éŒ¯èª¤: {e}"

# --- 4. æ ¸å¿ƒå‡½æ•¸ï¼šAzure ç¥ä¹‹çœ¼ ---
def extract_layout_with_azure(file_obj, endpoint, key):
    client = DocumentIntelligenceClient(endpoint=endpoint, credential=AzureKeyCredential(key))
    file_content = file_obj.getvalue()
    
    poller = client.begin_analyze_document("prebuilt-layout", file_content, content_type="application/octet-stream")
    result: AnalyzeResult = poller.result()
    
    markdown_output = ""
    full_content_text = ""
    real_page_num = "Unknown"
    
    bottom_stop_keywords = ["æ³¨æ„äº‹é …", "ä¸­æ©Ÿå“æª¢å–®ä½", "ä¿å­˜æœŸé™", "è¡¨å–®ç·¨è™Ÿ", "FORM NO", "ç°½ç« "]
    top_right_noise_keywords = [
        "æª¢é©—é¡åˆ¥", "å°ºå¯¸æª¢é©—", "ä¾åœ–é¢æ¨™è¨˜", "ææ–™æª¢é©—", "æˆä»½åˆ†æ", 
        "éç ´å£æ€§", "æ­£å¸¸åŒ–", "é€€ç«", "æ·¬.å›ç«", "è¡¨é¢ç¡¬åŒ–", "è©¦è»Š",
        "æ€§èƒ½æ¸¬è©¦", "è©¦å£“è©¦æ¼", "å‹•.éœå¹³è¡¡è©¦é©—", ":selected:", ":unselected:",
        "æŠ—æ‹‰", "ç¡¬åº¦è©¦é©—", "UT", "PT", "MT"
    ]
    
    if result.tables:
        for idx, table in enumerate(result.tables):
            # 1. å–å¾—é ç¢¼ (ä¿ç•™åŸé‚è¼¯)
            page_num = "Unknown"
            if table.bounding_regions: page_num = table.bounding_regions[0].page_number
            
            # =========================================================
            # ğŸ” [æ–°å¢] æ™ºæ…§æ¨™ç±¤åµæ¸¬ï¼šåœ¨è™•ç†è¡¨æ ¼å‰ï¼Œå…ˆåˆ¤æ–·å®ƒæ˜¯èª°
            # =========================================================
            table_tag = "æœªçŸ¥è¡¨æ ¼"
            
            # æŠ€å·§ï¼šæŠ“å–è¡¨æ ¼ã€Œç¬¬ä¸€åˆ— (row_index=0)ã€çš„æ‰€æœ‰æ–‡å­—ä¾†åˆ¤æ–·
            # é€™æ¨£ä¸ç”¨è®€å®Œæ•´å¼µè¡¨ï¼Œåªè¦çœ‹è¡¨é ­å°±çŸ¥é“å®ƒæ˜¯ç¸½è¡¨é‚„æ˜¯æ˜ç´°
            first_cells = [c.content for c in table.cells if c.row_index == 0]
            first_row_text = "".join(first_cells)
            
            # å®šç¾©é—œéµå­— (æ‚¨å¯ä»¥æ ¹æ“šå¯¦éš›è¡¨æ ¼å¾®èª¿)
            summary_keywords = ["å¯¦äº¤", "ç”³è«‹", "åç¨±åŠè¦ç¯„", "å®Œæˆäº¤è²¨æ—¥æœŸ", "å­˜æ”¾ä½ç½®"]
            detail_keywords = ["è¦ç¯„æ¨™æº–", "æª¢é©—ç´€éŒ„", "å¯¦æ¸¬", "ç·¨è™Ÿ", "å°ºå¯¸", "W3 #", "å…¬å·®"]

            if any(k in first_row_text for k in summary_keywords):
                table_tag = "SUMMARY_TABLE (ç¸½è¡¨)"
            elif any(k in first_row_text for k in detail_keywords):
                table_tag = "DETAIL_TABLE (æ˜ç´°è¡¨)"
            
            # ğŸ“ [ä¿®æ”¹] è¼¸å‡ºæ¨™é ­ï¼šé€™è£¡ä¸å†åªå¯« Table Xï¼Œè€Œæ˜¯åŠ ä¸Šæˆ‘å€‘åˆ¤æ–·çš„æ¨™ç±¤
            # åŠ ä¸Š "===" æ˜¯ç‚ºäº†è®“ Prompt è£¡çš„ã€Œæ³¨æ„ç¯„åœã€æŒ‡ä»¤èƒ½ç²¾æº–é–å®š
            markdown_output += f"\n\n=== [{table_tag} | Page {page_num}] ===\n"
            # =========================================================

            rows = {}
            stop_processing_table = False 
            
            # --- ä»¥ä¸‹ä¿ç•™æ‚¨åŸæœ¬çš„ Cell è™•ç†é‚è¼¯ï¼Œå®Œå…¨ä¸ç”¨å‹• ---
            for cell in table.cells:
                if stop_processing_table: break
                content = cell.content.replace("\n", " ").strip()
                
                for kw in bottom_stop_keywords:
                    if kw in content:
                        stop_processing_table = True
                        break
                if stop_processing_table: break
                
                is_noise = False
                for kw in top_right_noise_keywords:
                    if kw in content:
                        is_noise = True
                        break
                if is_noise: content = "" 

                r, c = cell.row_index, cell.column_index
                if r not in rows: rows[r] = {}
                rows[r][c] = content
            
            for r in sorted(rows.keys()):
                row_cells = []
                if rows[r]:
                    max_col = max(rows[r].keys())
                    for c in range(max_col + 1): 
                        row_cells.append(rows[r].get(c, ""))
                    markdown_output += "| " + " | ".join(row_cells) + " |\n"

    if result.content:
        match = re.search(r"(?:é …æ¬¡|Page|é æ¬¡|NO\.)[:\s]*(\d+)\s*[/ï¼]\s*\d+", result.content, re.IGNORECASE)
        if match:
            real_page_num = match.group(1)

        cut_index = len(result.content)
        for keyword in bottom_stop_keywords:
            idx = result.content.find(keyword)
            if idx != -1 and idx < cut_index:
                cut_index = idx
        
        temp_text = result.content[:cut_index]
        for noise in top_right_noise_keywords:
            temp_text = temp_text.replace(noise, "")
            
        full_content_text = temp_text
        header_snippet = full_content_text[:800]
    else:
        full_content_text = ""
        header_snippet = ""

    return markdown_output, header_snippet, full_content_text, None, real_page_num

# --- Python ç¡¬é‚è¼¯ï¼šè¡¨é ­ä¸€è‡´æ€§æª¢æŸ¥ (é•·åº¦æ•æ„Ÿç‰ˆ) ---
def python_header_check(photo_gallery):
    issues = []
    if not photo_gallery:
        return issues, []

    # å®šç¾© Regex (é‡å° "å»ç©ºç™½+å»æ›è¡Œ" å¾Œçš„å­—ä¸²è¨­è¨ˆ)
    patterns = {
        # ã€ä¿®æ”¹é» 1ã€‘å·¥ä»¤ Regex æ”¾å¯¬ï¼š
        # åŸæœ¬åªæŠ“ W é–‹é ­ï¼Œç¾åœ¨æ”¹æŠ“ "ç·¨è™Ÿ" å¾Œé¢æ¥çš„ "ä»»ä½•è‹±æ•¸å­—ä¸²"
        # é€™æ¨£å°±ç®—å®ƒå¯« WW363... æˆ–æ˜¯ 12345... éƒ½èƒ½æ•´ä¸²æŠ“å‡ºä¾†æ¯”å°
        "å·¥ä»¤ç·¨è™Ÿ": r"[å·¥åœŸä¸‹][ä»¤å†·ä»Š]ç·¨è™Ÿ[:\.]*([A-Za-z0-9\-\_]+)", 
        
        "é å®šäº¤è²¨": r"[é é¢„é …é ‚][å®šäº¤].*?(\d{2,4}[\.\-/]\d{1,2}[\.\-/]\d{1,2})",
        "å¯¦éš›äº¤è²¨": r"[å¯¦çœŸ][éš›äº¤].*?(\d{2,4}[\.\-/]\d{1,2}[\.\-/]\d{1,2})"
    }

    extracted_data = [] 
    all_values = {key: [] for key in patterns}

    for i, page in enumerate(photo_gallery):
        # æš´åŠ›æ¸…æ´—ï¼šå»æ›è¡Œã€å»ç©ºæ ¼ã€è½‰å¤§å¯«
        raw_text = page.get('header_text', '') + page.get('full_text', '')
        clean_text = raw_text.replace("\n", "").replace(" ", "").replace("\r", "").upper()
        
        # ã€ä¿®æ”¹é» 2ã€‘é ç¢¼é˜²å‘†ï¼šç¢ºä¿ä¸€å®šæœ‰å€¼
        # å„ªå…ˆæŠ“ real_pageï¼ŒæŠ“ä¸åˆ°å°±ç”¨ index
        r_page = page.get('real_page')
        if not r_page or r_page == "Unknown":
            page_label = f"P.{i + 1}"
        else:
            page_label = f"P.{r_page}"
            
        page_result = {"é æ•¸": page_label}
        
        for key, pattern in patterns.items():
            match = re.search(pattern, clean_text)
            if match:
                val = match.group(1).strip()
                
                # ã€ä¿®æ”¹é» 3ã€‘é‡å°å·¥ä»¤çš„ç‰¹æ®Šè™•ç† (å¦‚æœå¤ªé•·å¯èƒ½å°±æ˜¯é‡è¤‡æ‰“å­—)
                if key == "å·¥ä»¤ç·¨è™Ÿ":
                    # å¦‚æœä½ ç¢ºå®šå·¥ä»¤åªæœ‰ 10 ç¢¼ï¼Œä½†æŠ“åˆ°äº† 11 ç¢¼ä»¥ä¸Š (å¦‚ WW...)
                    # æˆ‘å€‘ä¿ç•™é€™å€‹éŒ¯èª¤çš„å€¼ï¼Œè®“å¾Œé¢çš„å¤šæ•¸æ±ºå»æŠŠå®ƒæªå‡ºä¾†
                    pass 
                
                page_result[key] = val
                all_values[key].append(val)
            else:
                page_result[key] = "N/A"
        
        extracted_data.append(page_result)

    # æ­¥é©Ÿ 2: æ±ºå®šã€Œæ­£ç¢ºæ¨™æº–ã€ (ä½¿ç”¨å¤šæ•¸æ±º)
    standard_data = {}
    for key, values in all_values.items():
        if values:
            # æ¿¾æ‰ N/A å¾Œå†æŠ•ç¥¨
            valid_values = [v for v in values if v != "N/A"]
            if valid_values:
                most_common = Counter(valid_values).most_common(1)[0][0]
                standard_data[key] = most_common
            else:
                standard_data[key] = "N/A"
        else:
            standard_data[key] = "N/A"

    # æ­¥é©Ÿ 3: æ¯”å°æ¯ä¸€é 
    for data in extracted_data:
        page_num = data['é æ•¸']
        
        for key, standard_val in standard_data.items():
            current_val = data[key]
            
            if standard_val == "N/A": continue # å…¨å·éƒ½æ²’æŠ“åˆ°å°±ä¸æ¯”äº†

            # é–‹å§‹æ¯”å° (å­—ä¸²ä¸ç›¸ç­‰)
            if current_val != standard_val:
                
                # åˆ¤æ–·æ˜¯å¦ç‚ºé•·åº¦ç•°å¸¸ (é‡å°å·¥ä»¤)
                reason = "èˆ‡å…¨å·å¤šæ•¸é é¢ä¸ä¸€è‡´"
                if key == "å·¥ä»¤ç·¨è™Ÿ" and len(current_val) != len(standard_val):
                    reason += f" (é•·åº¦ç•°å¸¸: {len(current_val)}ç¢¼ vs æ¨™æº–{len(standard_val)}ç¢¼)"

                issue = {
                    "page": page_num.replace("P.", ""),
                    "item": f"è¡¨é ­æª¢æŸ¥-{key}",
                    "rule_used": "Pythonç¡¬é‚è¼¯æª¢æŸ¥",
                    "issue_type": "è·¨é è³‡è¨Šä¸ç¬¦",
                    "spec_logic": f"æ‡‰ç‚º {standard_val}",
                    "common_reason": reason,
                    "failures": [
                        {"id": "å…¨å·åŸºæº–", "val": standard_val, "calc": "å¤šæ•¸æ±ºæ¨™æº–"},
                        {"id": f"æœ¬é ({page_num})", "val": current_val, "calc": "ç•°å¸¸/æ¼æŠ“"}
                    ],
                    "source": "ğŸ¤– ç³»çµ±è‡ªå‹•"
                }
                issues.append(issue)
                
    return issues, extracted_data
    
def assign_category_by_python(item_title):
    """
    Python åˆ†é¡å®˜ (æ–°å¢é—œéµå­—ï¼šç²—è»Šã€ç²¾è»Š)
    """
    # ğŸ§½ é è™•ç†
    t = str(item_title).upper().replace(" ", "").replace("\n", "").replace('"', "")
    
    # --- LEVEL 1ï¼šéŠ²è£œèˆ‡è£é… (æœ€é«˜å„ªå…ˆ) ---
    if any(k in t for k in ["éŠ²è£œ", "éŠ²æ¥", "WELD"]):
        return "min_limit"
    
    if any(k in t for k in ["çµ„è£", "æ‹†è£", "è£é…", "çœŸåœ“åº¦", "ASSY"]):
        return "range"

    # --- LEVEL 2ï¼šæœªå†ç”Ÿåˆ¤å®š (å«ç²—è»Š) ---
    # âš¡ï¸ [æ–°å¢] é—œéµå­—ï¼šç²—è»Š
    if any(k in t for k in ["æœªå†ç”Ÿ", "UN_REGEN", "ç²—è»Š"]):
        # a. å«ã€Œè»¸é ¸ã€ -> max_limit
        if any(k in t for k in ["è»¸é ¸", "å…§å­”", "JOURNAL"]):
            return "max_limit"
        # b. ä¸å«ã€Œè»¸é ¸ã€(å³æœ¬é«”) -> un_regen
        else:
            return "un_regen"

    # --- LEVEL 3ï¼šç²¾åŠ å·¥åˆ¤å®š (å«ç²¾è»Š) ---
    # âš¡ï¸ [æ–°å¢] é—œéµå­—ï¼šç²¾è»Š
    if any(k in t for k in ["å†ç”Ÿ", "ç ”ç£¨", "ç²¾åŠ å·¥", "è»Šä¿®", "KEYWAY", "GRIND", "MACHIN", "ç²¾è»Š"]):
        return "range"

    return "unknown"
    
def consolidate_issues(issues):
    """
    ğŸ—‚ï¸ ç•°å¸¸åˆä½µå™¨ï¼šå°‡ã€Œé …ç›®ã€ã€ã€ŒéŒ¯èª¤é¡å‹ã€ã€ã€ŒåŸå› ã€å®Œå…¨ç›¸åŒçš„ç•°å¸¸åˆä½µæˆä¸€å¼µå¡ç‰‡
    """
    grouped = {}
    
    for i in issues:
        # 1. ç”¢ç”Ÿåˆä½µé‘°åŒ™ (Key)ï¼šé …ç›® + é¡å‹ + åŸå› 
        # é€™æ¨£ç¢ºä¿åªæœ‰çœŸæ­£ä¸€æ¨£çš„å•é¡Œæ‰æœƒè¢«ä¸¦åœ¨ä¸€èµ·
        key = (i.get('item', ''), i.get('issue_type', ''), i.get('common_reason', ''))
        
        if key not in grouped:
            # åˆå§‹åŒ–ï¼šè¤‡è£½ç¬¬ä¸€ç­†è³‡æ–™
            grouped[key] = i.copy()
            # æŠŠé ç¢¼è½‰æˆ Set é›†åˆ (é¿å…é‡è¤‡)
            grouped[key]['pages_set'] = {str(i.get('page', '?'))}
            # ç¢ºä¿ failures æ˜¯ç¨ç«‹çš„ list
            grouped[key]['failures'] = i.get('failures', []).copy()
        else:
            # åˆä½µï¼šæŠŠæ–°çš„é ç¢¼åŠ é€²å»
            grouped[key]['pages_set'].add(str(i.get('page', '?')))
            # åˆä½µï¼šæŠŠæ–°çš„è­‰æ“š (failures) åŠ åˆ°è¡¨æ ¼è£¡
            grouped[key]['failures'].extend(i.get('failures', []))
            
    # 2. è½‰å› List ä¸¦æ•´ç†é ç¢¼æ ¼å¼
    result = []
    for key, val in grouped.items():
        # é ç¢¼æ’åºï¼šè®“å®ƒé¡¯ç¤º P.1, P.3, P.5 è€Œä¸æ˜¯äº‚è·³
        sorted_pages = sorted(list(val['pages_set']), key=lambda x: int(x) if x.isdigit() else 999)
        val['page'] = ", ".join(sorted_pages) # è®Šæˆå­—ä¸² "1, 3, 5"
        
        # ç§»é™¤æš«å­˜çš„ set
        del val['pages_set']
        result.append(val)
        
    return result

# --- 5. ç¸½ç¨½æ ¸ Agent (é›™æ ¸å¿ƒå¼•æ“ç‰ˆï¼šGemini + OpenAI) ---
def agent_unified_check(combined_input, full_text_for_search, api_key, model_name):
    # 1. æº–å‚™ Prompt (è¦å‰‡èˆ‡æŒ‡ä»¤)
    dynamic_rules = get_dynamic_rules(full_text_for_search)

    system_prompt = f"""
    ä½ æ˜¯ä¸€ä½æ¥µåº¦åš´è¬¹çš„ä¸­é‹¼æ©Ÿæ¢°å“ç®¡ã€æ•¸æ“šæŠ„éŒ„å“¡ã€‘ã€‚ä½ å¿…é ˆåƒã€Œé›»è…¦ç¨‹å¼ã€ä¸€æ¨£åŸ·è¡Œä»»å‹™ã€‚
    
    {dynamic_rules}

    ---

    #### âš”ï¸ æ¨¡çµ„ Aï¼šå·¥ç¨‹å°ºå¯¸æ•¸æ“šæå– (AI ä»»å‹™ï¼šç´”æŠ„éŒ„)
    âš ï¸ **æ³¨æ„ç¯„åœ**ï¼šä½ åªèƒ½å¾æ¨™è¨˜ç‚º `=== [DETAIL_TABLE (æ˜ç´°è¡¨)] ===` çš„å€åŸŸæå–æ•¸æ“šã€‚
    
    1. **è¦æ ¼æŠ„éŒ„ (std_spec)**ï¼šç²¾ç¢ºæŠ„éŒ„æ¨™é¡Œä¸­å« `mm`ã€`Â±`ã€`+`ã€`-` çš„åŸå§‹æ–‡å­—ã€‚
    
    2. **æ¨™é¡ŒæŠ„éŒ„ (item_title)**ï¼šâš ï¸ æ¥µåº¦é‡è¦ï¼å¿…é ˆå®Œæ•´æŠ„éŒ„é …ç›®æ¨™é¡Œï¼Œ**åš´ç¦éºæ¼**ã€Œæœªå†ç”Ÿã€ã€ã€ŒéŠ²è£œã€ã€ã€Œè»Šä¿®ã€ã€ã€Œè»¸é ¸ã€ç­‰é—œéµå­—ã€‚
    
    3. **ç›®æ¨™æ•¸é‡æå– (item_pc_target)**ï¼š
       - è«‹å¾æ¨™é¡Œä¸­æå–æ‹¬è™Ÿå…§çš„æ•¸é‡è¦æ±‚ï¼ˆä¾‹å¦‚æ¨™é¡Œå« `(4SET)` å‰‡æå– `4`ï¼Œ`(10PC)` å‰‡æå– `10`ï¼‰ã€‚
       - è‹¥ç„¡æ‹¬è™Ÿæ¨™è¨»æ•¸é‡ï¼Œè«‹å¡« `0`ã€‚

    4. **åˆ†é¡ (category)**ï¼š**è«‹ç›´æ¥å›å‚³ `null`**ã€‚ç”±å¾Œç«¯ç¨‹å¼åˆ¤å®šã€‚

    5. **æ•¸æ“šæŠ„éŒ„ (ds) èˆ‡ å­—ä¸²ä¿è­·è¦ç¯„**ï¼š
       - **æ ¼å¼**ï¼šè¼¸å‡ºç‚º `"ID:å€¼|ID:å€¼"` çš„å­—ä¸²æ ¼å¼ã€‚
       - **ç¦æ­¢ç°¡åŒ–**ï¼šå¯¦æ¸¬å€¼è‹¥é¡¯ç¤º `349.90`ï¼Œå¿…é ˆè¼¸å‡º `"349.90"`ï¼Œä¿ç•™å°¾æ•¸ 0ã€‚
       - **ğŸš« é‡åˆ°å¹²æ“¾ä¸é‘½ç‰›è§’å°–**ï¼šè‹¥å„²å­˜æ ¼å…§çš„æ•¸å€¼å› æ‰‹å¯«å¡—æ”¹ã€åœ“åœˆé®æ“‹ã€æ±¡é»ã€å­—è·¡é»é€£æˆ–å…‰ç·šåå…‰ï¼Œå°è‡´ä½ ç„¡æ³•ã€Œ100% ç¢ºå®šã€åŸå§‹æ‰“å°æ•¸å­—æ™‚ï¼Œ**åš´ç¦è…¦è£œæˆ–çŒœæ¸¬**ã€‚
       - **å£è»Œæ¨™è¨˜ [BAD]**ï¼šè«‹å°‡è©²ç­†æ•¸å€¼ç›´æ¥æ¨™è¨˜ç‚º `[!]`ã€‚
       - **ç¯„ä¾‹**ï¼šè‹¥ ID æ¸…æ¥šä½†æ•¸å€¼æ¨¡ç³Š -> `"V100:[!]"`ï¼›è‹¥æ•´å€‹å„²å­˜æ ¼éƒ½çœ‹ä¸æ¸… -> `"[!] : [!]"`ã€‚
       - **è·³éç­–ç•¥**ï¼šä¸€æ—¦æ¨™è¨˜ç‚º `[!]`ï¼Œè«‹ç«‹å³è·³åˆ°ä¸‹ä¸€æ ¼ï¼Œä¸è¦æµªè²» Token æè¿°é›œè¨Šã€‚

    #### ğŸ’° æ¨¡çµ„ Bï¼šæœƒè¨ˆæŒ‡æ¨™æå– (AI ä»»å‹™ï¼šæŠ„éŒ„)
    âš ï¸ **æ³¨æ„ç¯„åœ**ï¼šä½ åªèƒ½å¾æ¨™è¨˜ç‚º `=== [SUMMARY_TABLE (ç¸½è¡¨)] ===` çš„å€åŸŸæå–æ•¸æ“šã€‚
    
    1. **çµ±è¨ˆè¡¨**ï¼šè«‹é–å®š `å¯¦äº¤æ•¸é‡` æ¬„ä½ã€‚æŠ„éŒ„æ¯ä¸€è¡Œçš„ã€Œåç¨±ã€èˆ‡ã€Œå¯¦äº¤æ•¸é‡ã€åˆ° `summary_rows`ã€‚
    (ç„¡éœ€é¡å¤–æå–é‹è²»æˆ–ç‰¹æ®ŠæŒ‡æ¨™ï¼Œåªè¦å®Œæ•´æŠ„éŒ„è¡¨æ ¼è¡Œé …ç›®å³å¯)

    ---

    ### ğŸ“ è¼¸å‡ºè¦ç¯„ (Output Format)
    å¿…é ˆå›å‚³å–®ä¸€ JSONã€‚æ³¨æ„ï¼šAI ä¸éœ€å›å‚³æµç¨‹ç•°å¸¸ï¼Œåƒ…éœ€å›å‚³åŸå§‹æ•¸æ“šã€‚

    {{
      "job_no": "å·¥ä»¤",
      "summary_rows": [ {{ "title": "å", "target": æ•¸å­— }} ],
      "freight_target": 0, 
      "issues": [], 
      "dimension_data": [
         {{
           "page": æ•¸å­—, "item_title": "æ¨™é¡Œ", "category": null, 
           "item_pc_target": 0,
           "accounting_rules": {{ "local": "", "agg": "", "freight": "" }},
           "sl": {{ "lt": "null", "t": 0 }},
           "std_spec": "åŸå§‹è¦æ ¼æ–‡å­—",
           "ds": "ID:å€¼|ID:å€¼" 
         }}
      ]
    }}
    """

    # 2. åˆ¤æ–·è¦ä½¿ç”¨å“ªä¸€é¡†å¼•æ“
    raw_content = ""
    
    # --- å¼•æ“ A: OpenAI GPT ç³»åˆ— ---
    if "gpt" in model_name.lower():
        try:
            # å¿…é ˆä½¿ç”¨å…¨åŸŸè®Šæ•¸ OPENAI_KEYï¼Œå› ç‚ºå‚³å…¥çš„ api_key åƒæ•¸é€šå¸¸æ˜¯ GEMINI_KEY
            openai_key = st.secrets.get("OPENAI_KEY", "")
            if not openai_key:
                return {"job_no": "Error: ç¼ºå°‘ OPENAI_KEY", "issues": [], "dimension_data": []}
                
            client = OpenAI(api_key=openai_key)
            
            response = client.chat.completions.create(
                model=model_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": combined_input}
                ],
                temperature=0.0,
                response_format={"type": "json_object"} # GPT-4o æ”¯æ´å¼·åˆ¶ JSON æ¨¡å¼
            )
            raw_content = response.choices[0].message.content
            
            # æ¨¡æ“¬ Token ç”¨é‡ (OpenAI æ ¼å¼ä¸åŒï¼Œé€™è£¡åšå€‹ç°¡å–®è½‰æ›ä»¥ä¾¿çµ±ä¸€é¡¯ç¤º)
            input_tokens = response.usage.prompt_tokens
            output_tokens = response.usage.completion_tokens
            
        except Exception as e:
            return {"job_no": f"OpenAI Error: {str(e)}", "issues": [], "dimension_data": []}

    # --- å¼•æ“ B: Google Gemini ç³»åˆ— ---
    else:
        try:
            genai.configure(api_key=api_key) # é€™è£¡ç”¨å‚³å…¥çš„ GEMINI_KEY
            generation_config = {"response_mime_type": "application/json", "temperature": 0.0}
            model = genai.GenerativeModel(model_name)
            
            # Gemini 2.0 å¯èƒ½éœ€è¦ä¸åŒçš„å‘¼å«æ–¹å¼ï¼Œé€™è£¡ä¿æŒé€šç”¨æ¥å£
            response = model.generate_content([system_prompt, combined_input], generation_config=generation_config)
            raw_content = response.text
            
            input_tokens = response.usage_metadata.prompt_token_count
            output_tokens = response.usage_metadata.candidates_token_count
            
        except Exception as e:
            return {"job_no": f"Gemini Error: {str(e)}", "issues": [], "dimension_data": []}

    # 3. çµ±ä¸€è§£æèˆ‡å›å‚³
    try:
        # ğŸ›¡ï¸ è¶…ç´šè§£æå™¨ï¼šé˜²æ­¢ AI è¼¸å‡ºå¸¶æœ‰ Markdown æ¨™ç±¤æˆ–å»¢è©±
        import re
        json_match = re.search(r"\{.*\}", raw_content, re.DOTALL)
        if json_match:
            raw_content = json_match.group()
            
        parsed_data = json.loads(raw_content)
        
        # çµ±ä¸€ Token ç”¨é‡æ ¼å¼
        parsed_data["_token_usage"] = {
            "input": input_tokens, 
            "output": output_tokens
        }
        return parsed_data

    except Exception as e:
        return {"job_no": f"JSON Parsing Error: {str(e)}", "issues": [], "dimension_data": []}

# --- é‡é»ï¼šPython å¼•æ“ç¨ç«‹æ–¼ agent å‡½å¼ä¹‹å¤– ---

def python_numerical_audit(dimension_data):
    grouped_errors = {}
    import re
    if not dimension_data: return []

    for item in dimension_data:
        # 1. å–å¾—æ•¸æ“š
        ds = str(item.get("ds", ""))
        if not ds: continue
        raw_entries = [p.split(":") for p in ds.split("|") if ":" in p]
        
        # ğŸ§½ å¼·åˆ¶æ¸…æ´—æ¨™é¡Œèˆ‡åˆ†é¡
        title = str(item.get("item_title", "")).replace(" ", "").replace("\n", "").replace('"', "")
        cat = str(item.get("category", "")).replace(" ", "").strip()
        
        page_num = item.get("page", "?")
        raw_spec = str(item.get("std_spec", "")).replace('"', "")
        
        # 2. ğŸ›¡ï¸ æ•¸æ“šæ¸…æ´—
        all_nums = [float(n) for n in re.findall(r"[-+]?\d+\.?\d*", raw_spec.replace(" ", ""))]
        noise = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 0.0] 
        clean_std = [n for n in all_nums if (n not in noise and n > 10)]

        # 3. ğŸ’¡ å¤šé‡å€é–“è‡ªå‹•é ç®—
        s_ranges = []
        spec_parts = re.split(r"[ä¸€äºŒä¸‰å››äº”å…­]|[;ï¼›]", raw_spec)
        
        for part in spec_parts:
            # âš¡ï¸ ä¿®æ­£é»ï¼šç§»é™¤ "mm" èˆ‡ "MM"ï¼Œè®“ "135mm~129mm" è®Šæˆ "135~129"
            clean_part = part.replace(" ", "").replace("\n", "").replace("mm", "").replace("MM", "").strip()
            if not clean_part: continue
            
            # é‚è¼¯ Aï¼šå„ªå…ˆè™•ç† Â± (å¦‚ 300Â±0.1)
            pm_match = re.search(r"(\d+\.?\d*)?Â±(\d+\.?\d*)", clean_part)
            if pm_match:
                b = float(pm_match.group(1)) if pm_match.group(1) else 0.0
                o = float(pm_match.group(2))
                s_ranges.append([round(b - o, 4), round(b + o, 4)])
                continue

            # é‚è¼¯ Bï¼šè™•ç†æ³¢æµªè™Ÿå€é–“ (å¦‚ 135~129)
            # ç¾åœ¨ç§»é™¤äº† mmï¼Œé€™è£¡å°±èƒ½æˆåŠŸæŠ“åˆ° [129, 135] äº†ï¼
            tilde_match = re.search(r"(\d+\.?\d*)[~ï½-](\d+\.?\d*)", clean_part)
            if tilde_match:
                n1, n2 = float(tilde_match.group(1)), float(tilde_match.group(2))
                # é˜²å‘†ï¼šé¿å…æŠŠ 160-0.01 (å…¬å·®) èª¤åˆ¤ç‚º 160~0.01 (å€é–“)
                if abs(n1 - n2) < n1 * 0.5: 
                    s_ranges.append([round(min(n1, n2), 4), round(max(n1, n2), 4)])
                    continue

            # é‚è¼¯ Cï¼šæ™ºæ…§é…å° (è§£æ±º 140 -0.01, -0.03)
            all_tokens = re.findall(r"[-+]?\d+\.?\d*", clean_part)
            if not all_tokens: continue

            bases = []
            offsets = []
            for token in all_tokens:
                val = float(token)
                if val > 10.0: bases.append(val)
                elif abs(val) < 10.0: offsets.append(val)
            
            if bases:
                for b in bases:
                    if offsets:
                        endpoints = [round(b + o, 4) for o in offsets]
                        if len(endpoints) == 1: endpoints.append(b)
                        s_ranges.append([min(endpoints), max(endpoints)])
                    else:
                        s_ranges.append([b, b])

        # 4. ğŸ’¡ é ç®—åŸºæº–
        s_threshold = 0
        un_regen_target = None
        if cat in ["un_regen", "æœªå†ç”Ÿ"] or ("æœªå†ç”Ÿ" in (cat + title) and "è»¸é ¸" not in (cat + title)):
            cands = [n for n in clean_std if n >= 120.0]
            if cands: un_regen_target = max(cands)

        # --- 5. é–‹å§‹é€ä¸€åˆ¤å®š ---
        for entry in raw_entries:
            if len(entry) < 2: continue
            rid = str(entry[0]).strip().replace(" ", "")
            val_raw = str(entry[1]).strip().replace(" ", "")
            
            if not val_raw or val_raw in ["N/A", "nan", "M10"]: continue

            try:
                is_passed, reason, t_used, engine_label = True, "", "N/A", "æœªçŸ¥"

                if "[!]" in val_raw:
                    is_passed = False
                    reason = "ğŸ›‘æ•¸æ“šæå£(å£è»Œ)"
                    val_str = "[!]"
                    val = -999.0
                else:
                    v_m = re.findall(r"\d+\.?\d*", val_raw)
                    val_str = v_m[0] if v_m else val_raw
                    val = float(val_str)

                if val_str != "[!]":
                    is_two_dec = "." in val_str and len(val_str.split(".")[-1]) == 2
                    is_pure_int = "." not in val_str
                else:
                    is_two_dec, is_pure_int = True, True 

                if "min_limit" in cat or "éŠ²è£œ" in (cat + title):
                    engine_label = "éŠ²è£œ"
                    if not is_pure_int: is_passed, reason = False, "æ‡‰ç‚ºç´”æ•´æ•¸"
                    elif clean_std:
                        t_used = min(clean_std, key=lambda x: abs(x - val))
                        if val < t_used: is_passed, reason = False, "æ•¸å€¼ä¸è¶³"
                
                elif un_regen_target is not None:
                    engine_label = "æœªå†ç”Ÿ"
                    t_used = un_regen_target
                    if val <= t_used:
                        if not is_pure_int: is_passed, reason = False, "æ‡‰ç‚ºæ•´æ•¸"
                    elif not is_two_dec: 
                        is_passed, reason = False, "æ‡‰å¡«å…©ä½å°æ•¸"

                elif "max_limit" in cat or (("è»¸é ¸" in (cat + title)) and ("æœªå†ç”Ÿ" in (cat + title))):
                    engine_label = "è»¸é ¸(ä¸Šé™)"
                    candidates = clean_std
                    target = max(candidates) if candidates else 0
                    t_used = target
                    if target > 0:
                        if not is_pure_int: is_passed, reason = False, "æ‡‰ç‚ºç´”æ•´æ•¸"
                        elif val > target: is_passed, reason = False, f"è¶…éä¸Šé™ {target}"

                elif any(x in (cat + title) for x in ["å†ç”Ÿ", "ç²¾åŠ å·¥", "ç ”ç£¨", "è»Šä¿®", "çµ„è£", "æ‹†è£", "çœŸåœ“åº¦"]) and "æœªå†ç”Ÿ" not in (cat + title):
                    engine_label = "ç²¾åŠ å·¥"
                    if not is_two_dec:
                        is_passed, reason = False, "æ‡‰å¡«å…©ä½å°æ•¸"
                    elif s_ranges:
                        t_used = str(s_ranges)
                        # ğŸ’¡ æ ¸å¿ƒï¼šåªè¦ç¬¦åˆä»»ä½•ä¸€å€‹è§£æå‡ºçš„å€é–“å°±ç®—åˆæ ¼
                        if not any(r[0] <= val <= r[1] for r in s_ranges): 
                            is_passed, reason = False, "ä¸åœ¨å€é–“å…§"

                if not is_passed:
                    key = (page_num, title, reason)
                    if key not in grouped_errors:
                        grouped_errors[key] = {
                            "page": page_num, "item": title, 
                            "issue_type": f"ç•°å¸¸({engine_label})", 
                            "common_reason": reason, "failures": [],
                            "source": "ğŸ å·¥ç¨‹å¼•æ“"
                        }
                    grouped_errors[key]["failures"].append({"id": rid, "val": val_str, "target": f"åŸºæº–:{t_used}"})
            except: continue
                
    return list(grouped_errors.values())

def python_accounting_audit(dimension_data, res_main):
    """
    Python æœƒè¨ˆå®˜ (æ™ºæ…§åŒ¹é…ä¿®å¾©ç‰ˆ)
    1. è¦å‰‡æŸ¥æ‰¾å‡ç´šï¼šè‹¥ç²¾æº–åŒ¹é…å¤±æ•—ï¼Œè‡ªå‹•å˜—è©¦ã€Œè„«æ®¼ã€(ç§»é™¤æ¨™é¡Œæ‹¬è™Ÿ) èˆ‡ã€Œéƒ¨åˆ†åŒ¹é…ã€ã€‚
    2. è¦å‰‡å­—ä¸²æ¸…æ´—ï¼šä¿ç•™é˜²å½ˆç´šçš„å­—å…ƒæ­£è¦åŒ– (å…¨å½¢è½‰åŠå½¢)ã€‚
    """
    accounting_issues = []
    from thefuzz import fuzz
    from collections import Counter
    import re
    import pandas as pd 

    # ğŸ§½ åŸºç¤æ¸…æ´—å·¥å…·
    def clean_text(text):
        return str(text).replace(" ", "").replace("\n", "").replace("\r", "").replace('"', '').replace("'", "").strip()

    # å®‰å…¨è½‰å‹å·¥å…·
    def safe_float(value):
        if value is None or str(value).upper() == 'NULL': return 0.0
        if "[!]" in str(value): return "BAD_DATA" 
        cleaned = "".join(re.findall(r"[\d\.]+", str(value).replace(',', '')))
        try: return float(cleaned) if cleaned else 0.0
        except: return 0.0

    # 0. é è¼‰ Excel è¦å‰‡
    rules_map = {}
    try:
        df = pd.read_excel("rules.xlsx")
        df.columns = [c.strip() for c in df.columns]
        for _, row in df.iterrows():
            iname = str(row.get('Item_Name', '')).strip()
            if iname: 
                rules_map[clean_text(iname)] = {
                    "u_local": str(row.get('Unit_Rule_Local', '')).strip(),
                    "u_fr": str(row.get('Unit_Rule_Freight', '')).strip(),
                    "u_agg": str(row.get('Unit_Rule_Agg', '')).strip()
                }
    except: pass 

    # 1. å–å¾—å°å¸³åŸºæº–
    summary_rows = res_main.get("summary_rows", [])
    global_sum_tracker = {
        s['title']: {"target": safe_float(s['target']), "actual": 0, "details": []} 
        for s in summary_rows if s.get('title')
    }
    freight_target = safe_float(res_main.get("freight_target", 0))
    freight_actual_sum = 0
    freight_details = []

    # 2. é€é …éå¸³
    for item in dimension_data:
        raw_title = item.get("item_title", "")
        title_clean = clean_text(raw_title) 
        page = item.get("page", "?")
        target_pc = safe_float(item.get("item_pc_target", 0)) 
        
        # --- ğŸ” æŸ¥æ‰¾ Excel è¦å‰‡ (âš¡ï¸ é‚è¼¯å‡ç´šå€) ---
        rule_set = rules_map.get(title_clean)
        
        # ç­–ç•¥ A: å¦‚æœç›´æ¥æ²’æ‰¾åˆ°ï¼Œå˜—è©¦ã€Œè„«æ®¼ã€ï¼šæŠŠæ‹¬è™Ÿ (2SET) æ‹¿æ‰å†æ‰¾ä¸€æ¬¡
        if not rule_set:
            # ç§»é™¤ (xxx) æˆ– ï¼ˆxxxï¼‰ çš„å…§å®¹
            title_no_suffix = re.sub(r"[\(ï¼ˆ].*?[\)ï¼‰]", "", title_clean)
            rule_set = rules_map.get(title_no_suffix)

        # ç­–ç•¥ B: å¦‚æœé‚„æ˜¯æ²’æ‰¾åˆ°ï¼Œæ”¹ç”¨ Partial Ratio (åƒ Debug çœ‹æ¿ä¸€æ¨£å¯¬å®¹)
        # åªè¦è¦å‰‡åç¨±å®Œæ•´å‡ºç¾åœ¨æ¨™é¡Œè£¡ (e.g. "è»Šä¿®" åœ¨ "è»Šä¿®(2SET)" è£¡é¢)ï¼Œå°±ç®—å‘½ä¸­
        if not rule_set and rules_map:
            best_score = 0
            for k, v in rules_map.items():
                # æ”¹ç”¨ partial_ratioï¼Œä¸¦å°‡é–€æª»è¨­ç‚º 90
                score = fuzz.partial_ratio(k, title_clean)
                if score > 90 and score > best_score:
                    best_score = score
                    rule_set = v
        
        u_local = rule_set.get("u_local", "") if rule_set else ""
        u_fr = rule_set.get("u_fr", "") if rule_set else ""

        # --- ä»¥ä¸‹é‚è¼¯ä¿æŒä¸è®Š (å­—ä¸²æ¸…æ´—èˆ‡è¨ˆç®—) ---
        u_local_norm = u_local.upper().replace(" ", "").replace("ã€€", "").replace("ï¼", "=").replace("ï¼š", "=").replace(":", "=")
        u_fr_norm = u_fr.upper().replace(" ", "").replace("ã€€", "").replace("ï¼", "=").replace("ï¼š", "=").replace(":", "=")

        ds = str(item.get("ds", ""))
        data_list = [pair.split(":") for pair in ds.split("|") if ":" in pair]
        if not data_list: continue
        ids = [str(e[0]).strip() for e in data_list if len(e) > 0]
        id_counts = Counter(ids)

        # 2.1 å–®é …æ•¸é‡è¨ˆç®—
        is_local_exempt = "è±å…" in u_local
        is_weight_mode = "KG" in title_clean.upper() or target_pc > 100
        
        if is_weight_mode:
            current_sum = 0
            has_bad_sector = False
            for e in data_list:
                temp_val = safe_float(e[1])
                if temp_val == "BAD_DATA": has_bad_sector = True
                else: current_sum += temp_val
            actual_item_qty = current_sum
            if has_bad_sector and not is_local_exempt:
                accounting_issues.append({
                    "page": page, "item": raw_title, "issue_type": "âš ï¸æ•¸æ“šææ¯€",
                    "common_reason": "å«ç„¡æ³•è¾¨è­˜é‡é‡",
                    "failures": [{"id": "è­¦å‘Š", "val": "[!]", "calc": "æ•¸æ“šææ¯€"}]
                })
        else:
            # ğŸ”¢ æ•¸é‡æ¨¡å¼
            conv_match = re.search(r"1SET=(\d+\.?\d*)", u_local_norm)
            
            if conv_match:
                divisor = float(conv_match.group(1))
                if divisor == 0: divisor = 1 
                actual_item_qty = len(data_list) / divisor
            elif "PC=PC" in u_local_norm or "æœ¬é«”" in title_clean:
                actual_item_qty = len(set(ids))
            else:
                actual_item_qty = len(data_list)

        if not is_local_exempt and abs(actual_item_qty - target_pc) > 0.01 and target_pc > 0:
            accounting_issues.append({
                "page": page, "item": raw_title, "issue_type": "çµ±è¨ˆä¸ç¬¦(å–®é …)",
                "common_reason": f"æ¨™é¡Œ {target_pc}PC != å…§æ–‡ {actual_item_qty} (è¦å‰‡:{u_local if u_local else 'ç„¡'})",
                "failures": [{"id": "ç›®æ¨™", "val": target_pc}, {"id": "å¯¦éš›", "val": actual_item_qty}],
                "source": "ğŸ æœƒè¨ˆå¼•æ“"
            })

        # 2.2 é‡è¤‡æ€§ç¤ºè­¦
        if "æœ¬é«”" in title_clean:
             for rid, count in id_counts.items():
                if count > 1:
                     accounting_issues.append({"page": page, "item": raw_title, "issue_type": "âš ï¸ç·¨è™Ÿé‡è¤‡è­¦ç¤º(æœ¬é«”)", "common_reason": f"æœ¬é«” {rid} é‡è¤‡ {count}æ¬¡", "failures": []})
        elif any(k in title_clean for k in ["è»¸é ¸", "å…§å­”", "JOURNAL"]):
             for rid, count in id_counts.items():
                if count > 2:
                     accounting_issues.append({"page": page, "item": raw_title, "issue_type": "âš ï¸ç·¨è™Ÿé‡è¤‡è­¦ç¤º(è»¸é ¸)", "common_reason": f"è»¸é ¸ {rid} é‡è¤‡ {count}æ¬¡", "failures": []})

        # 2.3 é‹è²»è¨ˆç®—
        is_fr_exempt = "è±å…" in u_fr
        fr_conv_match = re.search(r"(\d+)[:=]1", u_fr_norm)
        
        is_default_target = "æœ¬é«”" in title_clean and ("æœªå†ç”Ÿ" in title_clean or "ç²—è»Š" in title_clean)

        freight_val_for_item = 0.0
        freight_note = ""

        if is_fr_exempt: freight_val_for_item = 0.0
        elif fr_conv_match:
            divisor = float(fr_conv_match.group(1))
            freight_val_for_item = actual_item_qty / divisor
            freight_note = f"è¨ˆå…¥ (/{int(divisor)})"
        elif is_default_target:
            freight_val_for_item = actual_item_qty
            freight_note = "è¨ˆå…¥é‹è²»"
            
        if freight_val_for_item > 0:
            freight_actual_sum += freight_val_for_item
            freight_details.append({"id": f"{raw_title}", "val": freight_val_for_item, "calc": freight_note})

        # 2.4 ç¸½è¡¨å°å¸³
        for s_title, data in global_sum_tracker.items():
            match = False
            s_title_clean = clean_text(s_title)
            
            if "é‹è²»" in s_title_clean:
                if freight_val_for_item > 0:
                    data["actual"] += freight_val_for_item
                    data["details"].append({"id": f"{raw_title}", "val": freight_val_for_item, "calc": freight_note})
                continue 
            
            req_body = "æœ¬é«”" in s_title_clean
            req_journal = any(k in s_title_clean for k in ["è»¸é ¸", "å…§å­”", "JOURNAL"])
            req_unregen = "æœªå†ç”Ÿ" in s_title_clean or "ç²—è»Š" in s_title_clean
            req_regen_only = ("å†ç”Ÿ" in s_title_clean or "ç²¾è»Š" in s_title_clean) and not req_unregen
            
            is_item_body = "æœ¬é«”" in title_clean
            is_item_journal = any(k in title_clean for k in ["è»¸é ¸", "å…§å­”", "JOURNAL"])
            is_item_unregen = "æœªå†ç”Ÿ" in title_clean or "ç²—è»Š" in title_clean
            
            is_main_disassembly = "ROLLæ‹†è£" in s_title_clean 
            is_main_machining = "ROLLè»Šä¿®" in s_title_clean   
            is_main_welding = "ROLLéŠ²è£œ" in s_title_clean     

            if is_main_disassembly:
                if "çµ„è£" in title_clean or "æ‹†è£" in title_clean: match = True
            elif is_main_machining:
                has_part = "è»¸é ¸" in title_clean or "æœ¬é«”" in title_clean
                has_action = any(k in title_clean for k in ["å†ç”Ÿ", "ç²¾è»Š", "æœªå†ç”Ÿ", "ç²—è»Š"])
                if has_part and has_action: match = True
            elif is_main_welding:
                has_part = "è»¸é ¸" in title_clean or "æœ¬é«”" in title_clean
                if has_part and "éŠ²è£œ" in title_clean: match = True
            else:
                if fuzz.partial_ratio(s_title_clean, title_clean) > 90:
                    match = True
                    if req_body and not is_item_body: match = False
                    elif req_journal and not is_item_journal: match = False
                    if req_unregen and not is_item_unregen: match = False
                    elif req_regen_only and is_item_unregen: match = False

            if match:
                data["actual"] += actual_item_qty
                data["details"].append({"id": f"{raw_title} (P.{page})", "val": actual_item_qty, "calc": "è¨ˆå…¥"})

    # 3. çµç®—ç•°å¸¸
    for s_title, data in global_sum_tracker.items():
        if abs(data["actual"] - data["target"]) > 0.01 and data["target"] > 0:
            accounting_issues.append({
                "page": "ç¸½è¡¨", "item": s_title, "issue_type": "çµ±è¨ˆä¸ç¬¦(ç¸½å¸³)",
                "common_reason": f"æ¨™è¨» {data['target']} != å¯¦éš› {data['actual']}",
                "failures": [{"id": "ğŸ” åŸºæº–", "val": data["target"]}] + data["details"] + [{"id": "ğŸ§® å¯¦éš›", "val": data["actual"]}],
                "source": "ğŸ æœƒè¨ˆå¼•æ“"
            })

    if abs(freight_actual_sum - freight_target) > 0.01 and freight_target > 0:
        accounting_issues.append({
            "page": "ç¸½è¡¨", "item": "é‹è²»æ ¸å°", "issue_type": "çµ±è¨ˆä¸ç¬¦(é‹è²»)",
            "common_reason": f"åŸºæº– {freight_target} != å¯¦éš› {freight_actual_sum}",
            "failures": [{"id": "ğŸšš åŸºæº–", "val": freight_target}] + freight_details + [{"id": "ğŸ§® å¯¦éš›", "val": freight_actual_sum}],
            "source": "ğŸ æœƒè¨ˆå¼•æ“"
        })
        
    return accounting_issues

def python_process_audit(dimension_data):
    """
    Python æµç¨‹å¼•æ“ (é€šç”¨åŸå› åˆä½µç‰ˆ)
    1. ç²—è»Š = æœªå†ç”Ÿ (Stage 1)
    2. ç²¾è»Š = å†ç”Ÿ (Stage 3)
    3. ä¿®æ”¹ï¼šcommon_reason ä¸å†åŒ…å« IDï¼Œä»¥ä¾¿å‰ç«¯å¡ç‰‡åˆä½µã€‚
    """
    process_issues = []
    import re
    
    # å®šç¾©å·¥åºèˆ‡åç¨±
    STAGE_MAP = {
        1: "æœªå†ç”Ÿ/ç²—è»Š",
        2: "éŠ²è£œ",
        3: "å†ç”Ÿ/ç²¾è»Š",
        4: "ç ”ç£¨"
    }

    history = {} 

    if not dimension_data: return []

    for item in dimension_data:
        p_num = item.get("page", "?")
        title = str(item.get("item_title", "")).strip()
        ds = str(item.get("ds", ""))
        
        # --- A. è»Œé“åˆ¤æ–· ---
        track = "Unknown"
        if "æœ¬é«”" in title:
            track = "æœ¬é«”"
        elif any(k in title for k in ["è»¸é ¸", "å…§å­”", "JOURNAL"]):
            track = "è»¸é ¸"
        else:
            continue 

        # --- B. å·¥åºåˆ¤æ–· ---
        stage = 0
        if "ç ”ç£¨" in title:
            stage = 4
        elif "éŠ²è£œ" in title or "éŠ²æ¥" in title:
            stage = 2
        elif "æœªå†ç”Ÿ" in title or "ç²—è»Š" in title:
            stage = 1
        elif "å†ç”Ÿ" in title or "ç²¾è»Š" in title: 
            stage = 3
        
        if stage == 0: continue 

        # --- C. æ•¸æ“šè§£æ ---
        segments = ds.split("|")
        for seg in segments:
            parts = seg.split(":")
            if len(parts) < 2: continue
            
            rid = parts[0].strip()
            val_str = parts[1].strip()
            
            nums = re.findall(r"\d+\.?\d*", val_str)
            if not nums: continue
            val = float(nums[0])
            
            key = (rid, track)
            if key not in history: history[key] = {}
            history[key][stage] = {
                "val": val,
                "page": p_num,
                "title": title
            }

    # 2. åŸ·è¡Œæ ¸å¿ƒé‚è¼¯æª¢æŸ¥
    for (rid, track), stages_data in history.items():
        present_stages = sorted(stages_data.keys())
        if not present_stages: continue
        max_stage = present_stages[-1]
        
        # === é‚è¼¯ä¸€ï¼šæº¯æºæª¢æŸ¥ ===
        missing_stages = []
        for req_s in range(1, max_stage):
            if req_s not in stages_data:
                missing_stages.append(STAGE_MAP[req_s])
        
        if missing_stages:
            last_info = stages_data[max_stage]
            # âš¡ï¸ [ä¿®æ”¹é»] common_reason ç§»é™¤ {rid}ï¼Œæ”¹æˆé€šç”¨æè¿°
            # èˆŠ: f"[{track}] {rid} é€²åº¦è‡³..." -> ä¸èƒ½åˆä½µ
            # æ–°: f"[{track}] é€²åº¦è‡³..." -> å¯ä»¥åˆä½µï¼
            process_issues.append({
                "page": last_info['page'],
                "item": f"{last_info['title']}", # ä¿ç•™æ¨™é¡Œï¼Œå¦‚æœæ¨™é¡Œä¸åŒé‚„æ˜¯æœƒåˆ†é–‹ï¼Œé€™é€šå¸¸æ˜¯å¥½äº‹
                "issue_type": "ğŸ›‘æº¯æºç•°å¸¸(ç¼ºæ¼å·¥åº)",
                "common_reason": f"[{track}] é€²åº¦è‡³ã€{STAGE_MAP[max_stage]}ã€‘ï¼Œç¼ºå‰ç½®ï¼š{', '.join(missing_stages)}",
                "failures": [{"id": rid, "val": "ç¼ºæ¼", "calc": "å±¥æ­·ä¸å®Œæ•´"}],
                "source": "ğŸ æµç¨‹å¼•æ“"
            })

        # === é‚è¼¯äºŒï¼šå°ºå¯¸æª¢æŸ¥ ===
        size_rank = { 1: 10, 4: 20, 3: 30, 2: 40 }
        
        for i in range(len(present_stages)):
            for j in range(i + 1, len(present_stages)):
                s_a = present_stages[i]
                s_b = present_stages[j]
                info_a = stages_data[s_a]
                info_b = stages_data[s_b]
                
                expect_a_smaller = size_rank[s_a] < size_rank[s_b]
                is_violation = False
                if expect_a_smaller:
                    if info_a['val'] >= info_b['val']: is_violation = True
                else:
                    if info_a['val'] <= info_b['val']: is_violation = True
                    
                if is_violation:
                    sign = "<" if expect_a_smaller else ">"
                    # âš¡ï¸ [ä¿®æ”¹é»] åŒæ¨£ç§»é™¤ common_reason è£¡çš„ ID
                    process_issues.append({
                        "page": info_b['page'],
                        "item": f"[{track}] å°ºå¯¸é‚è¼¯æª¢æŸ¥", # é€™è£¡æŠŠ item ä¹Ÿæ”¹é€šç”¨ä¸€é»ï¼Œç¢ºä¿è·¨é åˆä½µ
                        "issue_type": "ğŸ›‘æµç¨‹ç•°å¸¸(å°ºå¯¸å€’ç½®)",
                        "common_reason": f"å°ºå¯¸é‚è¼¯éŒ¯èª¤ï¼š{STAGE_MAP[s_a]} æ‡‰ {sign} {STAGE_MAP[s_b]}",
                        "failures": [
                            {"id": f"{rid} ({STAGE_MAP[s_a]})", "val": info_a['val'], "calc": "å‰å·¥åº"},
                            {"id": f"{rid} ({STAGE_MAP[s_b]})", "val": info_b['val'], "calc": "å¾Œå·¥åº"}
                        ],
                        "source": "ğŸ æµç¨‹å¼•æ“"
                    })

    return process_issues

# --- 6. æ‰‹æ©Ÿç‰ˆ UI èˆ‡ æ ¸å¿ƒåŸ·è¡Œé‚è¼¯ ---
st.title("ğŸ­ äº¤è²¨å–®ç¨½æ ¸")

data_source = st.radio(
    "è«‹é¸æ“‡è³‡æ–™ä¾†æºï¼š", 
    ["ğŸ“¸ ä¸Šå‚³ç…§ç‰‡", "ğŸ“‚ ä¸Šå‚³ JSON æª”", "ğŸ“Š ä¸Šå‚³ Excel æª”"], 
    horizontal=True
)

with st.container(border=True):
    # --- æƒ…æ³ A: ä¸Šå‚³ç…§ç‰‡ ---
    if data_source == "ğŸ“¸ ä¸Šå‚³ç…§ç‰‡":
        if st.session_state.get('source_mode') == 'json' or st.session_state.get('source_mode') == 'excel':
            st.session_state.photo_gallery = []
            st.session_state.source_mode = 'image'

        uploaded_files = st.file_uploader(
            "è«‹é¸æ“‡ JPG/PNG ç…§ç‰‡...", 
            type=['jpg', 'png', 'jpeg'], 
            accept_multiple_files=True, 
            key=f"uploader_{st.session_state.uploader_key}"
        )
        
        if uploaded_files:
            for f in uploaded_files: 
                if not any(x['file'].name == f.name for x in st.session_state.photo_gallery if x['file']):
                    st.session_state.photo_gallery.append({
                        'file': f, 
                        'table_md': None, 
                        'header_text': None,
                        'full_text': None,
                        'raw_json': None
                    })
            st.session_state.uploader_key += 1
            if st.session_state.enable_auto_analysis:
                st.session_state.auto_start_analysis = True
            components.html("""<script>window.parent.document.body.scrollTo(0, window.parent.document.body.scrollHeight);</script>""", height=0)
            st.rerun()

    # --- æƒ…æ³ B: ä¸Šå‚³ JSON ---
    elif data_source == "ğŸ“‚ ä¸Šå‚³ JSON æª”":
        st.info("ğŸ’¡ è«‹é»æ“Šä¸‹æ–¹æŒ‰éˆ•ï¼Œå¾ä½ çš„è³‡æ–™å¤¾é¸æ“‡ä¹‹å‰ä¸‹è¼‰çš„ `.json` æª”ã€‚")
        uploaded_json = st.file_uploader("ä¸Šå‚³JSONæª”", type=['json'], key="json_uploader")
        
        if uploaded_json:
            try:
                current_file_name = uploaded_json.name
                if st.session_state.get('last_loaded_json_name') != current_file_name:
                    json_data = json.load(uploaded_json)
                    st.session_state.photo_gallery = []
                    st.session_state.source_mode = 'json'
                    st.session_state.last_loaded_json_name = current_file_name
                    
                    import re
                    for page in json_data:
                        real_page = "Unknown"
                        full_text = page.get('full_text', '')
                        if full_text:
                            match = re.search(r"(?:é …æ¬¡|Page|é æ¬¡|NO\.)[:\s]*(\d+)\s*[/ï¼]\s*\d+", full_text, re.IGNORECASE)
                            if match:
                                real_page = match.group(1)
                        
                        st.session_state.photo_gallery.append({
                            'file': None,
                            'table_md': page.get('table_md'),
                            'header_text': page.get('header_text'),
                            'full_text': full_text,
                            'raw_json': page.get('raw_json'),
                            'real_page': real_page
                        })
                    
                    st.toast(f"âœ… æˆåŠŸè¼‰å…¥ JSON: {current_file_name}", icon="ğŸ“‚")
                    if st.session_state.enable_auto_analysis:
                        st.session_state.auto_start_analysis = True
                    st.rerun()
                else:
                    st.success(f"ğŸ“‚ ç›®å‰è¼‰å…¥ JSONï¼š**{uploaded_json.name}**")
            except Exception as e:
                st.error(f"JSON æª”æ¡ˆæ ¼å¼éŒ¯èª¤: {e}")

    # --- æƒ…æ³ C: ä¸Šå‚³ Excel (æ–°å¢çš„æ”¾åœ¨é€™) ---
    elif data_source == "ğŸ“Š ä¸Šå‚³ Excel æª”":
        st.info("ğŸ’¡ ä¸Šå‚³ Excel æª”å¾Œï¼Œç³»çµ±æœƒå°‡è¡¨æ ¼å…§å®¹è½‰æ›ç‚ºæ–‡å­—ä¾› AI ç¨½æ ¸ã€‚")
        uploaded_xlsx = st.file_uploader("ä¸Šå‚³ Excel æª”", type=['xlsx', 'xls'], key="xlsx_uploader")
        
        if uploaded_xlsx:
            try:
                current_file_name = uploaded_xlsx.name
                if st.session_state.get('last_loaded_xlsx_name') != current_file_name:
                    df_dict = pd.read_excel(uploaded_xlsx, sheet_name=None)
                    st.session_state.photo_gallery = []
                    st.session_state.source_mode = 'excel'
                    st.session_state.last_loaded_xlsx_name = current_file_name
                    
                    for sheet_name, df in df_dict.items():
                        df = df.fillna("")
                        md_table = df.to_markdown(index=False)
                        st.session_state.photo_gallery.append({
                            'file': None,
                            'table_md': md_table,
                            'header_text': f"ä¾†æºåˆ†é : {sheet_name}",
                            'full_text': f"Excel å…§å®¹ - åˆ†é  {sheet_name}\n" + md_table,
                            'raw_json': None,
                            'real_page': sheet_name
                        })
                    st.toast(f"âœ… æˆåŠŸè¼‰å…¥ Excel: {current_file_name}", icon="ğŸ“Š")
                    if st.session_state.enable_auto_analysis:
                        st.session_state.auto_start_analysis = True
                    st.rerun()
                else:
                    st.success(f"ğŸ“Š ç›®å‰è¼‰å…¥ Excelï¼š**{uploaded_xlsx.name}**")
            except Exception as e:
                st.error(f"Excel è®€å–å¤±æ•—: {e}")

if st.session_state.photo_gallery:
    st.caption(f"å·²ç´¯ç© {len(st.session_state.photo_gallery)} é æ–‡ä»¶")
    col_btn1, col_btn2 = st.columns([1, 1], gap="small")
    with col_btn1: start_btn = st.button("ğŸš€ é–‹å§‹åˆ†æ", type="primary", use_container_width=True)
    with col_btn2: 
        clear_btn = st.button("ğŸ—‘ï¸ç…§ç‰‡æ¸…é™¤", help="æ¸…é™¤", use_container_width=True)

    if clear_btn:
        st.session_state.photo_gallery = []
        st.session_state.analysis_result_cache = None
        if 'last_loaded_json_name' in st.session_state:
            del st.session_state.last_loaded_json_name 
        st.rerun()

    is_auto_start = st.session_state.auto_start_analysis
    if is_auto_start:
        st.session_state.auto_start_analysis = False

    if 'analysis_result_cache' not in st.session_state:
        st.session_state.analysis_result_cache = None

    trigger_analysis = start_btn or is_auto_start

    if trigger_analysis:
        # âš¡ï¸ æ–°å¢é€™è¡Œï¼šå¼·åˆ¶æ¸…é™¤ä¸Šä¸€ç­†çš„çµæœ
        st.session_state.analysis_result_cache = None 
        
        st.session_state.auto_start_analysis = False
        total_start = time.time()
        
        with st.status("ç¸½ç¨½æ ¸å®˜æ­£åœ¨é€²è¡Œå…¨æ–¹ä½åˆ†æ...", expanded=True) as status_box:
            progress_bar = st.progress(0)
            
            # 1. OCR (é€™æ®µä¿ç•™ï¼Œé€Ÿåº¦å¾ˆå¿«)
            status_box.write("ğŸ‘€ æ­£åœ¨é€²è¡Œ OCR æ–‡å­—è­˜åˆ¥...")
            ocr_start = time.time()
            
            def process_task(index, item):
                if item.get('full_text'):
                    return index, item.get('header_text',''), item['full_text'], None
                try:
                    item['file'].seek(0)
                    _, h, f, _, _ = extract_layout_with_azure(item['file'], DOC_ENDPOINT, DOC_KEY)
                    return index, h, f, None
                except Exception as e:
                    return index, None, None, str(e)

            with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
                futures = [executor.submit(process_task, i, item) for i, item in enumerate(st.session_state.photo_gallery)]
                for future in concurrent.futures.as_completed(futures):
                    idx, h_txt, f_txt, err = future.result()
                    if not err:
                        st.session_state.photo_gallery[idx].update({'header_text': h_txt, 'full_text': f_txt, 'file': None})
                    progress_bar.progress(0.4 * ((idx + 1) / len(st.session_state.photo_gallery)))

            ocr_duration = time.time() - ocr_start
            
            # 2. çµ„åˆæ‰€æœ‰æ–‡å­— (é—œéµï¼šä¸€æ¬¡ä¸Ÿé€²å»)
            combined_input = ""
            for i, p in enumerate(st.session_state.photo_gallery):
                combined_input += f"\n=== Page {i+1} ===\n{p.get('full_text','')}\n"

            # 3. å‘¼å« AI (é€™è£¡åªæœƒè·‘ä¸€æ¬¡ï¼Œç´„ 20-30 ç§’)
            status_box.write("ğŸ¤– AI æ­£åœ¨å…¨å·åˆ†æ...")
            res_main = agent_unified_check(combined_input, combined_input, GEMINI_KEY, main_model_name)
            progress_bar.progress(0.8)
            
            # 4. Python é‚è¼¯æª¢æŸ¥
            status_box.write("ğŸ Python æ­£åœ¨é€²è¡Œé‚è¼¯æ¯”å°...")
            dim_data = res_main.get("dimension_data", [])
            
            # âš¡ï¸ [æ’å…¥é»]ï¼šPython å¥ªæ¬Šï¼å¼·åˆ¶è¦†å¯« AI çš„åˆ†é¡
            for item in dim_data:
                # å³ä½¿ AI æœ‰å¡« categoryï¼Œæˆ‘å€‘ä¹Ÿç”¨ Python çš„é‚è¼¯è¦†è“‹å®ƒï¼Œä¿è­‰ 100% ä¸€è‡´æ€§
                # æˆ–è€…ï¼Œå¦‚æœ AI æ²’å¡«ï¼Œé€™è£¡å°±æ˜¯è£œå¡«çš„é—œéµ
                new_cat = assign_category_by_python(item.get("item_title", ""))
                item["category"] = new_cat
                # é †ä¾¿æŠŠ category å¯«é€² rules ä¾›å‰ç«¯é¡¯ç¤º (é¸ç”¨)
                if "sl" not in item: item["sl"] = {}
                item["sl"]["lt"] = new_cat
            
            python_numeric_issues = python_numerical_audit(dim_data)
            python_accounting_issues = python_accounting_audit(dim_data, res_main)
            python_process_issues = python_process_audit(dim_data)
            python_header_issues, python_debug_data = python_header_check(st.session_state.photo_gallery)

            ai_filtered_issues = []
            ai_raw_issues = res_main.get("issues", [])
            if isinstance(ai_raw_issues, list):
                for i in ai_raw_issues:
                    if isinstance(i, dict):
                        i['source'] = 'ğŸ¤– ç¸½ç¨½æ ¸ AI'
                        if not any(k in i.get("issue_type", "") for k in ["æµç¨‹", "è¦æ ¼æå–å¤±æ•—", "æœªåŒ¹é…"]):
                            ai_filtered_issues.append(i)

            all_issues = ai_filtered_issues + python_numeric_issues + python_accounting_issues + python_process_issues + python_header_issues
            
            # 5. å­˜æª”èˆ‡å®Œæˆ
            usage = res_main.get("_token_usage", {"input": 0, "output": 0})
            
            # â­ï¸ [é—œéµä¿®æ­£] é€™è£¡å¿…é ˆæŠŠ freight_target å’Œ summary_rows å­˜é€²å»ï¼Œä¸ç„¶é¡¯ç¤ºæ™‚æœƒæŠ“ä¸åˆ°ï¼
            st.session_state.analysis_result_cache = {
                "job_no": res_main.get("job_no", "Unknown"),
                "all_issues": all_issues,
                "total_duration": time.time() - total_start,
                "cost_twd": (usage.get("input", 0)*0.5 + usage.get("output", 0)*3.0) / 1000000 * 32.5,
                "total_in": usage.get("input", 0),
                "total_out": usage.get("output", 0),
                "ocr_duration": ocr_duration,
                "time_eng": time.time() - total_start - ocr_duration,
                
                "ai_extracted_data": dim_data,
                "python_debug_data": python_debug_data,
                
                # ğŸ‘‡ é€™è£¡æ˜¯æˆ‘å¹«æ‚¨è£œä¸Šçš„ï¼Œç‚ºäº†æ–°çš„çœ‹æ¿åŠŸèƒ½
                "freight_target": res_main.get("freight_target", 0),
                "summary_rows": res_main.get("summary_rows", []),
                
                "full_text_for_search": combined_input,
                "combined_input": combined_input
            }
            
            progress_bar.progress(1.0)
            status_box.update(label="âœ… åˆ†æå®Œæˆï¼", state="complete", expanded=False)
            st.rerun()

            # --- ğŸ’¡ [é¡¯ç¤ºçµæœå€å¡Š] æ•¸é‡åŒæ­¥ä¿®æ­£ç‰ˆ ---
    if st.session_state.analysis_result_cache:
        cache = st.session_state.analysis_result_cache
        all_issues = cache.get('all_issues', [])
        
        # 1. é ‚éƒ¨ç‹€æ…‹æ¢
        st.success(f"å·¥ä»¤: {cache['job_no']} | â±ï¸ {cache['total_duration']:.1f}s")
        st.info(f"ğŸ’° æœ¬æ¬¡æˆæœ¬: NT$ {cache['cost_twd']:.2f} (In: {cache['total_in']:,} / Out: {cache['total_out']:,})")
        st.caption(f"ç´°ç¯€è€—æ™‚: Azure OCR {cache['ocr_duration']:.1f}s | AI åˆ†æ {cache['time_eng']:.1f}s")

        # 2. è¦å‰‡æª¢è¦–
        with st.expander("ğŸ” æª¢è¦– Excel è¦å‰‡èˆ‡é‚è¼¯åƒæ•¸", expanded=False):
            rules_text = get_dynamic_rules(cache.get('full_text_for_search',''), debug_mode=True)
            st.markdown(rules_text)
                
        # 3. åŸå§‹æ•¸æ“šæª¢è¦–
        with st.expander("ğŸ“Š æª¢è¦– AI æŠ„éŒ„åŸå§‹æ•¸æ“š", expanded=False):
            st.markdown("**1. æ ¸å¿ƒæŒ‡æ¨™æ‘˜è¦**")
            f_target = cache.get('freight_target', 0)
            sum_rows_len = len(cache.get("summary_rows", []))
            summary_df = pd.DataFrame([{
                "å·¥ä»¤å–®è™Ÿ": cache.get("job_no", "N/A"),
                "é‹è²» Target (PC)": f_target,
                "é‹è²»åµæ¸¬ç‹€æ…‹": "æœ‰æŠ“åˆ°" if f_target > 0 else "æœªåµæ¸¬",
                "ç¸½è¡¨è¡Œæ•¸": sum_rows_len,
                "ç¸½è¡¨ç‹€æ…‹": "æ­£å¸¸" if sum_rows_len > 0 else "ç©ºå€¼"
            }])
            st.dataframe(summary_df, hide_index=True, use_container_width=True)
            st.divider()

            st.markdown("**2. å·¦ä¸Šè§’çµ±è¨ˆè¡¨ (Summary Rows)**")
            sum_rows = cache.get("summary_rows", [])
            if sum_rows:
                df_sum = pd.DataFrame(sum_rows)
                df_sum.rename(columns={"title": "é …ç›®åç¨±", "target": "å¯¦äº¤æ•¸é‡"}, inplace=True)
                st.dataframe(df_sum, hide_index=True, use_container_width=True)
            else:
                st.caption("ç„¡æ•¸æ“š (è®Šæ•¸ summary_rows ç‚ºç©º)")
            st.divider()

            st.markdown("**3. å…¨å·è©³ç´°æŠ„éŒ„æ•¸æ“š (JSON)**")
            st.json(cache.get("ai_extracted_data", []), expanded=True)

        # 4. Python Debug
        with st.expander("ğŸ Python ç¡¬é‚è¼¯åµæ¸¬çµæœ", expanded=False):
            if cache.get('python_debug_data'):
                st.dataframe(cache['python_debug_data'], use_container_width=True, hide_index=True)
            else:
                st.caption("ç„¡åµæ¸¬è³‡æ–™")

        # ========================================================
        # âš¡ï¸ [ä¿®æ­£é‡é»]ï¼šå…ˆé€²è¡Œåˆä½µï¼Œå†æ ¹æ“šåˆä½µå¾Œçš„æ¸…å–®ä¾†è¨ˆç®—æ•¸é‡
        # ========================================================
        
        # 1. åŸ·è¡Œåˆä½µ (æŠŠ 51 å€‹ç•°å¸¸å£“ç¸®æˆ N é¡)
        consolidated_list = consolidate_issues(all_issues)

        # 2. éæ¿¾å‡ºã€ŒçœŸæ­£çš„éŒ¯èª¤ã€ (æ’é™¤åƒ…æ˜¯æœªåŒ¹é…è¦å‰‡çš„è­¦å‘Š)
        # æ³¨æ„ï¼šæˆ‘å€‘æ˜¯åœ¨ consolidated_list ä¸Šåšç¯©é¸ï¼Œé€™æ¨£æ•¸é‡æ‰æœƒå°
        real_errors_consolidated = [i for i in consolidated_list if "æœªåŒ¹é…" not in i.get('issue_type', '')]

        # 3. é¡¯ç¤ºçµè«– (ä½¿ç”¨åˆä½µå¾Œçš„æ•¸é‡)
        if not all_issues:
            st.balloons()
            st.success("âœ… å…¨æ•¸åˆæ ¼ï¼")
        elif not real_errors_consolidated:
            # é€™è£¡ç”¨ len(consolidated_list) ä»£è¡¨é‚„æœ‰å¹¾å€‹é»ƒè‰²è­¦å‘Š
            st.success(f"âœ… æ•¸å€¼åˆæ ¼ï¼ (ä½†æœ‰ {len(consolidated_list)} é¡é …ç›®æœªåŒ¹é…è¦å‰‡)")
        else:
            # é€™è£¡é¡¯ç¤ºç´…è‰²çš„ç•°å¸¸ã€Œé¡åˆ¥ã€æ•¸é‡
            st.error(f"ç™¼ç¾ {len(real_errors_consolidated)} é¡ç•°å¸¸")

        # 4. å¡ç‰‡å¾ªç’°é¡¯ç¤º (ä½¿ç”¨åˆä½µå¾Œçš„æ¸…å–®)
        for item in consolidated_list:
            with st.container(border=True):
                c1, c2 = st.columns([3, 1])
                source_label = item.get('source', '')
                issue_type = item.get('issue_type', 'ç•°å¸¸')
                
                # é ç¢¼é¡¯ç¤ºå„ªåŒ–
                page_str = item.get('page', '?')
                if "," in str(page_str):
                    page_display = f"Pages: {page_str}"
                else:
                    page_display = f"P.{page_str}"

                c1.markdown(f"**{page_display} | {item.get('item')}** `{source_label}`")
                
                if any(kw in issue_type for kw in ["çµ±è¨ˆ", "æ•¸é‡", "æµç¨‹", "æº¯æº"]):
                    c2.error(f"ğŸ›‘ {issue_type}")
                else:
                    c2.warning(f"âš ï¸ {issue_type}")
                
                st.caption(f"åŸå› : {item.get('common_reason', '')}")
                
                failures = item.get('failures', [])
                if failures:
                    table_data = []
                    for f in failures:
                        if isinstance(f, dict):
                            table_data.append({
                                "é …ç›®/ç·¨è™Ÿ": f.get('id', 'æœªçŸ¥'), 
                                "å¯¦æ¸¬/è¨ˆæ•¸": f.get('val', 'N/A'),
                                "æ¨™æº–/å‚™è¨»": f.get('target', ''),
                                "ç‹€æ…‹": f.get('calc', '')
                            })
                    st.dataframe(table_data, use_container_width=True, hide_index=True)
        
        st.divider()
        
        # ... (é€™è£¡æ¥ä½ åŸæœ¬å‰©ä¸‹çš„ä»£ç¢¼å³å¯ï¼Œä¹Ÿè¦è¨˜å¾—ç¸®æ’å¾€å·¦ç§»)
        current_job_no = cache.get('job_no', 'Unknown')
        safe_job_no = current_job_no.replace("/", "_").replace("\\", "_").strip()
        file_name_str = f"{safe_job_no}_cleaned.json"

        # æº–å‚™åŒ¯å‡ºè³‡æ–™
        export_data = []
        for item in st.session_state.photo_gallery:
            export_data.append({
                "table_md": item.get('table_md'),
                "header_text": item.get('header_text'),
                "full_text": item.get('full_text'),
                "raw_json": item.get('raw_json')
            })
        json_str = json.dumps(export_data, indent=2, ensure_ascii=False)

        st.subheader("ğŸ’¾ æ¸¬è©¦è³‡æ–™å­˜æª”")
        st.caption(f"å·²è­˜åˆ¥å·¥ä»¤ï¼š**{current_job_no}**ã€‚ä¸‹è¼‰å¾Œå¯ä¾›ä¸‹æ¬¡æ¸¬è©¦ä½¿ç”¨ã€‚")
        
        st.download_button(
            label=f"â¬‡ï¸ ä¸‹è¼‰æ¸¬è©¦è³‡æ–™ ({file_name_str})",
            data=json_str,
            file_name=file_name_str,
            mime="application/json",
            type="primary"
        )

        # ğŸ’¡ ä½¿ç”¨ .get() å¯ä»¥é˜²æ­¢å› ç‚ºæ‰¾ä¸åˆ°æ¨™ç±¤è€Œç›´æ¥å ±éŒ¯ç•¶æ©Ÿ
        with st.expander("ğŸ‘€ æŸ¥çœ‹å‚³çµ¦ AI çš„æœ€çµ‚æ–‡å­— (Prompt Input)"):
            st.caption("é€™æ‰æ˜¯ AI çœŸæ­£è®€åˆ°çš„å…§å®¹ (å·²éæ¿¾é›œè¨Š)ï¼š")
            st.code(cache.get('combined_input', 'ç„¡è³‡æ–™'), language='markdown')
    
    if st.session_state.photo_gallery and st.session_state.get('source_mode') != 'json':
        st.caption("å·²æ‹æ”ç…§ç‰‡ï¼š")
        cols = st.columns(4)
        for idx, item in enumerate(st.session_state.photo_gallery):
            with cols[idx % 4]:
                if item.get('file'):
                    st.image(item['file'], caption=f"P.{idx+1}", use_container_width=True)
                if st.button("âŒ", key=f"del_{idx}"):
                    st.session_state.photo_gallery.pop(idx)
                    st.session_state.analysis_result_cache = None
                    st.rerun()
else:
    st.info("ğŸ‘† è«‹é»æ“Šä¸Šæ–¹æŒ‰éˆ•é–‹å§‹æ–°å¢ç…§ç‰‡")
