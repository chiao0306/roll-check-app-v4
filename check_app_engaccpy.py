import streamlit as st
import streamlit.components.v1 as components
from azure.core.credentials import AzureKeyCredential
from azure.ai.documentintelligence import DocumentIntelligenceClient
from azure.ai.documentintelligence.models import AnalyzeResult
import google.generativeai as genai
from openai import OpenAI
import json
import time
import concurrent.futures
import pandas as pd
from thefuzz import fuzz
from collections import Counter
import re

# --- 1. é é¢è¨­å®š ---
st.set_page_config(page_title="äº¤è²¨å–®ç¨½æ ¸", page_icon="ğŸ­", layout="centered")

# --- CSS æ¨£å¼ ---
st.markdown("""
<style>
/* 1. æ¨™é¡Œå¤§å°æ§åˆ¶ */
h1 {
    font-size: 1.7rem !important; 
    white-space: nowrap !important;
    overflow: hidden !important; 
    text-overflow: ellipsis !important;
}

/* 2. ä¸»åŠŸèƒ½æŒ‰éˆ• (ç´…è‰² Primary) -> è®Šå¤§ã€è®Šé«˜ */
/* é€™æœƒå½±éŸ¿ã€Œé–‹å§‹åˆ†æã€å’Œã€Œç…§ç‰‡æ¸…é™¤ã€ */
button[kind="primary"] {
    height: 60px;               
    font-size: 20px !important; 
    font-weight: bold !important;
    border-radius: 10px !important;
    margin-top: 0px !important;    
    margin-bottom: 5px !important; 
    width: 100%;                
}

/* 3. æ¬¡è¦æŒ‰éˆ• (ç°è‰² Secondary) -> ä¿æŒåŸç‹€ */
/* é€™æœƒå½±éŸ¿æ¯ä¸€å¼µç…§ç‰‡ä¸‹é¢çš„ã€ŒXã€æŒ‰éˆ•ï¼Œè®“å®ƒç¶­æŒå°å°çš„ */
button[kind="secondary"] {
    height: auto !important;
    font-weight: normal !important;
}
</style>
""", unsafe_allow_html=True)
# --- 2. ç§˜å¯†é‡‘é‘°è®€å– ---
try:
    DOC_ENDPOINT = st.secrets["DOC_ENDPOINT"]
    DOC_KEY = st.secrets["DOC_KEY"]
    GEMINI_KEY = st.secrets["GEMINI_KEY"]
    OPENAI_KEY = st.secrets.get("OPENAI_KEY", "")
except:
    st.error("æ‰¾ä¸åˆ°é‡‘é‘°ï¼è«‹åœ¨ Streamlit Cloud è¨­å®š Secretsã€‚")
    st.stop()

# --- 3. åˆå§‹åŒ– Session State ---
if 'photo_gallery' not in st.session_state: st.session_state.photo_gallery = []
if 'uploader_key' not in st.session_state: st.session_state.uploader_key = 0
if 'auto_start_analysis' not in st.session_state: st.session_state.auto_start_analysis = False

# --- å´é‚Šæ¬„æ¨¡å‹è¨­å®š (åˆä½µç‚ºå–®ä¸€é¸æ“‡) ---
with st.sidebar:
    st.header("æ¨¡å‹è¨­å®š")
    
    # é€™è£¡åŠ å…¥æœ€æ–°çš„ Gemini æ¨¡å‹
    model_options = {
        "Gemini 3 Flash preview": "gemini-3-flash-preview",
        "Gemini 2.5 Flash": "models/gemini-2.5-flash",
        "Gemini 2.5 Flash Lite": "gemini-2.5-flash-lite",
        "Gemini 2.5 Pro": "models/gemini-2.5-pro",
        "GPT-5 Mini": "models/gpt-5-mini-2025-08-07",
        "GPT-5 Nano": "models/gpt-5-nano-2025-08-07",
        
    }
    options_list = list(model_options.keys())
    
    st.subheader("ğŸ¤– ç¸½ç¨½æ ¸ Agent")
    model_selection = st.selectbox(
        "è² è²¬ï¼šè¦æ ¼ã€è£½ç¨‹ã€æ•¸é‡ã€çµ±è¨ˆå…¨åŒ…", 
        options=options_list, 
        index=1, 
        key="main_model"
    )
    main_model_name = model_options[model_selection]
    
    st.divider()
    
    default_auto = st.query_params.get("auto", "true") == "true"
    def update_url_param():
        current_state = "true" if st.session_state.enable_auto_analysis else "false"
        st.query_params["auto"] = current_state

    st.toggle(
        "âš¡ ä¸Šå‚³å¾Œè‡ªå‹•åˆ†æ", 
        value=default_auto, 
        key="enable_auto_analysis", 
        on_change=update_url_param
    )

# --- Excel è¦å‰‡è®€å–å‡½æ•¸ (å°ˆæ¥­æ¥µç°¡ç‰ˆ) ---
@st.cache_data
def get_dynamic_rules(ocr_text, debug_mode=False):
    try:
        df = pd.read_excel("rules.xlsx")
        df.columns = [c.strip() for c in df.columns]
        ocr_text_clean = str(ocr_text).upper().replace(" ", "").replace("\n", "")
        
        ai_prompt_list = []    # çµ¦ AI çš„ (ç´”æ–‡å­—)
        debug_view_list = []   # çµ¦äººçœ‹çš„ (æ’ç‰ˆæ¸…æ½”)

        for index, row in df.iterrows():
            item_name = str(row.get('Item_Name', '')).strip()
            if not item_name or "(é€šç”¨)" in item_name: continue
            
            score = fuzz.partial_ratio(item_name.upper().replace(" ", ""), ocr_text_clean)
            if score >= 85:
                # å–å€¼èˆ‡æ¸…æ´—
                def clean(v): return str(v).strip() if v and str(v) != 'nan' else None
                
                spec = clean(row.get('Standard_Spec', ''))
                logic = clean(row.get('Logic_Prompt', ''))
                u_fr = clean(row.get('Unit_Rule_Freight', ''))
                u_loc = clean(row.get('Unit_Rule_Local', ''))
                u_agg = clean(row.get('Unit_Rule_Agg', ''))

                # --- A. å»ºæ§‹ AI Prompt (ç¶­æŒä¸è®Š) ---
                if not debug_mode:
                    if spec or logic:
                        desc = f"- [åƒè€ƒè³‡è¨Š] {item_name}\n"
                        if spec: desc += f"  - æ¨™æº–è¦æ ¼: {spec}\n"
                        if logic: desc += f"  - æ³¨æ„äº‹é …: {logic}\n"
                        ai_prompt_list.append(desc)
                
                # --- B. å»ºæ§‹ Debug é¡¯ç¤º (å»é™¤åœ–æ¡ˆï¼Œæ”¹ç”¨è¡¨æ ¼æ„Ÿæ’ç‰ˆ) ---
                else:
                    # ä½¿ç”¨ Markdown çš„å¼•ç”¨å€å¡Š (>) ä¾†åšå±¤ç´šå€åˆ†ï¼Œçœ‹èµ·ä¾†å¾ˆä¹¾æ·¨
                    block = f"#### â–  {item_name} (åŒ¹é…åº¦ {score}%)\n"
                    
                    # AI å€å¡Š
                    block += "**[ AI Prompt è¼¸å…¥ ]**\n"
                    if spec or logic:
                        if spec: block += f"- è¦æ ¼æ¨™æº– : `{spec}`\n"
                        if logic: block += f"- æ³¨æ„äº‹é … : `{logic}`\n"
                    else:
                        block += "- (ç„¡ç‰¹å®šè¼¸å…¥)\n"

                    # Python å€å¡Š
                    block += "\n**[ Python ç¡¬é‚è¼¯è¨­å®š ]**\n"
                    has_py = False
                    if u_fr: 
                        block += f"- é‹è²»é‚è¼¯ : `{u_fr}`\n"
                        has_py = True
                    if u_loc:
                        block += f"- å–®é …è¦å‰‡ : `{u_loc}`\n"
                        has_py = True
                    if u_agg:
                        block += f"- èšåˆè¦å‰‡ : `{u_agg}`\n"
                        has_py = True
                    
                    if not has_py:
                        block += "- (ä½¿ç”¨é è¨­é‚è¼¯)\n"
                    
                    block += "\n---\n"
                    debug_view_list.append(block)

        if debug_mode:
            if not debug_view_list: return "ç„¡ç‰¹å®šè¦å‰‡å‘½ä¸­ã€‚"
            return "\n".join(debug_view_list)
        else:
            return "\n".join(ai_prompt_list) if ai_prompt_list else ""

    except Exception as e:
        return f"è®€å–éŒ¯èª¤: {e}"

# --- 4. æ ¸å¿ƒå‡½æ•¸ï¼šAzure ç¥ä¹‹çœ¼ ---
def extract_layout_with_azure(file_obj, endpoint, key):
    client = DocumentIntelligenceClient(endpoint=endpoint, credential=AzureKeyCredential(key))
    file_content = file_obj.getvalue()
    
    poller = client.begin_analyze_document("prebuilt-layout", file_content, content_type="application/octet-stream")
    result: AnalyzeResult = poller.result()
    
    markdown_output = ""
    full_content_text = ""
    real_page_num = "Unknown"
    
    bottom_stop_keywords = ["æ³¨æ„äº‹é …", "ä¸­æ©Ÿå“æª¢å–®ä½", "ä¿å­˜æœŸé™", "è¡¨å–®ç·¨è™Ÿ", "FORM NO", "ç°½ç« "]
    top_right_noise_keywords = [
        "æª¢é©—é¡åˆ¥", "å°ºå¯¸æª¢é©—", "ä¾åœ–é¢æ¨™è¨˜", "ææ–™æª¢é©—", "æˆä»½åˆ†æ", 
        "éç ´å£æ€§", "æ­£å¸¸åŒ–", "é€€ç«", "æ·¬.å›ç«", "è¡¨é¢ç¡¬åŒ–", "è©¦è»Š",
        "æ€§èƒ½æ¸¬è©¦", "è©¦å£“è©¦æ¼", "å‹•.éœå¹³è¡¡è©¦é©—", ":selected:", ":unselected:",
        "æŠ—æ‹‰", "ç¡¬åº¦è©¦é©—", "UT", "PT", "MT"
    ]
    
    if result.tables:
        for idx, table in enumerate(result.tables):
            # 1. å–å¾—é ç¢¼ (ä¿ç•™åŸé‚è¼¯)
            page_num = "Unknown"
            if table.bounding_regions: page_num = table.bounding_regions[0].page_number
            
            # =========================================================
            # ğŸ” [æ–°å¢] æ™ºæ…§æ¨™ç±¤åµæ¸¬ï¼šåœ¨è™•ç†è¡¨æ ¼å‰ï¼Œå…ˆåˆ¤æ–·å®ƒæ˜¯èª°
            # =========================================================
            table_tag = "æœªçŸ¥è¡¨æ ¼"
            
            # æŠ€å·§ï¼šæŠ“å–è¡¨æ ¼ã€Œç¬¬ä¸€åˆ— (row_index=0)ã€çš„æ‰€æœ‰æ–‡å­—ä¾†åˆ¤æ–·
            # é€™æ¨£ä¸ç”¨è®€å®Œæ•´å¼µè¡¨ï¼Œåªè¦çœ‹è¡¨é ­å°±çŸ¥é“å®ƒæ˜¯ç¸½è¡¨é‚„æ˜¯æ˜ç´°
            first_cells = [c.content for c in table.cells if c.row_index == 0]
            first_row_text = "".join(first_cells)
            
            # å®šç¾©é—œéµå­— (æ‚¨å¯ä»¥æ ¹æ“šå¯¦éš›è¡¨æ ¼å¾®èª¿)
            summary_keywords = ["å¯¦äº¤", "ç”³è«‹", "åç¨±åŠè¦ç¯„", "å®Œæˆäº¤è²¨æ—¥æœŸ", "å­˜æ”¾ä½ç½®"]
            detail_keywords = ["è¦ç¯„æ¨™æº–", "æª¢é©—ç´€éŒ„", "å¯¦æ¸¬", "ç·¨è™Ÿ", "å°ºå¯¸", "W3 #", "å…¬å·®"]

            if any(k in first_row_text for k in summary_keywords):
                table_tag = "SUMMARY_TABLE (ç¸½è¡¨)"
            elif any(k in first_row_text for k in detail_keywords):
                table_tag = "DETAIL_TABLE (æ˜ç´°è¡¨)"
            
            # ğŸ“ [ä¿®æ”¹] è¼¸å‡ºæ¨™é ­ï¼šé€™è£¡ä¸å†åªå¯« Table Xï¼Œè€Œæ˜¯åŠ ä¸Šæˆ‘å€‘åˆ¤æ–·çš„æ¨™ç±¤
            # åŠ ä¸Š "===" æ˜¯ç‚ºäº†è®“ Prompt è£¡çš„ã€Œæ³¨æ„ç¯„åœã€æŒ‡ä»¤èƒ½ç²¾æº–é–å®š
            markdown_output += f"\n\n=== [{table_tag} | Page {page_num}] ===\n"
            # =========================================================

            rows = {}
            stop_processing_table = False 
            
            # --- ä»¥ä¸‹ä¿ç•™æ‚¨åŸæœ¬çš„ Cell è™•ç†é‚è¼¯ï¼Œå®Œå…¨ä¸ç”¨å‹• ---
            for cell in table.cells:
                if stop_processing_table: break
                content = cell.content.replace("\n", " ").strip()
                
                for kw in bottom_stop_keywords:
                    if kw in content:
                        stop_processing_table = True
                        break
                if stop_processing_table: break
                
                is_noise = False
                for kw in top_right_noise_keywords:
                    if kw in content:
                        is_noise = True
                        break
                if is_noise: content = "" 

                r, c = cell.row_index, cell.column_index
                if r not in rows: rows[r] = {}
                rows[r][c] = content
            
            for r in sorted(rows.keys()):
                row_cells = []
                if rows[r]:
                    max_col = max(rows[r].keys())
                    for c in range(max_col + 1): 
                        row_cells.append(rows[r].get(c, ""))
                    markdown_output += "| " + " | ".join(row_cells) + " |\n"

    if result.content:
        match = re.search(r"(?:é …æ¬¡|Page|é æ¬¡|NO\.)[:\s]*(\d+)\s*[/ï¼]\s*\d+", result.content, re.IGNORECASE)
        if match:
            real_page_num = match.group(1)

        cut_index = len(result.content)
        for keyword in bottom_stop_keywords:
            idx = result.content.find(keyword)
            if idx != -1 and idx < cut_index:
                cut_index = idx
        
        temp_text = result.content[:cut_index]
        for noise in top_right_noise_keywords:
            temp_text = temp_text.replace(noise, "")
            
        full_content_text = temp_text
        header_snippet = full_content_text[:800]
    else:
        full_content_text = ""
        header_snippet = ""

    return markdown_output, header_snippet, full_content_text, None, real_page_num

# --- Python ç¡¬é‚è¼¯ï¼šè¡¨é ­ä¸€è‡´æ€§æª¢æŸ¥ (é•·åº¦æ•æ„Ÿç‰ˆ) ---
def python_header_check(photo_gallery):
    issues = []
    if not photo_gallery:
        return issues, []

    # å®šç¾© Regex (é‡å° "å»ç©ºç™½+å»æ›è¡Œ" å¾Œçš„å­—ä¸²è¨­è¨ˆ)
    patterns = {
        # ã€ä¿®æ”¹é» 1ã€‘å·¥ä»¤ Regex æ”¾å¯¬ï¼š
        # åŸæœ¬åªæŠ“ W é–‹é ­ï¼Œç¾åœ¨æ”¹æŠ“ "ç·¨è™Ÿ" å¾Œé¢æ¥çš„ "ä»»ä½•è‹±æ•¸å­—ä¸²"
        # é€™æ¨£å°±ç®—å®ƒå¯« WW363... æˆ–æ˜¯ 12345... éƒ½èƒ½æ•´ä¸²æŠ“å‡ºä¾†æ¯”å°
        "å·¥ä»¤ç·¨è™Ÿ": r"[å·¥åœŸä¸‹][ä»¤å†·ä»Š]ç·¨è™Ÿ[:\.]*([A-Za-z0-9\-\_]+)", 
        
        "é å®šäº¤è²¨": r"[é é¢„é …é ‚][å®šäº¤].*?(\d{2,4}[\.\-/]\d{1,2}[\.\-/]\d{1,2})",
        "å¯¦éš›äº¤è²¨": r"[å¯¦çœŸ][éš›äº¤].*?(\d{2,4}[\.\-/]\d{1,2}[\.\-/]\d{1,2})"
    }

    extracted_data = [] 
    all_values = {key: [] for key in patterns}

    for i, page in enumerate(photo_gallery):
        # æš´åŠ›æ¸…æ´—ï¼šå»æ›è¡Œã€å»ç©ºæ ¼ã€è½‰å¤§å¯«
        raw_text = page.get('header_text', '') + page.get('full_text', '')
        clean_text = raw_text.replace("\n", "").replace(" ", "").replace("\r", "").upper()
        
        # ã€ä¿®æ”¹é» 2ã€‘é ç¢¼é˜²å‘†ï¼šç¢ºä¿ä¸€å®šæœ‰å€¼
        # å„ªå…ˆæŠ“ real_pageï¼ŒæŠ“ä¸åˆ°å°±ç”¨ index
        r_page = page.get('real_page')
        if not r_page or r_page == "Unknown":
            page_label = f"P.{i + 1}"
        else:
            page_label = f"P.{r_page}"
            
        page_result = {"é æ•¸": page_label}
        
        for key, pattern in patterns.items():
            match = re.search(pattern, clean_text)
            if match:
                val = match.group(1).strip()
                
                # ã€ä¿®æ”¹é» 3ã€‘é‡å°å·¥ä»¤çš„ç‰¹æ®Šè™•ç† (å¦‚æœå¤ªé•·å¯èƒ½å°±æ˜¯é‡è¤‡æ‰“å­—)
                if key == "å·¥ä»¤ç·¨è™Ÿ":
                    # å¦‚æœä½ ç¢ºå®šå·¥ä»¤åªæœ‰ 10 ç¢¼ï¼Œä½†æŠ“åˆ°äº† 11 ç¢¼ä»¥ä¸Š (å¦‚ WW...)
                    # æˆ‘å€‘ä¿ç•™é€™å€‹éŒ¯èª¤çš„å€¼ï¼Œè®“å¾Œé¢çš„å¤šæ•¸æ±ºå»æŠŠå®ƒæªå‡ºä¾†
                    pass 
                
                page_result[key] = val
                all_values[key].append(val)
            else:
                page_result[key] = "N/A"
        
        extracted_data.append(page_result)

    # æ­¥é©Ÿ 2: æ±ºå®šã€Œæ­£ç¢ºæ¨™æº–ã€ (ä½¿ç”¨å¤šæ•¸æ±º)
    standard_data = {}
    for key, values in all_values.items():
        if values:
            # æ¿¾æ‰ N/A å¾Œå†æŠ•ç¥¨
            valid_values = [v for v in values if v != "N/A"]
            if valid_values:
                most_common = Counter(valid_values).most_common(1)[0][0]
                standard_data[key] = most_common
            else:
                standard_data[key] = "N/A"
        else:
            standard_data[key] = "N/A"

    # æ­¥é©Ÿ 3: æ¯”å°æ¯ä¸€é 
    for data in extracted_data:
        page_num = data['é æ•¸']
        
        for key, standard_val in standard_data.items():
            current_val = data[key]
            
            if standard_val == "N/A": continue # å…¨å·éƒ½æ²’æŠ“åˆ°å°±ä¸æ¯”äº†

            # é–‹å§‹æ¯”å° (å­—ä¸²ä¸ç›¸ç­‰)
            if current_val != standard_val:
                
                # åˆ¤æ–·æ˜¯å¦ç‚ºé•·åº¦ç•°å¸¸ (é‡å°å·¥ä»¤)
                reason = "èˆ‡å…¨å·å¤šæ•¸é é¢ä¸ä¸€è‡´"
                if key == "å·¥ä»¤ç·¨è™Ÿ" and len(current_val) != len(standard_val):
                    reason += f" (é•·åº¦ç•°å¸¸: {len(current_val)}ç¢¼ vs æ¨™æº–{len(standard_val)}ç¢¼)"

                issue = {
                    "page": page_num.replace("P.", ""),
                    "item": f"è¡¨é ­æª¢æŸ¥-{key}",
                    "rule_used": "Pythonç¡¬é‚è¼¯æª¢æŸ¥",
                    "issue_type": "è·¨é è³‡è¨Šä¸ç¬¦",
                    "spec_logic": f"æ‡‰ç‚º {standard_val}",
                    "common_reason": reason,
                    "failures": [
                        {"id": "å…¨å·åŸºæº–", "val": standard_val, "calc": "å¤šæ•¸æ±ºæ¨™æº–"},
                        {"id": f"æœ¬é ({page_num})", "val": current_val, "calc": "ç•°å¸¸/æ¼æŠ“"}
                    ],
                    "source": "ğŸ¤– ç³»çµ±è‡ªå‹•"
                }
                issues.append(issue)
                
    return issues, extracted_data
    
def python_engineering_audit(dimension_data):
    """
    Python å·¥ç¨‹å¼•æ“ (æ–°å¢ï¼šè² è²¬ Excel å¼·åˆ¶åˆ†é¡èˆ‡æ•¸å€¼æª¢æŸ¥)
    1. é€™æ˜¯åŸæœ¬æˆ‘å€‘è¦ä¿®æ”¹çš„é‚è¼¯ï¼Œç¾åœ¨ç¨ç«‹å‡ºä¾†ï¼Œä¸èˆ‡è¡¨é ­æª¢æŸ¥è¡çªã€‚
    2. è² è²¬åŸ·è¡Œï¼šRange(å†ç”Ÿ), Un_regen(æœ¬é«”), Max, Min, Exempt(è±å…)ã€‚
    """
    issues = []
    import re

    # è¼”åŠ©ï¼šæ•¸å€¼æå–
    def get_val(val_str):
        clean_v = "".join(re.findall(r"[\d\.\-]+", str(val_str)))
        try: return float(clean_v)
        except: return None

    # æ ¸å¿ƒæª¢æŸ¥è¿´åœˆ
    for item in dimension_data:
        p_num = item.get("page", "?")
        title = item.get("item_title", "Unknown")
        ds_str = item.get("ds", "")
        
        # 1. å–å¾—åˆ†é¡ (é€™è£¡æœƒå»å‘¼å«æˆ‘å€‘ç­‰ä¸‹è¦æ›´æ–°çš„ assign_category_by_python)
        # é€™ä¸€æ­¥æœ€é—œéµï¼å®ƒæœƒå»è®€ Excel çœ‹æœ‰æ²’æœ‰å¼·åˆ¶è¦å‰‡
        final_category = assign_category_by_python(title)
        
        # 2. âš¡ï¸ è±å…æ©Ÿåˆ¶ï¼šè‹¥ Excel è¨­å®šç‚ºã€Œè±å…ã€ï¼Œç›´æ¥è·³é
        if final_category == "exempt":
            continue

        # 3. åŸ·è¡Œå„é¡åˆ¥æª¢æŸ¥
        
        # A. Un_regen (æœ¬é«”æœªå†ç”Ÿ - å¼·åˆ¶æ•´æ•¸æª¢æŸ¥)
        if final_category == "un_regen":
            for pair in ds_str.split("|"):
                if ":" not in pair: continue
                rid, val_s = pair.split(":")[:2]
                val = get_val(val_s)
                
                if val is not None:
                    # æª¢æŸ¥æ˜¯å¦ç‚ºæ•´æ•¸ (å…è¨± 0.05 èª¤å·®)
                    if abs(val - round(val)) > 0.05:
                         issues.append({
                            "page": p_num,
                            "item": title,
                            "issue_type": "âš ï¸ç•°å¸¸(æœªå†ç”Ÿ)",
                            "common_reason": "æ‡‰ç‚ºæ•´æ•¸ (Excelè¦å‰‡:æœ¬é«”æœªå†ç”Ÿ)",
                            "failures": [{"id": rid, "val": val, "calc": "éæ•´æ•¸"}],
                            "source": "ğŸ å·¥ç¨‹å¼•æ“"
                        })

        # B. Range (å†ç”Ÿè»Šä¿® - å€é–“æª¢æŸ¥)
        elif final_category == "range":
            # é€™è£¡æ‚¨å¯ä»¥å‘¼å«åŸæœ¬å¯«å¥½çš„ check_range é‚è¼¯
            # æˆ–è€…æš«æ™‚ç•™ç©ºï¼Œè‡³å°‘å®ƒä¸æœƒèª¤åˆ¤æˆ "æœªå†ç”Ÿ"
            pass 

        # C. Max/Min Limit (è»¸é ¸/éŠ²è£œ)
        elif final_category == "max_limit" or final_category == "min_limit":
             # é€™è£¡å‘¼å«åŸæœ¬çš„ check_limit é‚è¼¯
             pass 

    return issues

def assign_category_by_python(item_title):
    """
    Python åˆ†é¡å®˜ (v5: äº’æ–¥é–åŠ å¼·ç‰ˆ)
    1. å¼•å…¥äº’æ–¥é‚è¼¯ï¼šè§£æ±ºæ¨™é¡ŒåŒæ™‚å‡ºç¾è¡çªé—œéµå­—æ™‚çš„èª¤åˆ¤ã€‚
       - è¦å‰‡ï¼šè‹¥æ¨™é¡Œå«æœ‰ã€Œæœªå†ç”Ÿ/ç²—è»Šã€ï¼Œå‰‡å¼·åˆ¶å±è”½ã€Œå†ç”Ÿ/ç²¾è»Šã€çš„åˆ¤å®šã€‚
         (ä¾‹å¦‚: "æœ¬é«”æœªå†ç”Ÿ (å¾ŒçºŒå†ç”Ÿ)" -> æ‡‰åˆ¤ç‚º Un_regenï¼Œè€Œé Range)
    2. å„ªå…ˆæ¬Šï¼šExcelå¼·åˆ¶è¦å‰‡ > è±å… > éŠ²è£œ > äº’æ–¥åˆ¤å®š > ä¸€èˆ¬é—œéµå­—ã€‚
    """
    import pandas as pd
    from thefuzz import fuzz
    import re

    # 0. æ¸…æ´—å·¥å…·
    def clean_text(text):
        return str(text).replace(" ", "").replace("\n", "").replace("\r", "").replace('"', '').replace("'", "").strip()

    title_clean = clean_text(item_title)
    t = str(item_title).upper().replace(" ", "").replace("\n", "").replace('"', "")

    # --- 1. å˜—è©¦è®€å– Excel å¼·åˆ¶è¦å‰‡ (æœ€é«˜å„ªå…ˆæ¬Š) ---
    try:
        df = pd.read_excel("rules.xlsx")
        df.columns = [c.strip() for c in df.columns]
        
        best_score = 0
        forced_rule = None
        
        for _, row in df.iterrows():
            rule_val = str(row.get('Category_Rule', '')).strip()
            if not rule_val or rule_val.lower() == 'nan': continue
            
            iname = str(row.get('Item_Name', '')).strip()
            iname_clean = clean_text(iname)
            
            score = fuzz.partial_ratio(iname_clean, title_clean)
            if score < 95: 
                 t_no = re.sub(r"[\(ï¼ˆ].*?[\)ï¼‰]", "", title_clean)
                 sc_no = fuzz.partial_ratio(iname_clean, t_no)
                 if sc_no > score: score = sc_no
            
            if score > 85: 
                if score > best_score:
                    best_score = score
                    forced_rule = rule_val
                elif score == best_score:
                    if len(rule_val) > len(forced_rule if forced_rule else ""):
                        forced_rule = rule_val

        if forced_rule:
            fr = forced_rule.upper()
            if "è±å…" in fr or "EXEMPT" in fr: return "exempt"
            if "å†ç”Ÿ" in fr or "ç²¾è»Š" in fr or "RANGE" in fr: return "range"
            if "éŠ²" in fr or "ç„Š" in fr or "MIN" in fr: return "min_limit" # éŠ²è£œå„ªå…ˆ
            if "è»¸é ¸" in fr or "MAX" in fr: return "max_limit"
            if "æœ¬é«”" in fr or "UN_REGEN" in fr: return "un_regen"
            
    except Exception: pass

    # --- 2. âš¡ï¸ äº’æ–¥é–é‚è¼¯ (Conflict Check) ---
    # å…ˆåˆ†æå…·å‚™å“ªäº›å±¬æ€§
    has_weld = any(k in t for k in ["éŠ²è£œ", "éŠ²æ¥", "ç„Š", "WELD"])
    has_unregen = any(k in t for k in ["æœªå†ç”Ÿ", "UN_REGEN", "ç²—è»Š"])
    has_regen = any(k in t for k in ["å†ç”Ÿ", "ç ”ç£¨", "ç²¾åŠ å·¥", "è»Šä¿®", "KEYWAY", "GRIND", "MACHIN", "ç²¾è»Š", "çµ„è£", "æ‹†è£", "è£é…", "ASSY"])
    
    # äº’æ–¥è¦å‰‡ A: éŠ²è£œæœ€å¤§ (ä¸€æ—¦æœ‰éŠ²è£œï¼Œé€šå¸¸å°±æ˜¯é©— Min Limitï¼Œä¸ç®¡æœ‰æ²’æœ‰å¯«æœªå†ç”Ÿ)
    if has_weld: return "min_limit"

    # äº’æ–¥è¦å‰‡ B: æœªå†ç”Ÿ vs å†ç”Ÿ
    # å¦‚æœåŒæ™‚å‡ºç¾ (ä¾‹å¦‚ "æœªå†ç”Ÿè»Šä¿®")ï¼Œæˆ‘å€‘å¸Œæœ›å®ƒæ˜¯ "Un_regen" (é©—æ•´æ•¸)ï¼Œè€Œä¸æ˜¯ "Range"
    if has_unregen:
        if any(k in t for k in ["è»¸é ¸", "å…§å­”", "JOURNAL"]): return "max_limit"
        return "un_regen"

    # --- 3. ä¸€èˆ¬é—œéµå­—åˆ¤æ–· (Fallback) ---
    # å¦‚æœé€šéäº†ä¸Šé¢çš„äº’æ–¥é– (ä»£è¡¨æ²’æœ‰éŠ²è£œï¼Œä¹Ÿæ²’æœ‰æœªå†ç”Ÿ)ï¼Œæ‰çœ‹æ˜¯ä¸æ˜¯å†ç”Ÿ
    if has_regen: return "range"

    return "unknown"

def consolidate_issues(issues):
    """
    ğŸ—‚ï¸ ç•°å¸¸åˆä½µå™¨ï¼šå°‡ã€Œé …ç›®ã€ã€ã€ŒéŒ¯èª¤é¡å‹ã€ã€ã€ŒåŸå› ã€å®Œå…¨ç›¸åŒçš„ç•°å¸¸åˆä½µæˆä¸€å¼µå¡ç‰‡
    """
    grouped = {}
    
    for i in issues:
        # 1. ç”¢ç”Ÿåˆä½µé‘°åŒ™ (Key)ï¼šé …ç›® + é¡å‹ + åŸå› 
        # é€™æ¨£ç¢ºä¿åªæœ‰çœŸæ­£ä¸€æ¨£çš„å•é¡Œæ‰æœƒè¢«ä¸¦åœ¨ä¸€èµ·
        key = (i.get('item', ''), i.get('issue_type', ''), i.get('common_reason', ''))
        
        if key not in grouped:
            # åˆå§‹åŒ–ï¼šè¤‡è£½ç¬¬ä¸€ç­†è³‡æ–™
            grouped[key] = i.copy()
            # æŠŠé ç¢¼è½‰æˆ Set é›†åˆ (é¿å…é‡è¤‡)
            grouped[key]['pages_set'] = {str(i.get('page', '?'))}
            # ç¢ºä¿ failures æ˜¯ç¨ç«‹çš„ list
            grouped[key]['failures'] = i.get('failures', []).copy()
        else:
            # åˆä½µï¼šæŠŠæ–°çš„é ç¢¼åŠ é€²å»
            grouped[key]['pages_set'].add(str(i.get('page', '?')))
            # åˆä½µï¼šæŠŠæ–°çš„è­‰æ“š (failures) åŠ åˆ°è¡¨æ ¼è£¡
            grouped[key]['failures'].extend(i.get('failures', []))
            
    # 2. è½‰å› List ä¸¦æ•´ç†é ç¢¼æ ¼å¼
    result = []
    for key, val in grouped.items():
        # é ç¢¼æ’åºï¼šè®“å®ƒé¡¯ç¤º P.1, P.3, P.5 è€Œä¸æ˜¯äº‚è·³
        sorted_pages = sorted(list(val['pages_set']), key=lambda x: int(x) if x.isdigit() else 999)
        val['page'] = ", ".join(sorted_pages) # è®Šæˆå­—ä¸² "1, 3, 5"
        
        # ç§»é™¤æš«å­˜çš„ set
        del val['pages_set']
        result.append(val)
        
    return result

# --- 5. ç¸½ç¨½æ ¸ Agent (é›™æ ¸å¿ƒå¼•æ“ç‰ˆï¼šGemini + OpenAI) ---
def agent_unified_check(combined_input, full_text_for_search, api_key, model_name):
    # 1. æº–å‚™ Prompt (è¦å‰‡èˆ‡æŒ‡ä»¤)
    dynamic_rules = get_dynamic_rules(full_text_for_search)

    system_prompt = f"""
    ä½ æ˜¯ä¸€ä½æ¥µåº¦åš´è¬¹çš„ä¸­é‹¼æ©Ÿæ¢°å“ç®¡ã€æ•¸æ“šæŠ„éŒ„å“¡ã€‘ã€‚ä½ å¿…é ˆåƒã€Œé›»è…¦ç¨‹å¼ã€ä¸€æ¨£åŸ·è¡Œä»»å‹™ã€‚
    
    {dynamic_rules}

    ---

    #### âš”ï¸ æ¨¡çµ„ Aï¼šå·¥ç¨‹å°ºå¯¸æ•¸æ“šæå– (AI ä»»å‹™ï¼šç´”æŠ„éŒ„)
    âš ï¸ **æ³¨æ„ç¯„åœ**ï¼šä½ åªèƒ½å¾æ¨™è¨˜ç‚º `=== [DETAIL_TABLE (æ˜ç´°è¡¨)] ===` çš„å€åŸŸæå–æ•¸æ“šã€‚
    
    1. **è¦æ ¼æŠ„éŒ„ (std_spec)**ï¼šç²¾ç¢ºæŠ„éŒ„æ¨™é¡Œä¸­å« `mm`ã€`Â±`ã€`+`ã€`-` çš„åŸå§‹æ–‡å­—ã€‚
    
    2. **æ¨™é¡ŒæŠ„éŒ„ (item_title)**ï¼šâš ï¸ æ¥µåº¦é‡è¦ï¼å¿…é ˆå®Œæ•´æŠ„éŒ„é …ç›®æ¨™é¡Œï¼Œ**åš´ç¦éºæ¼**ã€Œæœªå†ç”Ÿã€ã€ã€ŒéŠ²è£œã€ã€ã€Œè»Šä¿®ã€ã€ã€Œè»¸é ¸ã€ç­‰é—œéµå­—ã€‚
    
    3. **ç›®æ¨™æ•¸é‡æå– (item_pc_target)**ï¼š
       - è«‹å¾æ¨™é¡Œä¸­æå–æ‹¬è™Ÿå…§çš„æ•¸é‡è¦æ±‚ï¼ˆä¾‹å¦‚æ¨™é¡Œå« `(4SET)` å‰‡æå– `4`ï¼Œ`(10PC)` å‰‡æå– `10`ï¼‰ã€‚
       - è‹¥ç„¡æ‹¬è™Ÿæ¨™è¨»æ•¸é‡ï¼Œè«‹å¡« `0`ã€‚

    4. **åˆ†é¡ (category)**ï¼š**è«‹ç›´æ¥å›å‚³ `null`**ã€‚ç”±å¾Œç«¯ç¨‹å¼åˆ¤å®šã€‚

    5. **æ•¸æ“šæŠ„éŒ„ (ds) èˆ‡ å­—ä¸²ä¿è­·è¦ç¯„**ï¼š
       - **æ ¼å¼**ï¼šè¼¸å‡ºç‚º `"ID:å€¼|ID:å€¼"` çš„å­—ä¸²æ ¼å¼ã€‚
       - **ç¦æ­¢ç°¡åŒ–**ï¼šå¯¦æ¸¬å€¼è‹¥é¡¯ç¤º `349.90`ï¼Œå¿…é ˆè¼¸å‡º `"349.90"`ï¼Œä¿ç•™å°¾æ•¸ 0ã€‚
       - **ğŸš« é‡åˆ°å¹²æ“¾ä¸é‘½ç‰›è§’å°–**ï¼šè‹¥å„²å­˜æ ¼å…§çš„æ•¸å€¼å› æ‰‹å¯«å¡—æ”¹ã€åœ“åœˆé®æ“‹ã€æ±¡é»ã€å­—è·¡é»é€£æˆ–å…‰ç·šåå…‰ï¼Œå°è‡´ä½ ç„¡æ³•ã€Œ100% ç¢ºå®šã€åŸå§‹æ‰“å°æ•¸å­—æ™‚ï¼Œ**åš´ç¦è…¦è£œæˆ–çŒœæ¸¬**ã€‚
       - **å£è»Œæ¨™è¨˜ [BAD]**ï¼šè«‹å°‡è©²ç­†æ•¸å€¼ç›´æ¥æ¨™è¨˜ç‚º `[!]`ã€‚
       - **ç¯„ä¾‹**ï¼šè‹¥ ID æ¸…æ¥šä½†æ•¸å€¼æ¨¡ç³Š -> `"V100:[!]"`ï¼›è‹¥æ•´å€‹å„²å­˜æ ¼éƒ½çœ‹ä¸æ¸… -> `"[!] : [!]"`ã€‚
       - **è·³éç­–ç•¥**ï¼šä¸€æ—¦æ¨™è¨˜ç‚º `[!]`ï¼Œè«‹ç«‹å³è·³åˆ°ä¸‹ä¸€æ ¼ï¼Œä¸è¦æµªè²» Token æè¿°é›œè¨Šã€‚

    #### ğŸ’° æ¨¡çµ„ Bï¼šæœƒè¨ˆæŒ‡æ¨™æå– (AI ä»»å‹™ï¼šæŠ„éŒ„)
    âš ï¸ **æ³¨æ„ç¯„åœ**ï¼šä½ åªèƒ½å¾æ¨™è¨˜ç‚º `=== [SUMMARY_TABLE (ç¸½è¡¨)] ===` çš„å€åŸŸæå–æ•¸æ“šã€‚
    
    1. **çµ±è¨ˆè¡¨**ï¼šè«‹é–å®š `å¯¦äº¤æ•¸é‡` æ¬„ä½ã€‚æŠ„éŒ„æ¯ä¸€è¡Œçš„ã€Œåç¨±ã€èˆ‡ã€Œå¯¦äº¤æ•¸é‡ã€åˆ° `summary_rows`ã€‚
    2. **é ç¢¼æ¨™è¨»**ï¼šè«‹å‹™å¿…åœ¨æ¯å€‹ `summary_rows` ç‰©ä»¶ä¸­è¨˜éŒ„è©²è¡Œæ‰€åœ¨çš„é ç¢¼ (`page`)ã€‚

    ---

    ### ğŸ“ è¼¸å‡ºè¦ç¯„ (Output Format)
    å¿…é ˆå›å‚³å–®ä¸€ JSONã€‚

    {{
      "job_no": "å·¥ä»¤",
      "summary_rows": [ 
          {{ "page": é ç¢¼, "title": "å", "target": æ•¸å­— }} 
      ],
      "freight_target": 0, 
      "issues": [], 
      "dimension_data": [
         {{
           "page": æ•¸å­—, "item_title": "æ¨™é¡Œ", "category": null, 
           "item_pc_target": 0,
           "accounting_rules": {{ "local": "", "agg": "", "freight": "" }},
           "sl": {{ "lt": "null", "t": 0 }},
           "std_spec": "åŸå§‹è¦æ ¼æ–‡å­—",
           "ds": "ID:å€¼|ID:å€¼" 
         }}
      ]
    }}
    """

    # 2. åˆ¤æ–·è¦ä½¿ç”¨å“ªä¸€é¡†å¼•æ“
    raw_content = ""
    
    # --- å¼•æ“ A: OpenAI GPT ç³»åˆ— ---
    if "gpt" in model_name.lower():
        try:
            # å¿…é ˆä½¿ç”¨å…¨åŸŸè®Šæ•¸ OPENAI_KEYï¼Œå› ç‚ºå‚³å…¥çš„ api_key åƒæ•¸é€šå¸¸æ˜¯ GEMINI_KEY
            openai_key = st.secrets.get("OPENAI_KEY", "")
            if not openai_key:
                return {"job_no": "Error: ç¼ºå°‘ OPENAI_KEY", "issues": [], "dimension_data": []}
                
            client = OpenAI(api_key=openai_key)
            
            response = client.chat.completions.create(
                model=model_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": combined_input}
                ],
                temperature=0.0,
                response_format={"type": "json_object"} # GPT-4o æ”¯æ´å¼·åˆ¶ JSON æ¨¡å¼
            )
            raw_content = response.choices[0].message.content
            
            # æ¨¡æ“¬ Token ç”¨é‡ (OpenAI æ ¼å¼ä¸åŒï¼Œé€™è£¡åšå€‹ç°¡å–®è½‰æ›ä»¥ä¾¿çµ±ä¸€é¡¯ç¤º)
            input_tokens = response.usage.prompt_tokens
            output_tokens = response.usage.completion_tokens
            
        except Exception as e:
            return {"job_no": f"OpenAI Error: {str(e)}", "issues": [], "dimension_data": []}

    # --- å¼•æ“ B: Google Gemini ç³»åˆ— ---
    else:
        try:
            genai.configure(api_key=api_key) # é€™è£¡ç”¨å‚³å…¥çš„ GEMINI_KEY
            generation_config = {"response_mime_type": "application/json", "temperature": 0.0}
            model = genai.GenerativeModel(model_name)
            
            # Gemini 2.0 å¯èƒ½éœ€è¦ä¸åŒçš„å‘¼å«æ–¹å¼ï¼Œé€™è£¡ä¿æŒé€šç”¨æ¥å£
            response = model.generate_content([system_prompt, combined_input], generation_config=generation_config)
            raw_content = response.text
            
            input_tokens = response.usage_metadata.prompt_token_count
            output_tokens = response.usage_metadata.candidates_token_count
            
        except Exception as e:
            return {"job_no": f"Gemini Error: {str(e)}", "issues": [], "dimension_data": []}

    # 3. çµ±ä¸€è§£æèˆ‡å›å‚³
    try:
        # ğŸ›¡ï¸ è¶…ç´šè§£æå™¨ï¼šé˜²æ­¢ AI è¼¸å‡ºå¸¶æœ‰ Markdown æ¨™ç±¤æˆ–å»¢è©±
        import re
        json_match = re.search(r"\{.*\}", raw_content, re.DOTALL)
        if json_match:
            raw_content = json_match.group()
            
        parsed_data = json.loads(raw_content)
        
        # çµ±ä¸€ Token ç”¨é‡æ ¼å¼
        parsed_data["_token_usage"] = {
            "input": input_tokens, 
            "output": output_tokens
        }
        return parsed_data

    except Exception as e:
        return {"job_no": f"JSON Parsing Error: {str(e)}", "issues": [], "dimension_data": []}

# --- é‡é»ï¼šPython å¼•æ“ç¨ç«‹æ–¼ agent å‡½å¼ä¹‹å¤– ---

def python_numerical_audit(dimension_data):
    grouped_errors = {}
    import re
    if not dimension_data: return []

    for item in dimension_data:
        # 1. å–å¾—æ•¸æ“š
        ds = str(item.get("ds", ""))
        if not ds: continue
        raw_entries = [p.split(":") for p in ds.split("|") if ":" in p]
        
        # ğŸ§½ å¼·åˆ¶æ¸…æ´—æ¨™é¡Œèˆ‡åˆ†é¡
        title = str(item.get("item_title", "")).replace(" ", "").replace("\n", "").replace('"', "")
        cat = str(item.get("category", "")).replace(" ", "").strip()
        
        page_num = item.get("page", "?")
        raw_spec = str(item.get("std_spec", "")).replace('"', "")
        
        # 2. ğŸ›¡ï¸ æ•¸æ“šæ¸…æ´—
        all_nums = [float(n) for n in re.findall(r"[-+]?\d+\.?\d*", raw_spec.replace(" ", ""))]
        noise = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 0.0] 
        clean_std = [n for n in all_nums if (n not in noise and n > 10)]

        # 3. ğŸ’¡ å¤šé‡å€é–“è‡ªå‹•é ç®—
        s_ranges = []
        spec_parts = re.split(r"[ä¸€äºŒä¸‰å››äº”å…­]|[;ï¼›]", raw_spec)
        
        for part in spec_parts:
            # âš¡ï¸ ä¿®æ­£é»ï¼šç§»é™¤ "mm" èˆ‡ "MM"ï¼Œè®“ "135mm~129mm" è®Šæˆ "135~129"
            clean_part = part.replace(" ", "").replace("\n", "").replace("mm", "").replace("MM", "").strip()
            if not clean_part: continue
            
            # é‚è¼¯ Aï¼šå„ªå…ˆè™•ç† Â± (å¦‚ 300Â±0.1)
            pm_match = re.search(r"(\d+\.?\d*)?Â±(\d+\.?\d*)", clean_part)
            if pm_match:
                b = float(pm_match.group(1)) if pm_match.group(1) else 0.0
                o = float(pm_match.group(2))
                s_ranges.append([round(b - o, 4), round(b + o, 4)])
                continue

            # é‚è¼¯ Bï¼šè™•ç†æ³¢æµªè™Ÿå€é–“ (å¦‚ 135~129)
            # ç¾åœ¨ç§»é™¤äº† mmï¼Œé€™è£¡å°±èƒ½æˆåŠŸæŠ“åˆ° [129, 135] äº†ï¼
            tilde_match = re.search(r"(\d+\.?\d*)[~ï½-](\d+\.?\d*)", clean_part)
            if tilde_match:
                n1, n2 = float(tilde_match.group(1)), float(tilde_match.group(2))
                # é˜²å‘†ï¼šé¿å…æŠŠ 160-0.01 (å…¬å·®) èª¤åˆ¤ç‚º 160~0.01 (å€é–“)
                if abs(n1 - n2) < n1 * 0.5: 
                    s_ranges.append([round(min(n1, n2), 4), round(max(n1, n2), 4)])
                    continue

            # é‚è¼¯ Cï¼šæ™ºæ…§é…å° (è§£æ±º 140 -0.01, -0.03)
            all_tokens = re.findall(r"[-+]?\d+\.?\d*", clean_part)
            if not all_tokens: continue

            bases = []
            offsets = []
            for token in all_tokens:
                val = float(token)
                if val > 10.0: bases.append(val)
                elif abs(val) < 10.0: offsets.append(val)
            
            if bases:
                for b in bases:
                    if offsets:
                        endpoints = [round(b + o, 4) for o in offsets]
                        if len(endpoints) == 1: endpoints.append(b)
                        s_ranges.append([min(endpoints), max(endpoints)])
                    else:
                        s_ranges.append([b, b])

        # 4. ğŸ’¡ é ç®—åŸºæº–
        s_threshold = 0
        un_regen_target = None
        if cat in ["un_regen", "æœªå†ç”Ÿ"] or ("æœªå†ç”Ÿ" in (cat + title) and "è»¸é ¸" not in (cat + title)):
            cands = [n for n in clean_std if n >= 120.0]
            if cands: un_regen_target = max(cands)

        # --- 5. é–‹å§‹é€ä¸€åˆ¤å®š ---
        for entry in raw_entries:
            if len(entry) < 2: continue
            rid = str(entry[0]).strip().replace(" ", "")
            val_raw = str(entry[1]).strip().replace(" ", "")
            
            if not val_raw or val_raw in ["N/A", "nan", "M10"]: continue

            try:
                is_passed, reason, t_used, engine_label = True, "", "N/A", "æœªçŸ¥"

                if "[!]" in val_raw:
                    is_passed = False
                    reason = "ğŸ›‘æ•¸æ“šæå£(å£è»Œ)"
                    val_str = "[!]"
                    val = -999.0
                else:
                    v_m = re.findall(r"\d+\.?\d*", val_raw)
                    val_str = v_m[0] if v_m else val_raw
                    val = float(val_str)

                if val_str != "[!]":
                    is_two_dec = "." in val_str and len(val_str.split(".")[-1]) == 2
                    is_pure_int = "." not in val_str
                else:
                    is_two_dec, is_pure_int = True, True 

                if "min_limit" in cat or "éŠ²è£œ" in (cat + title):
                    engine_label = "éŠ²è£œ"
                    if not is_pure_int: is_passed, reason = False, "æ‡‰ç‚ºç´”æ•´æ•¸"
                    elif clean_std:
                        t_used = min(clean_std, key=lambda x: abs(x - val))
                        if val < t_used: is_passed, reason = False, "æ•¸å€¼ä¸è¶³"
                
                elif un_regen_target is not None:
                    engine_label = "æœªå†ç”Ÿ"
                    t_used = un_regen_target
                    if val <= t_used:
                        if not is_pure_int: is_passed, reason = False, "æ‡‰ç‚ºæ•´æ•¸"
                    elif not is_two_dec: 
                        is_passed, reason = False, "æ‡‰å¡«å…©ä½å°æ•¸"

                elif "max_limit" in cat or (("è»¸é ¸" in (cat + title)) and ("æœªå†ç”Ÿ" in (cat + title))):
                    engine_label = "è»¸é ¸(ä¸Šé™)"
                    candidates = clean_std
                    target = max(candidates) if candidates else 0
                    t_used = target
                    if target > 0:
                        if not is_pure_int: is_passed, reason = False, "æ‡‰ç‚ºç´”æ•´æ•¸"
                        elif val > target: is_passed, reason = False, f"è¶…éä¸Šé™ {target}"

                elif any(x in (cat + title) for x in ["å†ç”Ÿ", "ç²¾åŠ å·¥", "ç ”ç£¨", "è»Šä¿®", "çµ„è£", "æ‹†è£", "çœŸåœ“åº¦"]) and "æœªå†ç”Ÿ" not in (cat + title):
                    engine_label = "ç²¾åŠ å·¥"
                    if not is_two_dec:
                        is_passed, reason = False, "æ‡‰å¡«å…©ä½å°æ•¸"
                    elif s_ranges:
                        t_used = str(s_ranges)
                        # ğŸ’¡ æ ¸å¿ƒï¼šåªè¦ç¬¦åˆä»»ä½•ä¸€å€‹è§£æå‡ºçš„å€é–“å°±ç®—åˆæ ¼
                        if not any(r[0] <= val <= r[1] for r in s_ranges): 
                            is_passed, reason = False, "ä¸åœ¨å€é–“å…§"

                if not is_passed:
                    key = (page_num, title, reason)
                    if key not in grouped_errors:
                        grouped_errors[key] = {
                            "page": page_num, "item": title, 
                            "issue_type": f"ç•°å¸¸({engine_label})", 
                            "common_reason": reason, "failures": [],
                            "source": "ğŸ å·¥ç¨‹å¼•æ“"
                        }
                    grouped_errors[key]["failures"].append({"id": rid, "val": val_str, "target": f"åŸºæº–:{t_used}"})
            except: continue
                
    return list(grouped_errors.values())
    
def python_accounting_audit(dimension_data, res_main):
    """
    Python æœƒè¨ˆå®˜ (v13: å…¨æ–¹ä½äº’æ–¥é–ç‰ˆ)
    1. [å‹•ä½œäº’æ–¥å‡ç´š]: å»ºç«‹ã€Œæœªå†ç”Ÿ vs å†ç”Ÿ vs éŠ²è£œã€ä¸‰æ–¹äº’æ–¥æ©Ÿåˆ¶ã€‚
       - åªè¦ç¸½è¡¨ç±ƒå­å±¬æ€§æ˜ç¢ºï¼Œå°±æœƒå¼·åˆ¶è¸¢é™¤å¦å¤–å…©ç¨®å±¬æ€§çš„é …ç›® (ä¾‹å¦‚: ç¸½è¡¨æœªå†ç”Ÿ -> æ“‹ä½ éŠ²è£œ & å†ç”Ÿ)ã€‚
    2. [éƒ¨ä½äº’æ–¥æ–°å¢]: å»ºç«‹ã€Œæœ¬é«” vs è»¸é ¸ã€äº’æ–¥æ©Ÿåˆ¶ã€‚
       - è‹¥ç¸½è¡¨æŒ‡å®šæœ¬é«”ï¼Œæ“‹ä½è»¸é ¸é …ç›®ï¼›è‹¥ç¸½è¡¨æŒ‡å®šè»¸é ¸ï¼Œæ“‹ä½æœ¬é«”é …ç›®ã€‚
    3. ä¿ç•™ v12 çš„åŸå§‹æ•¸é‡è¨ˆç®— (Raw Count) èˆ‡é›™å‘æ›ç®—é‚è¼¯ã€‚
    """
    accounting_issues = []
    from thefuzz import fuzz
    from collections import Counter
    import re
    import pandas as pd 

    # 0. åŸºç¤å·¥å…·
    def clean_text(text):
        return str(text).replace(" ", "").replace("\n", "").replace("\r", "").replace('"', '').replace("'", "").strip()

    def safe_float(value):
        if value is None or str(value).upper() == 'NULL': return 0.0
        if "[!]" in str(value): return "BAD_DATA" 
        cleaned = "".join(re.findall(r"[\d\.]+", str(value).replace(',', '')))
        try: return float(cleaned) if cleaned else 0.0
        except: return 0.0

    # 1. è¼‰å…¥è¦å‰‡
    rules_map = {}
    try:
        df = pd.read_excel("rules.xlsx")
        df.columns = [c.strip() for c in df.columns]
        for _, row in df.iterrows():
            iname = str(row.get('Item_Name', '')).strip()
            if iname: 
                rules_map[clean_text(iname)] = {
                    "u_local": str(row.get('Unit_Rule_Local', '')).strip(),
                    "u_fr": str(row.get('Unit_Rule_Freight', '')).strip(),
                    "u_agg": str(row.get('Unit_Rule_Agg', '')).strip()
                }
    except: pass 

    summary_rows = res_main.get("summary_rows", [])
    global_sum_tracker = {
        s['title']: {
            "target": safe_float(s['target']), 
            "actual": 0, 
            "details": [], 
            "page": s.get('page', "ç¸½è¡¨")
        } 
        for s in summary_rows if s.get('title')
    }
    
    freight_target = safe_float(res_main.get("freight_target", 0))
    freight_actual_sum = 0
    freight_details = []

    # 2. é€é …éå¸³
    for item in dimension_data:
        raw_title = item.get("item_title", "")
        title_clean = clean_text(raw_title) 
        page = item.get("page", "?")
        target_pc = safe_float(item.get("item_pc_target", 0)) 
        
        # æŸ¥æ‰¾è¦å‰‡
        rule_set = rules_map.get(title_clean)
        if not rule_set:
            t_no = re.sub(r"[\(ï¼ˆ].*?[\)ï¼‰]", "", title_clean)
            rule_set = rules_map.get(t_no)
        if not rule_set and rules_map:
            best_score = 0
            for k, v in rules_map.items():
                sc = fuzz.partial_ratio(k, title_clean)
                if sc > 85 and sc > best_score:
                    best_score = sc
                    rule_set = v
        
        u_local = rule_set.get("u_local", "") if rule_set else ""
        u_fr = rule_set.get("u_fr", "") if rule_set else ""
        u_agg = rule_set.get("u_agg", "") if rule_set else ""

        u_local_norm = u_local.upper().replace(" ", "").replace("ã€€", "").replace("ï¼", "=").replace("ï¼š", "=").replace(":", "=")
        u_fr_norm = u_fr.upper().replace(" ", "").replace("ã€€", "").replace("ï¼", "=").replace("ï¼š", "=").replace(":", "=")

        ds = str(item.get("ds", ""))
        data_list = [pair.split(":") for pair in ds.split("|") if ":" in pair]
        
        # å®šç¾©åŸå§‹æ•¸é‡ (Raw Count)
        if not data_list: 
            raw_count = 0
        else:
            ids = [str(e[0]).strip() for e in data_list if len(e) > 0]
            if "PC=PC" in u_local_norm or "æœ¬é«”" in title_clean:
                raw_count = len(set(ids))
            else:
                raw_count = len(data_list)
        
        id_counts = Counter([str(e[0]).strip() for e in data_list if len(e)>0])

        # === 2.1 å–®é …æ•¸é‡ (Local) ===
        is_local_exempt = "è±å…" in u_local
        is_weight_mode = "KG" in title_clean.upper() or target_pc > 100
        
        actual_item_qty = 0
        
        if is_weight_mode:
            current_sum = 0
            has_bad = False
            for e in data_list:
                tv = safe_float(e[1])
                if tv == "BAD_DATA": has_bad = True
                else: current_sum += tv
            actual_item_qty = current_sum
            if has_bad and not is_local_exempt:
                accounting_issues.append({"page": page, "item": raw_title, "issue_type": "âš ï¸è³‡æ–™ç•°å¸¸", "common_reason": "å«ç„¡æ³•è¾¨è­˜æ•¸å€¼", "failures": [{"id": "è­¦å‘Š", "val": "[!]", "calc": "ç•°å¸¸"}]})
        else:
            conv_match = re.search(r"(\d+\.?\d*)[^\d=]*=[^\d=]*(\d+\.?\d*)", u_local_norm)
            if conv_match:
                l, r = float(conv_match.group(1)), float(conv_match.group(2))
                div = max(l, r) 
                actual_item_qty = raw_count / (div if div != 0 else 1)
            else:
                actual_item_qty = raw_count

        if not is_local_exempt and abs(actual_item_qty - target_pc) > 0.01 and target_pc > 0:
            accounting_issues.append({"page": page, "item": raw_title, "issue_type": "çµ±è¨ˆä¸ç¬¦(å–®é …)", "common_reason": f"æ¨™é¡Œ {target_pc}PC != å…§æ–‡ {actual_item_qty}", "failures": [{"id": "ç›®æ¨™", "val": target_pc}, {"id": "å¯¦éš›", "val": actual_item_qty}], "source": "ğŸ æœƒè¨ˆå¼•æ“"})

        # 2.2 é‡è¤‡è­¦ç¤º
        if "æœ¬é«”" in title_clean:
             for rid, count in id_counts.items():
                if count > 1: accounting_issues.append({"page": page, "item": raw_title, "issue_type": "âš ï¸ç·¨è™Ÿé‡è¤‡(æœ¬é«”)", "common_reason": f"{rid} é‡è¤‡ {count}æ¬¡", "failures": []})
        elif any(k in title_clean for k in ["è»¸é ¸", "å…§å­”", "JOURNAL"]):
             for rid, count in id_counts.items():
                if count > 2: accounting_issues.append({"page": page, "item": raw_title, "issue_type": "âš ï¸ç·¨è™Ÿé‡è¤‡(è»¸é ¸)", "common_reason": f"{rid} é‡è¤‡ {count}æ¬¡", "failures": []})

        # 2.3 é‹è²»è¨ˆç®—
        is_fr_exempt = "è±å…" in u_fr
        fr_conv_match = re.search(r"(\d+\.?\d*)[^\d=]*=[^\d=]*1", u_fr_norm)
        is_default_target = "æœ¬é«”" in title_clean and ("æœªå†ç”Ÿ" in title_clean or "ç²—è»Š" in title_clean)
        freight_val = 0.0
        f_note = ""

        if is_fr_exempt: freight_val = 0.0
        elif fr_conv_match:
            div = float(fr_conv_match.group(1))
            freight_val = raw_count / div
            f_note = f"è¨ˆå…¥ (/{int(div)})"
        elif is_default_target:
            freight_val = actual_item_qty 
            f_note = "è¨ˆå…¥"
            
        if freight_val > 0:
            freight_actual_sum += freight_val
            freight_details.append({"id": f"{raw_title}", "val": freight_val, "calc": f_note})

        # === 2.4 ç¸½è¡¨å°å¸³ (Agg) - v13 å…¨æ–¹ä½äº’æ–¥é– ===
        agg_mode = "B" 
        agg_divisor = 1.0
        has_agg_math = False
        
        if u_agg:
            parts = str(u_agg).upper().split(",")
            for p in parts:
                p_clean = p.replace(" ", "")
                if "è±å…" in p_clean or "EXEMPT" in p_clean: agg_mode = "EXEMPT"
                elif p_clean == "AB": agg_mode = "AB"
                elif p_clean == "A": agg_mode = "A"
                elif p_clean == "B": agg_mode = "B"
                elif "=" in p_clean:
                    match = re.search(r"(\d+\.?\d*)[^\d=]*=[^\d=]*(\d+\.?\d*)", p_clean)
                    if match:
                        l, r = float(match.group(1)), float(match.group(2))
                        if l > 0: 
                            agg_divisor = l / r
                            has_agg_math = True

        if agg_mode == "EXEMPT": continue 
        
        # æ•¸é‡è¨ˆç®—åˆ†æµ
        if has_agg_math:
            qty_agg = raw_count / agg_divisor
        else:
            qty_agg = actual_item_qty
            if agg_divisor == 1.0: agg_divisor = 1.0

        for s_title, data in global_sum_tracker.items():
            match = False
            s_clean = clean_text(s_title)
            
            if "é‹è²»" in s_clean:
                if freight_val > 0:
                    data["actual"] += freight_val
                    data["details"].append({"id": raw_title, "val": freight_val, "calc": f_note})
                continue 
            
            # --- æ¨¡å¼åŒ¹é… ---
            match_A = (fuzz.partial_ratio(s_clean, title_clean) > 95)
            
            match_B = False
            is_dis = "ROLLæ‹†è£" in s_clean
            is_mac = "ROLLè»Šä¿®" in s_clean
            is_weld = "ROLLéŠ²è£œ" in s_clean or "ROLLç„Šè£œ" in s_clean
            
            has_part = "æœ¬é«”" in title_clean or any(k in title_clean for k in ["è»¸é ¸", "JOURNAL"])
            has_act_mac = any(k in title_clean for k in ["å†ç”Ÿ", "ç²¾è»Š", "æœªå†ç”Ÿ", "ç²—è»Š"])
            has_act_weld = ("éŠ²è£œ" in title_clean or "ç„Š" in title_clean)
            is_assy = ("çµ„è£" in title_clean or "æ‹†è£" in title_clean)

            if is_dis and is_assy: match_B = True
            elif is_mac and has_part and has_act_mac: match_B = True
            elif is_weld and has_part and has_act_weld: match_B = True
            
            if agg_mode == "A": initial_match = match_A
            elif agg_mode == "AB": initial_match = match_A or match_B
            else: initial_match = match_B if match_B else match_A

            # --- âš¡ï¸ å…¨æ–¹ä½äº’æ–¥é– (Conflict Check) ---
            final_match = initial_match
            if final_match:
                # 1. æå–å‹•ä½œå±¬æ€§
                sum_unregen = "æœªå†ç”Ÿ" in s_clean or "ç²—è»Š" in s_clean
                sum_regen = ("å†ç”Ÿ" in s_clean or "ç²¾è»Š" in s_clean) and not sum_unregen
                sum_weld = "éŠ²è£œ" in s_clean or "ç„Š" in s_clean
                
                item_unregen = "æœªå†ç”Ÿ" in title_clean or "ç²—è»Š" in title_clean
                item_regen = ("å†ç”Ÿ" in title_clean or "ç²¾è»Š" in title_clean) and not item_unregen
                item_weld = "éŠ²è£œ" in title_clean or "ç„Š" in title_clean

                # 2. æå–éƒ¨ä½å±¬æ€§
                sum_body = "æœ¬é«”" in s_clean
                sum_journal = any(k in s_clean for k in ["è»¸é ¸", "å…§å­”", "JOURNAL"])
                
                item_body = "æœ¬é«”" in title_clean
                item_journal = any(k in title_clean for k in ["è»¸é ¸", "å…§å­”", "JOURNAL"])

                # --- å‹•ä½œäº’æ–¥è¦å‰‡ (Action Conflict) ---
                # A. ç¸½è¡¨æ˜¯éŠ²è£œ -> è¸¢é™¤ (æœªå†ç”Ÿ OR å†ç”Ÿ) â€»é™¤éé …ç›®æœ¬èº«ä¹Ÿæ˜¯éŠ²è£œ
                if sum_weld and (item_unregen or item_regen) and not item_weld: final_match = False
                
                # B. ç¸½è¡¨æ˜¯æœªå†ç”Ÿ -> è¸¢é™¤ (å†ç”Ÿ OR éŠ²è£œ)
                elif sum_unregen and (item_regen or item_weld): final_match = False
                
                # C. ç¸½è¡¨æ˜¯å†ç”Ÿ -> è¸¢é™¤ (æœªå†ç”Ÿ OR éŠ²è£œ)
                elif sum_regen and (item_unregen or item_weld): final_match = False

                # --- éƒ¨ä½äº’æ–¥è¦å‰‡ (Part Conflict) ---
                # D. ç¸½è¡¨æŒ‡å®šæœ¬é«” (ä¸”æ²’å¯«è»¸é ¸) -> è¸¢é™¤ è»¸é ¸é …ç›®
                if sum_body and not sum_journal and item_journal: final_match = False

                # E. ç¸½è¡¨æŒ‡å®šè»¸é ¸ (ä¸”æ²’å¯«æœ¬é«”) -> è¸¢é™¤ æœ¬é«”é …ç›®
                if sum_journal and not sum_body and item_body: final_match = False

            # --- æœ€çµ‚ç¢ºèª ---
            if final_match:
                data["actual"] += qty_agg
                c_msg = f"è¨ˆå…¥ (/{agg_divisor:.1f})" if agg_divisor != 1.0 else "è¨ˆå…¥"
                data["details"].append({"id": f"{raw_title} (P.{page})", "val": qty_agg, "calc": c_msg})

    # 3. ç•°å¸¸çµç®—
    for s_title, data in global_sum_tracker.items():
        if abs(data["actual"] - data["target"]) > 0.01 and data["target"] > 0:
            accounting_issues.append({
                "page": data["page"], "item": s_title, 
                "issue_type": "çµ±è¨ˆä¸ç¬¦(ç¸½å¸³)", 
                "common_reason": f"æ¨™è¨» {data['target']} != å¯¦éš› {data['actual']}", 
                "failures": [{"id": "ğŸ” åŸºæº–", "val": data["target"]}] + data["details"] + [{"id": "ğŸ§® å¯¦éš›", "val": data["actual"]}], 
                "source": "ğŸ æœƒè¨ˆå¼•æ“"
            })

    if abs(freight_actual_sum - freight_target) > 0.01 and freight_target > 0:
        accounting_issues.append({
            "page": "ç¸½è¡¨", "item": "é‹è²»æ ¸å°", 
            "issue_type": "çµ±è¨ˆä¸ç¬¦(é‹è²»)", 
            "common_reason": f"åŸºæº– {freight_target} != å¯¦éš› {freight_actual_sum}", 
            "failures": [{"id": "ğŸšš åŸºæº–", "val": freight_target}] + freight_details + [{"id": "ğŸ§® å¯¦éš›", "val": freight_actual_sum}], 
            "source": "ğŸ æœƒè¨ˆå¼•æ“"
        })
        
    return accounting_issues

def python_process_audit(dimension_data):
    """
    Python æµç¨‹å¼•æ“ (v3: å®Œæ•´æ”¯æ´ Process_Rule)
    1. æ”¯æ´å¾ Excel è®€å– 'Process_Rule' æ¬„ä½ã€‚
    2. è§£æé—œéµå­—ï¼š'æœ¬é«”/è»¸é ¸' æ±ºå®šè»Œé“, 'æœªå†ç”Ÿ/éŠ²è£œ/å†ç”Ÿ/ç ”ç£¨' æ±ºå®šå·¥åºã€‚
    3. å„ªå…ˆæ¬Šï¼šExcel è¦å‰‡ > æ¨™é¡Œé—œéµå­—ã€‚
    """
    process_issues = []
    import re
    import pandas as pd
    from thefuzz import fuzz

    # 0. æ¸…æ´—å·¥å…·
    def clean_text(text):
        return str(text).replace(" ", "").replace("\n", "").replace("\r", "").replace('"', '').replace("'", "").strip()

    # 1. é è¼‰ Excel Process_Rule
    rules_map = {}
    try:
        df = pd.read_excel("rules.xlsx")
        df.columns = [c.strip() for c in df.columns]
        for _, row in df.iterrows():
            iname = str(row.get('Item_Name', '')).strip()
            p_rule = str(row.get('Process_Rule', '')).strip() # è®€å– Process_Rule
            if iname and p_rule and p_rule.lower() != 'nan':
                rules_map[clean_text(iname)] = p_rule
    except: pass

    # å®šç¾©å·¥åºèˆ‡åç¨±
    STAGE_MAP = {
        1: "æœªå†ç”Ÿ/ç²—è»Š",
        2: "éŠ²è£œ/ç„Šè£œ",
        3: "å†ç”Ÿ/ç²¾è»Š",
        4: "ç ”ç£¨"
    }

    history = {} 

    if not dimension_data: return []

    for item in dimension_data:
        p_num = item.get("page", "?")
        title = str(item.get("item_title", "")).strip()
        title_clean = clean_text(title)
        ds = str(item.get("ds", ""))
        
        # --- A. æ±ºå®š Track èˆ‡ Stage ---
        track = "Unknown"
        stage = 0
        forced_rule = None

        # A-1. æŸ¥è¡¨
        if rules_map:
            # æ¨¡ç³ŠåŒ¹é…é‚è¼¯ (åŒåˆ†æ±ºå‹è² )
            best_score = 0
            best_len = 999
            
            # å˜—è©¦ç›´æ¥åŒ¹é…
            match = rules_map.get(title_clean)
            if match: forced_rule = match
            
            # å˜—è©¦è„«æ®¼åŒ¹é…
            if not forced_rule:
                t_no = re.sub(r"[\(ï¼ˆ].*?[\)ï¼‰]", "", title_clean)
                match = rules_map.get(t_no)
                if match: forced_rule = match

            # å˜—è©¦ Fuzzy åŒ¹é…
            if not forced_rule:
                for k, v in rules_map.items():
                    sc = fuzz.partial_ratio(k, title_clean)
                    ld = abs(len(k) - len(title_clean))
                    if sc > 85:
                        if sc > best_score:
                            best_score = sc
                            best_len = ld
                            forced_rule = v
                        elif sc == best_score and ld < best_len:
                            best_len = ld
                            forced_rule = v

        # A-2. è§£æå¼·åˆ¶è¦å‰‡
        if forced_rule:
            fr = forced_rule.upper()
            if "è±å…" in fr or "EXEMPT" in fr: continue # ğŸš€ è±å…

            # è§£æè»Œé“ (è‹¥å¯«äº† "è»¸é ¸éŠ²è£œ"ï¼Œé€™è£¡æœƒæŠ“åˆ° "è»¸é ¸")
            if "æœ¬é«”" in fr: track = "æœ¬é«”"
            elif "è»¸é ¸" in fr: track = "è»¸é ¸"
            
            # è§£æå·¥åº (è‹¥å¯«äº† "è»¸é ¸éŠ²è£œ"ï¼Œé€™è£¡æœƒæŠ“åˆ° "éŠ²")
            if "æœªå†ç”Ÿ" in fr or "ç²—è»Š" in fr: stage = 1
            elif "éŠ²" in fr or "ç„Š" in fr: stage = 2
            elif "å†ç”Ÿ" in fr or "ç²¾è»Š" in fr: stage = 3
            elif "ç ”ç£¨" in fr: stage = 4

        # A-3. Fallback: æ¨™é¡Œé—œéµå­—è‡ªå‹•åˆ¤æ–·
        if track == "Unknown":
            if "æœ¬é«”" in title: track = "æœ¬é«”"
            elif any(k in title for k in ["è»¸é ¸", "å…§å­”", "JOURNAL"]): track = "è»¸é ¸"
        
        if stage == 0:
            if "ç ”ç£¨" in title: stage = 4
            elif any(k in title for k in ["éŠ²è£œ", "éŠ²æ¥", "ç„Š"]): stage = 2
            elif "æœªå†ç”Ÿ" in title or "ç²—è»Š" in title: stage = 1
            elif "å†ç”Ÿ" in title or "ç²¾è»Š" in title: stage = 3
        
        if track == "Unknown" or stage == 0: continue 

        # --- B. æ•¸æ“šè§£æ (ç¶­æŒä¸è®Š) ---
        segments = ds.split("|")
        for seg in segments:
            parts = seg.split(":")
            if len(parts) < 2: continue
            rid = parts[0].strip()
            val_str = parts[1].strip()
            nums = re.findall(r"\d+\.?\d*", val_str)
            if not nums: continue
            val = float(nums[0])
            
            key = (rid, track)
            if key not in history: history[key] = {}
            history[key][stage] = {
                "val": val, "page": p_num, "title": title
            }

    # 2. åŸ·è¡Œæ ¸å¿ƒé‚è¼¯æª¢æŸ¥
    for (rid, track), stages_data in history.items():
        present_stages = sorted(stages_data.keys())
        if not present_stages: continue
        max_stage = present_stages[-1]
        
        missing_stages = []
        for req_s in range(1, max_stage):
            if req_s not in stages_data: missing_stages.append(STAGE_MAP[req_s])
        
        if missing_stages:
            last_info = stages_data[max_stage]
            process_issues.append({
                "page": last_info['page'],
                "item": f"{last_info['title']}",
                "issue_type": "ğŸ›‘æº¯æºç•°å¸¸(ç¼ºæ¼å·¥åº)",
                "common_reason": f"[{track}] é€²åº¦è‡³ã€{STAGE_MAP[max_stage]}ã€‘ï¼Œç¼ºå‰ç½®ï¼š{', '.join(missing_stages)}",
                "failures": [{"id": rid, "val": "ç¼ºæ¼", "calc": "å±¥æ­·ä¸å®Œæ•´"}],
                "source": "ğŸ æµç¨‹å¼•æ“"
            })

        size_rank = { 1: 10, 4: 20, 3: 30, 2: 40 }
        for i in range(len(present_stages)):
            for j in range(i + 1, len(present_stages)):
                s_a = present_stages[i]
                s_b = present_stages[j]
                info_a = stages_data[s_a]
                info_b = stages_data[s_b]
                
                expect_a_smaller = size_rank[s_a] < size_rank[s_b]
                is_violation = False
                if expect_a_smaller:
                    if info_a['val'] >= info_b['val']: is_violation = True
                else:
                    if info_a['val'] <= info_b['val']: is_violation = True
                    
                if is_violation:
                    sign = "<" if expect_a_smaller else ">"
                    process_issues.append({
                        "page": info_b['page'],
                        "item": f"[{track}] å°ºå¯¸é‚è¼¯",
                        "issue_type": "ğŸ›‘æµç¨‹ç•°å¸¸(å°ºå¯¸å€’ç½®)",
                        "common_reason": f"å°ºå¯¸é‚è¼¯éŒ¯èª¤ï¼š{STAGE_MAP[s_a]} æ‡‰ {sign} {STAGE_MAP[s_b]}",
                        "failures": [{"id": STAGE_MAP[s_a], "val": info_a['val'], "calc": "å‰"}, {"id": STAGE_MAP[s_b], "val": info_b['val'], "calc": "å¾Œ"}],
                        "source": "ğŸ æµç¨‹å¼•æ“"
                    })

    return process_issues

# --- 6. æ‰‹æ©Ÿç‰ˆ UI èˆ‡ æ ¸å¿ƒåŸ·è¡Œé‚è¼¯ ---
st.title("ğŸ­ äº¤è²¨å–®ç¨½æ ¸")

data_source = st.radio(
    "è«‹é¸æ“‡è³‡æ–™ä¾†æºï¼š", 
    ["ğŸ“¸ ä¸Šå‚³ç…§ç‰‡", "ğŸ“‚ ä¸Šå‚³ JSON æª”", "ğŸ“Š ä¸Šå‚³ Excel æª”"], 
    horizontal=True
)

with st.container(border=True):
    # --- æƒ…æ³ A: ä¸Šå‚³ç…§ç‰‡ ---
    if data_source == "ğŸ“¸ ä¸Šå‚³ç…§ç‰‡":
        if st.session_state.get('source_mode') == 'json' or st.session_state.get('source_mode') == 'excel':
            st.session_state.photo_gallery = []
            st.session_state.source_mode = 'image'

        uploaded_files = st.file_uploader(
            "è«‹é¸æ“‡ JPG/PNG ç…§ç‰‡...", 
            type=['jpg', 'png', 'jpeg'], 
            accept_multiple_files=True, 
            key=f"uploader_{st.session_state.uploader_key}"
        )
        
        if uploaded_files:
            for f in uploaded_files: 
                if not any(x['file'].name == f.name for x in st.session_state.photo_gallery if x['file']):
                    st.session_state.photo_gallery.append({
                        'file': f, 
                        'table_md': None, 
                        'header_text': None,
                        'full_text': None,
                        'raw_json': None
                    })
            st.session_state.uploader_key += 1
            if st.session_state.enable_auto_analysis:
                st.session_state.auto_start_analysis = True
            components.html("""<script>window.parent.document.body.scrollTo(0, window.parent.document.body.scrollHeight);</script>""", height=0)
            st.rerun()

    # --- æƒ…æ³ B: ä¸Šå‚³ JSON ---
    elif data_source == "ğŸ“‚ ä¸Šå‚³ JSON æª”":
        st.info("ğŸ’¡ è«‹é»æ“Šä¸‹æ–¹æŒ‰éˆ•ï¼Œå¾ä½ çš„è³‡æ–™å¤¾é¸æ“‡ä¹‹å‰ä¸‹è¼‰çš„ `.json` æª”ã€‚")
        uploaded_json = st.file_uploader("ä¸Šå‚³JSONæª”", type=['json'], key="json_uploader")
        
        if uploaded_json:
            try:
                current_file_name = uploaded_json.name
                if st.session_state.get('last_loaded_json_name') != current_file_name:
                    json_data = json.load(uploaded_json)
                    st.session_state.photo_gallery = []
                    st.session_state.source_mode = 'json'
                    st.session_state.last_loaded_json_name = current_file_name
                    
                    import re
                    for page in json_data:
                        real_page = "Unknown"
                        full_text = page.get('full_text', '')
                        if full_text:
                            match = re.search(r"(?:é …æ¬¡|Page|é æ¬¡|NO\.)[:\s]*(\d+)\s*[/ï¼]\s*\d+", full_text, re.IGNORECASE)
                            if match:
                                real_page = match.group(1)
                        
                        st.session_state.photo_gallery.append({
                            'file': None,
                            'table_md': page.get('table_md'),
                            'header_text': page.get('header_text'),
                            'full_text': full_text,
                            'raw_json': page.get('raw_json'),
                            'real_page': real_page
                        })
                    
                    st.toast(f"âœ… æˆåŠŸè¼‰å…¥ JSON: {current_file_name}", icon="ğŸ“‚")
                    if st.session_state.enable_auto_analysis:
                        st.session_state.auto_start_analysis = True
                    st.rerun()
                else:
                    st.success(f"ğŸ“‚ ç›®å‰è¼‰å…¥ JSONï¼š**{uploaded_json.name}**")
            except Exception as e:
                st.error(f"JSON æª”æ¡ˆæ ¼å¼éŒ¯èª¤: {e}")

    # --- æƒ…æ³ C: ä¸Šå‚³ Excel (æ–°å¢çš„æ”¾åœ¨é€™) ---
    elif data_source == "ğŸ“Š ä¸Šå‚³ Excel æª”":
        st.info("ğŸ’¡ ä¸Šå‚³ Excel æª”å¾Œï¼Œç³»çµ±æœƒå°‡è¡¨æ ¼å…§å®¹è½‰æ›ç‚ºæ–‡å­—ä¾› AI ç¨½æ ¸ã€‚")
        uploaded_xlsx = st.file_uploader("ä¸Šå‚³ Excel æª”", type=['xlsx', 'xls'], key="xlsx_uploader")
        
        if uploaded_xlsx:
            try:
                current_file_name = uploaded_xlsx.name
                if st.session_state.get('last_loaded_xlsx_name') != current_file_name:
                    df_dict = pd.read_excel(uploaded_xlsx, sheet_name=None)
                    st.session_state.photo_gallery = []
                    st.session_state.source_mode = 'excel'
                    st.session_state.last_loaded_xlsx_name = current_file_name
                    
                    for sheet_name, df in df_dict.items():
                        df = df.fillna("")
                        md_table = df.to_markdown(index=False)
                        st.session_state.photo_gallery.append({
                            'file': None,
                            'table_md': md_table,
                            'header_text': f"ä¾†æºåˆ†é : {sheet_name}",
                            'full_text': f"Excel å…§å®¹ - åˆ†é  {sheet_name}\n" + md_table,
                            'raw_json': None,
                            'real_page': sheet_name
                        })
                    st.toast(f"âœ… æˆåŠŸè¼‰å…¥ Excel: {current_file_name}", icon="ğŸ“Š")
                    if st.session_state.enable_auto_analysis:
                        st.session_state.auto_start_analysis = True
                    st.rerun()
                else:
                    st.success(f"ğŸ“Š ç›®å‰è¼‰å…¥ Excelï¼š**{uploaded_xlsx.name}**")
            except Exception as e:
                st.error(f"Excel è®€å–å¤±æ•—: {e}")

if st.session_state.photo_gallery:
    st.caption(f"å·²ç´¯ç© {len(st.session_state.photo_gallery)} é æ–‡ä»¶")
    col_btn1, col_btn2 = st.columns([1, 1], gap="small")
    with col_btn1: start_btn = st.button("ğŸš€ é–‹å§‹åˆ†æ", type="primary", use_container_width=True)
    with col_btn2: 
        clear_btn = st.button("ğŸ—‘ï¸ç…§ç‰‡æ¸…é™¤", help="æ¸…é™¤", use_container_width=True)

    if clear_btn:
        st.session_state.photo_gallery = []
        st.session_state.analysis_result_cache = None
        if 'last_loaded_json_name' in st.session_state:
            del st.session_state.last_loaded_json_name 
        st.rerun()

    is_auto_start = st.session_state.auto_start_analysis
    if is_auto_start:
        st.session_state.auto_start_analysis = False

    if 'analysis_result_cache' not in st.session_state:
        st.session_state.analysis_result_cache = None

    trigger_analysis = start_btn or is_auto_start

    if trigger_analysis:
        # âš¡ï¸ æ–°å¢é€™è¡Œï¼šå¼·åˆ¶æ¸…é™¤ä¸Šä¸€ç­†çš„çµæœ
        st.session_state.analysis_result_cache = None 
        
        st.session_state.auto_start_analysis = False
        total_start = time.time()
        
        with st.status("ç¸½ç¨½æ ¸å®˜æ­£åœ¨é€²è¡Œå…¨æ–¹ä½åˆ†æ...", expanded=True) as status_box:
            progress_bar = st.progress(0)
            
            # 1. OCR (é€™æ®µä¿ç•™ï¼Œé€Ÿåº¦å¾ˆå¿«)
            status_box.write("ğŸ‘€ æ­£åœ¨é€²è¡Œ OCR æ–‡å­—è­˜åˆ¥...")
            ocr_start = time.time()
            
            def process_task(index, item):
                if item.get('full_text'):
                    return index, item.get('header_text',''), item['full_text'], None
                try:
                    item['file'].seek(0)
                    _, h, f, _, _ = extract_layout_with_azure(item['file'], DOC_ENDPOINT, DOC_KEY)
                    return index, h, f, None
                except Exception as e:
                    return index, None, None, str(e)

            with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
                futures = [executor.submit(process_task, i, item) for i, item in enumerate(st.session_state.photo_gallery)]
                for future in concurrent.futures.as_completed(futures):
                    idx, h_txt, f_txt, err = future.result()
                    if not err:
                        st.session_state.photo_gallery[idx].update({'header_text': h_txt, 'full_text': f_txt, 'file': None})
                    progress_bar.progress(0.4 * ((idx + 1) / len(st.session_state.photo_gallery)))

            ocr_duration = time.time() - ocr_start
            
            # 2. çµ„åˆæ‰€æœ‰æ–‡å­— (é—œéµï¼šä¸€æ¬¡ä¸Ÿé€²å»)
            combined_input = ""
            for i, p in enumerate(st.session_state.photo_gallery):
                combined_input += f"\n=== Page {i+1} ===\n{p.get('full_text','')}\n"

            # 3. å‘¼å« AI (é€™è£¡åªæœƒè·‘ä¸€æ¬¡ï¼Œç´„ 20-30 ç§’)
            status_box.write("ğŸ¤– AI æ­£åœ¨å…¨å·åˆ†æ...")
            res_main = agent_unified_check(combined_input, combined_input, GEMINI_KEY, main_model_name)
            progress_bar.progress(0.8)
            
            # 4. Python é‚è¼¯æª¢æŸ¥
            status_box.write("ğŸ Python æ­£åœ¨é€²è¡Œé‚è¼¯æ¯”å°...")
            dim_data = res_main.get("dimension_data", [])
            
            # âš¡ï¸ [æ’å…¥é»]ï¼šPython å¥ªæ¬Šï¼å¼·åˆ¶è¦†å¯« AI çš„åˆ†é¡
            for item in dim_data:
                # å³ä½¿ AI æœ‰å¡« categoryï¼Œæˆ‘å€‘ä¹Ÿç”¨ Python çš„é‚è¼¯è¦†è“‹å®ƒï¼Œä¿è­‰ 100% ä¸€è‡´æ€§
                # æˆ–è€…ï¼Œå¦‚æœ AI æ²’å¡«ï¼Œé€™è£¡å°±æ˜¯è£œå¡«çš„é—œéµ
                new_cat = assign_category_by_python(item.get("item_title", ""))
                item["category"] = new_cat
                # é †ä¾¿æŠŠ category å¯«é€² rules ä¾›å‰ç«¯é¡¯ç¤º (é¸ç”¨)
                if "sl" not in item: item["sl"] = {}
                item["sl"]["lt"] = new_cat
            
            python_numeric_issues = python_numerical_audit(dim_data)
            python_accounting_issues = python_accounting_audit(dim_data, res_main)
            python_process_issues = python_process_audit(dim_data)
            python_header_issues, python_debug_data = python_header_check(st.session_state.photo_gallery)

            ai_filtered_issues = []
            ai_raw_issues = res_main.get("issues", [])
            if isinstance(ai_raw_issues, list):
                for i in ai_raw_issues:
                    if isinstance(i, dict):
                        i['source'] = 'ğŸ¤– ç¸½ç¨½æ ¸ AI'
                        if not any(k in i.get("issue_type", "") for k in ["æµç¨‹", "è¦æ ¼æå–å¤±æ•—", "æœªåŒ¹é…"]):
                            ai_filtered_issues.append(i)

            all_issues = ai_filtered_issues + python_numeric_issues + python_accounting_issues + python_process_issues + python_header_issues
            
            # 5. å­˜æª”èˆ‡å®Œæˆ
            usage = res_main.get("_token_usage", {"input": 0, "output": 0})
            
            # â­ï¸ [é—œéµä¿®æ­£] é€™è£¡å¿…é ˆæŠŠ freight_target å’Œ summary_rows å­˜é€²å»ï¼Œä¸ç„¶é¡¯ç¤ºæ™‚æœƒæŠ“ä¸åˆ°ï¼
            st.session_state.analysis_result_cache = {
                "job_no": res_main.get("job_no", "Unknown"),
                "all_issues": all_issues,
                "total_duration": time.time() - total_start,
                "cost_twd": (usage.get("input", 0)*0.3 + usage.get("output", 0)*2.5) / 1000000 * 32.5,
                "total_in": usage.get("input", 0),
                "total_out": usage.get("output", 0),
                "ocr_duration": ocr_duration,
                "time_eng": time.time() - total_start - ocr_duration,
                
                "ai_extracted_data": dim_data,
                "python_debug_data": python_debug_data,
                
                # ğŸ‘‡ é€™è£¡æ˜¯æˆ‘å¹«æ‚¨è£œä¸Šçš„ï¼Œç‚ºäº†æ–°çš„çœ‹æ¿åŠŸèƒ½
                "freight_target": res_main.get("freight_target", 0),
                "summary_rows": res_main.get("summary_rows", []),
                
                "full_text_for_search": combined_input,
                "combined_input": combined_input
            }
            
            progress_bar.progress(1.0)
            status_box.update(label="âœ… åˆ†æå®Œæˆï¼", state="complete", expanded=False)
            st.rerun()

            # --- ğŸ’¡ [é¡¯ç¤ºçµæœå€å¡Š] æ•¸é‡åŒæ­¥ä¿®æ­£ç‰ˆ ---
    if st.session_state.analysis_result_cache:
        cache = st.session_state.analysis_result_cache
        all_issues = cache.get('all_issues', [])
        
        # 1. é ‚éƒ¨ç‹€æ…‹æ¢
        st.success(f"å·¥ä»¤: {cache['job_no']} | â±ï¸ {cache['total_duration']:.1f}s")
        st.info(f"ğŸ’° æœ¬æ¬¡æˆæœ¬: NT$ {cache['cost_twd']:.2f} (In: {cache['total_in']:,} / Out: {cache['total_out']:,})")
        st.caption(f"ç´°ç¯€è€—æ™‚: Azure OCR {cache['ocr_duration']:.1f}s | AI åˆ†æ {cache['time_eng']:.1f}s")

        # 2. è¦å‰‡æª¢è¦–
        with st.expander("ğŸ” æª¢è¦– Excel è¦å‰‡èˆ‡é‚è¼¯åƒæ•¸", expanded=False):
            rules_text = get_dynamic_rules(cache.get('full_text_for_search',''), debug_mode=True)
            st.markdown(rules_text)
                
        # 3. åŸå§‹æ•¸æ“šæª¢è¦–
        with st.expander("ğŸ“Š æª¢è¦– AI æŠ„éŒ„åŸå§‹æ•¸æ“š", expanded=False):
            st.markdown("**1. æ ¸å¿ƒæŒ‡æ¨™æ‘˜è¦**")
            f_target = cache.get('freight_target', 0)
            sum_rows_len = len(cache.get("summary_rows", []))
            summary_df = pd.DataFrame([{
                "å·¥ä»¤å–®è™Ÿ": cache.get("job_no", "N/A"),
                "é‹è²» Target (PC)": f_target,
                "é‹è²»åµæ¸¬ç‹€æ…‹": "æœ‰æŠ“åˆ°" if f_target > 0 else "æœªåµæ¸¬",
                "ç¸½è¡¨è¡Œæ•¸": sum_rows_len,
                "ç¸½è¡¨ç‹€æ…‹": "æ­£å¸¸" if sum_rows_len > 0 else "ç©ºå€¼"
            }])
            st.dataframe(summary_df, hide_index=True, use_container_width=True)
            
            st.divider()
 
            # B. ç¸½è¡¨æ¸…å–® (âš¡ï¸ ä¿®æ”¹é»ï¼šæ–°å¢é ç¢¼æ¬„ä½)
            st.markdown("**2. å·¦ä¸Šè§’çµ±è¨ˆè¡¨ (Summary Rows)**")
            sum_rows = cache.get("summary_rows", [])
            
            if sum_rows:
                df_sum = pd.DataFrame(sum_rows)
                # ç¢ºä¿ page æ¬„ä½å­˜åœ¨ (å¦‚æœèˆŠçš„ Cache æ²’æœ‰ pageï¼Œè£œä¸Š "?")
                if "page" not in df_sum.columns:
                    df_sum["page"] = "?"
                
                # é‡æ–°å‘½åèˆ‡æ’åºï¼šæŠŠé ç¢¼æ”¾åœ¨ç¬¬ä¸€æ¬„
                df_sum.rename(columns={"page": "é ç¢¼", "title": "é …ç›®åç¨±", "target": "å¯¦äº¤æ•¸é‡"}, inplace=True)
                
                # èª¿æ•´é¡¯ç¤ºé †åº
                cols = ["é ç¢¼", "é …ç›®åç¨±", "å¯¦äº¤æ•¸é‡"]
                # ç¢ºä¿åªé¡¯ç¤ºå­˜åœ¨çš„æ¬„ä½ (é˜²å‘†)
                cols = [c for c in cols if c in df_sum.columns]
                
                st.dataframe(df_sum[cols], hide_index=True, use_container_width=True)
            else:
                st.caption("ç„¡æ•¸æ“š (è®Šæ•¸ summary_rows ç‚ºç©º)")

            st.divider()

            st.markdown("**3. å…¨å·è©³ç´°æŠ„éŒ„æ•¸æ“š (JSON)**")
            st.json(cache.get("ai_extracted_data", []), expanded=True)

        # 4. Python Debug
        with st.expander("ğŸ Python ç¡¬é‚è¼¯åµæ¸¬çµæœ", expanded=False):
            if cache.get('python_debug_data'):
                st.dataframe(cache['python_debug_data'], use_container_width=True, hide_index=True)
            else:
                st.caption("ç„¡åµæ¸¬è³‡æ–™")

        # ========================================================
        # âš¡ï¸ [ä¿®æ­£é‡é»]ï¼šå…ˆé€²è¡Œåˆä½µï¼Œå†æ ¹æ“šåˆä½µå¾Œçš„æ¸…å–®ä¾†è¨ˆç®—æ•¸é‡
        # ========================================================
        
        # 1. åŸ·è¡Œåˆä½µ (æŠŠ 51 å€‹ç•°å¸¸å£“ç¸®æˆ N é¡)
        consolidated_list = consolidate_issues(all_issues)

        # 2. éæ¿¾å‡ºã€ŒçœŸæ­£çš„éŒ¯èª¤ã€ (æ’é™¤åƒ…æ˜¯æœªåŒ¹é…è¦å‰‡çš„è­¦å‘Š)
        # æ³¨æ„ï¼šæˆ‘å€‘æ˜¯åœ¨ consolidated_list ä¸Šåšç¯©é¸ï¼Œé€™æ¨£æ•¸é‡æ‰æœƒå°
        real_errors_consolidated = [i for i in consolidated_list if "æœªåŒ¹é…" not in i.get('issue_type', '')]

        # 3. é¡¯ç¤ºçµè«– (ä½¿ç”¨åˆä½µå¾Œçš„æ•¸é‡)
        if not all_issues:
            st.balloons()
            st.success("âœ… å…¨æ•¸åˆæ ¼ï¼")
        elif not real_errors_consolidated:
            # é€™è£¡ç”¨ len(consolidated_list) ä»£è¡¨é‚„æœ‰å¹¾å€‹é»ƒè‰²è­¦å‘Š
            st.success(f"âœ… æ•¸å€¼åˆæ ¼ï¼ (ä½†æœ‰ {len(consolidated_list)} é¡é …ç›®æœªåŒ¹é…è¦å‰‡)")
        else:
            # é€™è£¡é¡¯ç¤ºç´…è‰²çš„ç•°å¸¸ã€Œé¡åˆ¥ã€æ•¸é‡
            st.error(f"ç™¼ç¾ {len(real_errors_consolidated)} é¡ç•°å¸¸")

        # 4. å¡ç‰‡å¾ªç’°é¡¯ç¤º (ä½¿ç”¨åˆä½µå¾Œçš„æ¸…å–®)
        for item in consolidated_list:
            with st.container(border=True):
                c1, c2 = st.columns([3, 1])
                source_label = item.get('source', '')
                issue_type = item.get('issue_type', 'ç•°å¸¸')
                
                # é ç¢¼é¡¯ç¤ºå„ªåŒ–
                page_str = item.get('page', '?')
                if "," in str(page_str):
                    page_display = f"Pages: {page_str}"
                else:
                    page_display = f"P.{page_str}"

                c1.markdown(f"**{page_display} | {item.get('item')}** `{source_label}`")
                
                if any(kw in issue_type for kw in ["çµ±è¨ˆ", "æ•¸é‡", "æµç¨‹", "æº¯æº"]):
                    c2.error(f"ğŸ›‘ {issue_type}")
                else:
                    c2.warning(f"âš ï¸ {issue_type}")
                
                st.caption(f"åŸå› : {item.get('common_reason', '')}")
                
                failures = item.get('failures', [])
                if failures:
                    table_data = []
                    for f in failures:
                        if isinstance(f, dict):
                            table_data.append({
                                "é …ç›®/ç·¨è™Ÿ": f.get('id', 'æœªçŸ¥'), 
                                "å¯¦æ¸¬/è¨ˆæ•¸": f.get('val', 'N/A'),
                                "æ¨™æº–/å‚™è¨»": f.get('target', ''),
                                "ç‹€æ…‹": f.get('calc', '')
                            })
                    st.dataframe(table_data, use_container_width=True, hide_index=True)
        
        st.divider()
        
        # ... (é€™è£¡æ¥ä½ åŸæœ¬å‰©ä¸‹çš„ä»£ç¢¼å³å¯ï¼Œä¹Ÿè¦è¨˜å¾—ç¸®æ’å¾€å·¦ç§»)
        current_job_no = cache.get('job_no', 'Unknown')
        safe_job_no = current_job_no.replace("/", "_").replace("\\", "_").strip()
        file_name_str = f"{safe_job_no}_cleaned.json"

        # æº–å‚™åŒ¯å‡ºè³‡æ–™
        export_data = []
        for item in st.session_state.photo_gallery:
            export_data.append({
                "table_md": item.get('table_md'),
                "header_text": item.get('header_text'),
                "full_text": item.get('full_text'),
                "raw_json": item.get('raw_json')
            })
        json_str = json.dumps(export_data, indent=2, ensure_ascii=False)

        st.subheader("ğŸ’¾ æ¸¬è©¦è³‡æ–™å­˜æª”")
        st.caption(f"å·²è­˜åˆ¥å·¥ä»¤ï¼š**{current_job_no}**ã€‚ä¸‹è¼‰å¾Œå¯ä¾›ä¸‹æ¬¡æ¸¬è©¦ä½¿ç”¨ã€‚")
        
        st.download_button(
            label=f"â¬‡ï¸ ä¸‹è¼‰æ¸¬è©¦è³‡æ–™ ({file_name_str})",
            data=json_str,
            file_name=file_name_str,
            mime="application/json",
            type="primary"
        )

        # ğŸ’¡ ä½¿ç”¨ .get() å¯ä»¥é˜²æ­¢å› ç‚ºæ‰¾ä¸åˆ°æ¨™ç±¤è€Œç›´æ¥å ±éŒ¯ç•¶æ©Ÿ
        with st.expander("ğŸ‘€ æŸ¥çœ‹å‚³çµ¦ AI çš„æœ€çµ‚æ–‡å­— (Prompt Input)"):
            st.caption("é€™æ‰æ˜¯ AI çœŸæ­£è®€åˆ°çš„å…§å®¹ (å·²éæ¿¾é›œè¨Š)ï¼š")
            st.code(cache.get('combined_input', 'ç„¡è³‡æ–™'), language='markdown')
    
    if st.session_state.photo_gallery and st.session_state.get('source_mode') != 'json':
        st.caption("å·²æ‹æ”ç…§ç‰‡ï¼š")
        cols = st.columns(4)
        for idx, item in enumerate(st.session_state.photo_gallery):
            with cols[idx % 4]:
                if item.get('file'):
                    st.image(item['file'], caption=f"P.{idx+1}", use_container_width=True)
                if st.button("âŒ", key=f"del_{idx}"):
                    st.session_state.photo_gallery.pop(idx)
                    st.session_state.analysis_result_cache = None
                    st.rerun()
else:
    st.info("ğŸ‘† è«‹é»æ“Šä¸Šæ–¹æŒ‰éˆ•é–‹å§‹æ–°å¢ç…§ç‰‡")
