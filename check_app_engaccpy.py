import streamlit as st
import streamlit.components.v1 as components
from azure.core.credentials import AzureKeyCredential
from azure.ai.documentintelligence import DocumentIntelligenceClient
from azure.ai.documentintelligence.models import AnalyzeResult
import google.generativeai as genai
from openai import OpenAI
import json
import time
import concurrent.futures
import pandas as pd
from thefuzz import fuzz
from collections import Counter
import re

# --- 1. é é¢è¨­å®š ---
st.set_page_config(page_title="äº¤è²¨å–®ç¨½æ ¸", page_icon="ğŸ­", layout="centered")

# --- CSS æ¨£å¼ ---
st.markdown("""
<style>
/* 1. æ¨™é¡Œå¤§å°æ§åˆ¶ */
h1 {
    font-size: 1.7rem !important; 
    white-space: nowrap !important;
    overflow: hidden !important; 
    text-overflow: ellipsis !important;
}

/* 2. ä¸»åŠŸèƒ½æŒ‰éˆ• (ç´…è‰² Primary) -> è®Šå¤§ã€è®Šé«˜ */
/* é€™æœƒå½±éŸ¿ã€Œé–‹å§‹åˆ†æã€å’Œã€Œç…§ç‰‡æ¸…é™¤ã€ */
button[kind="primary"] {
    height: 60px;               
    font-size: 20px !important; 
    font-weight: bold !important;
    border-radius: 10px !important;
    margin-top: 0px !important;    
    margin-bottom: 5px !important; 
    width: 100%;                
}

/* 3. æ¬¡è¦æŒ‰éˆ• (ç°è‰² Secondary) -> ä¿æŒåŸç‹€ */
/* é€™æœƒå½±éŸ¿æ¯ä¸€å¼µç…§ç‰‡ä¸‹é¢çš„ã€ŒXã€æŒ‰éˆ•ï¼Œè®“å®ƒç¶­æŒå°å°çš„ */
button[kind="secondary"] {
    height: auto !important;
    font-weight: normal !important;
}
</style>
""", unsafe_allow_html=True)
# --- 2. ç§˜å¯†é‡‘é‘°è®€å– ---
try:
    DOC_ENDPOINT = st.secrets["DOC_ENDPOINT"]
    DOC_KEY = st.secrets["DOC_KEY"]
    GEMINI_KEY = st.secrets["GEMINI_KEY"]
    OPENAI_KEY = st.secrets.get("OPENAI_KEY", "")
except:
    st.error("æ‰¾ä¸åˆ°é‡‘é‘°ï¼è«‹åœ¨ Streamlit Cloud è¨­å®š Secretsã€‚")
    st.stop()

# --- 3. åˆå§‹åŒ– Session State ---
if 'photo_gallery' not in st.session_state: st.session_state.photo_gallery = []
if 'uploader_key' not in st.session_state: st.session_state.uploader_key = 0
if 'auto_start_analysis' not in st.session_state: st.session_state.auto_start_analysis = False

# --- å´é‚Šæ¬„æ¨¡å‹è¨­å®š (åˆä½µç‚ºå–®ä¸€é¸æ“‡) ---
with st.sidebar:
    st.header("æ¨¡å‹è¨­å®š")
    
    # é€™è£¡åŠ å…¥æœ€æ–°çš„ Gemini æ¨¡å‹
    model_options = {
        "Gemini 3 Flash preview": "gemini-3-flash-preview",
        "Gemini 2.5 Flash": "models/gemini-2.5-flash",
        "Gemini 2.5 Pro": "models/gemini-2.5-pro",
        #"GPT-5(ç„¡æ•ˆ)": "models/gpt-5",
        #"GPT-5 Mini(ç„¡æ•ˆ)": "models/gpt-5-mini",
    }
    options_list = list(model_options.keys())
    
    st.subheader("ğŸ¤– ç¸½ç¨½æ ¸ Agent")
    model_selection = st.selectbox(
        "è² è²¬ï¼šè¦æ ¼ã€è£½ç¨‹ã€æ•¸é‡ã€çµ±è¨ˆå…¨åŒ…", 
        options=options_list, 
        index=0, 
        key="main_model"
    )
    main_model_name = model_options[model_selection]
    
    st.divider()
    
    default_auto = st.query_params.get("auto", "true") == "true"
    def update_url_param():
        current_state = "true" if st.session_state.enable_auto_analysis else "false"
        st.query_params["auto"] = current_state

    st.toggle(
        "âš¡ ä¸Šå‚³å¾Œè‡ªå‹•åˆ†æ", 
        value=default_auto, 
        key="enable_auto_analysis", 
        on_change=update_url_param
    )

# --- Excel è¦å‰‡è®€å–å‡½æ•¸ (å–®ä¸€ä»£ç†æ•´åˆç‰ˆ) ---
@st.cache_data
def get_dynamic_rules(ocr_text, debug_mode=False):
    try:
        df = pd.read_excel("rules.xlsx")
        df.columns = [c.strip() for c in df.columns]
        ocr_text_clean = str(ocr_text).upper().replace(" ", "").replace("\n", "")
        specific_rules = []

        for index, row in df.iterrows():
            item_name = str(row.get('Item_Name', '')).strip()
            # ğŸ’¡ è·³éåŸæœ¬çš„ã€Œ(é€šç”¨)ã€é …ç›®ï¼ŒåªæŠ“ç‰¹è¦
            if not item_name or "(é€šç”¨)" in item_name: continue
            
            # ä½¿ç”¨æ¨¡ç³ŠåŒ¹é…åˆ¤æ–·æ˜¯å¦ç‚ºç•¶å‰è™•ç†çš„é …ç›®
            score = fuzz.partial_ratio(item_name.upper().replace(" ", ""), ocr_text_clean)
            if score >= 85:
                # æå–ç‰¹è¦è³‡è¨Š
                spec = str(row.get('Standard_Spec', ''))
                logic = str(row.get('Logic_Prompt', ''))
                u_local = str(row.get('Unit_Rule_Local', ''))
                u_agg = str(row.get('Unit_Rule_Agg', ''))
                u_freight = str(row.get('Unit_Rule_Freight', ''))
                
                desc = f"- **[ç‰¹å®šé …ç›®è¦å‰‡] {item_name}**\n"
                if spec != 'nan' and spec: desc += f"  - [å¼·åˆ¶è¦æ ¼]: {spec}\n"
                if logic != 'nan' and logic: desc += f"  - [ä¾‹å¤–æŒ‡ä»¤]: {logic}\n"
                if u_local != 'nan' and u_local: desc += f"  - [æœƒè¨ˆå–®é …]: {u_local}\n"
                if u_agg != 'nan' and u_agg: desc += f"  - [æœƒè¨ˆèšåˆ]: {u_agg}\n"
                if u_freight != 'nan' and u_freight: desc += f"  - [æœƒè¨ˆé‹è²»]: {u_freight}\n"
                specific_rules.append(desc)
        
        return "\n".join(specific_rules) if specific_rules else "ç„¡ç‰¹å®šå°ˆæ¡ˆè¦å‰‡ï¼Œè«‹ä¾ç…§é€šç”¨æ†²æ³•åŸ·è¡Œã€‚"
    except Exception as e:
        return f"è®€å–è¦å‰‡æª”æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}"
        
# --- 4. æ ¸å¿ƒå‡½æ•¸ï¼šAzure ç¥ä¹‹çœ¼ ---
def extract_layout_with_azure(file_obj, endpoint, key):
    client = DocumentIntelligenceClient(endpoint=endpoint, credential=AzureKeyCredential(key))
    file_content = file_obj.getvalue()
    
    poller = client.begin_analyze_document("prebuilt-layout", file_content, content_type="application/octet-stream")
    result: AnalyzeResult = poller.result()
    
    markdown_output = ""
    full_content_text = ""
    real_page_num = "Unknown"
    
    bottom_stop_keywords = ["æ³¨æ„äº‹é …", "ä¸­æ©Ÿå“æª¢å–®ä½", "ä¿å­˜æœŸé™", "è¡¨å–®ç·¨è™Ÿ", "FORM NO", "ç°½ç« "]
    top_right_noise_keywords = [
        "æª¢é©—é¡åˆ¥", "å°ºå¯¸æª¢é©—", "ä¾åœ–é¢æ¨™è¨˜", "ææ–™æª¢é©—", "æˆä»½åˆ†æ", 
        "éç ´å£æ€§", "æ­£å¸¸åŒ–", "é€€ç«", "æ·¬.å›ç«", "è¡¨é¢ç¡¬åŒ–", "è©¦è»Š",
        "æ€§èƒ½æ¸¬è©¦", "è©¦å£“è©¦æ¼", "å‹•.éœå¹³è¡¡è©¦é©—", ":selected:", ":unselected:",
        "æŠ—æ‹‰", "ç¡¬åº¦è©¦é©—", "UT", "PT", "MT"
    ]
    
    if result.tables:
        for idx, table in enumerate(result.tables):
            page_num = "Unknown"
            if table.bounding_regions: page_num = table.bounding_regions[0].page_number
            markdown_output += f"\n### Table {idx + 1} (Page {page_num}):\n"
            rows = {}
            stop_processing_table = False 
            
            for cell in table.cells:
                if stop_processing_table: break
                content = cell.content.replace("\n", " ").strip()
                
                for kw in bottom_stop_keywords:
                    if kw in content:
                        stop_processing_table = True
                        break
                if stop_processing_table: break
                
                is_noise = False
                for kw in top_right_noise_keywords:
                    if kw in content:
                        is_noise = True
                        break
                if is_noise: content = "" 

                r, c = cell.row_index, cell.column_index
                if r not in rows: rows[r] = {}
                rows[r][c] = content
            
            for r in sorted(rows.keys()):
                row_cells = []
                if rows[r]:
                    max_col = max(rows[r].keys())
                    for c in range(max_col + 1): 
                        row_cells.append(rows[r].get(c, ""))
                    markdown_output += "| " + " | ".join(row_cells) + " |\n"
    
    if result.content:
        match = re.search(r"(?:é …æ¬¡|Page|é æ¬¡|NO\.)[:\s]*(\d+)\s*[/ï¼]\s*\d+", result.content, re.IGNORECASE)
        if match:
            real_page_num = match.group(1)

        cut_index = len(result.content)
        for keyword in bottom_stop_keywords:
            idx = result.content.find(keyword)
            if idx != -1 and idx < cut_index:
                cut_index = idx
        
        temp_text = result.content[:cut_index]
        for noise in top_right_noise_keywords:
            temp_text = temp_text.replace(noise, "")
            
        full_content_text = temp_text
        header_snippet = full_content_text[:800]
    else:
        full_content_text = ""
        header_snippet = ""

    return markdown_output, header_snippet, full_content_text, None, real_page_num

# --- Python ç¡¬é‚è¼¯ï¼šè¡¨é ­ä¸€è‡´æ€§æª¢æŸ¥ (é•·åº¦æ•æ„Ÿç‰ˆ) ---
def python_header_check(photo_gallery):
    issues = []
    if not photo_gallery:
        return issues, []

    # å®šç¾© Regex (é‡å° "å»ç©ºç™½+å»æ›è¡Œ" å¾Œçš„å­—ä¸²è¨­è¨ˆ)
    patterns = {
        # ã€ä¿®æ”¹é» 1ã€‘å·¥ä»¤ Regex æ”¾å¯¬ï¼š
        # åŸæœ¬åªæŠ“ W é–‹é ­ï¼Œç¾åœ¨æ”¹æŠ“ "ç·¨è™Ÿ" å¾Œé¢æ¥çš„ "ä»»ä½•è‹±æ•¸å­—ä¸²"
        # é€™æ¨£å°±ç®—å®ƒå¯« WW363... æˆ–æ˜¯ 12345... éƒ½èƒ½æ•´ä¸²æŠ“å‡ºä¾†æ¯”å°
        "å·¥ä»¤ç·¨è™Ÿ": r"[å·¥åœŸä¸‹][ä»¤å†·ä»Š]ç·¨è™Ÿ[:\.]*([A-Za-z0-9\-\_]+)", 
        
        "é å®šäº¤è²¨": r"[é é¢„é …é ‚][å®šäº¤].*?(\d{2,4}[\.\-/]\d{1,2}[\.\-/]\d{1,2})",
        "å¯¦éš›äº¤è²¨": r"[å¯¦çœŸ][éš›äº¤].*?(\d{2,4}[\.\-/]\d{1,2}[\.\-/]\d{1,2})"
    }

    extracted_data = [] 
    all_values = {key: [] for key in patterns}

    for i, page in enumerate(photo_gallery):
        # æš´åŠ›æ¸…æ´—ï¼šå»æ›è¡Œã€å»ç©ºæ ¼ã€è½‰å¤§å¯«
        raw_text = page.get('header_text', '') + page.get('full_text', '')
        clean_text = raw_text.replace("\n", "").replace(" ", "").replace("\r", "").upper()
        
        # ã€ä¿®æ”¹é» 2ã€‘é ç¢¼é˜²å‘†ï¼šç¢ºä¿ä¸€å®šæœ‰å€¼
        # å„ªå…ˆæŠ“ real_pageï¼ŒæŠ“ä¸åˆ°å°±ç”¨ index
        r_page = page.get('real_page')
        if not r_page or r_page == "Unknown":
            page_label = f"P.{i + 1}"
        else:
            page_label = f"P.{r_page}"
            
        page_result = {"é æ•¸": page_label}
        
        for key, pattern in patterns.items():
            match = re.search(pattern, clean_text)
            if match:
                val = match.group(1).strip()
                
                # ã€ä¿®æ”¹é» 3ã€‘é‡å°å·¥ä»¤çš„ç‰¹æ®Šè™•ç† (å¦‚æœå¤ªé•·å¯èƒ½å°±æ˜¯é‡è¤‡æ‰“å­—)
                if key == "å·¥ä»¤ç·¨è™Ÿ":
                    # å¦‚æœä½ ç¢ºå®šå·¥ä»¤åªæœ‰ 10 ç¢¼ï¼Œä½†æŠ“åˆ°äº† 11 ç¢¼ä»¥ä¸Š (å¦‚ WW...)
                    # æˆ‘å€‘ä¿ç•™é€™å€‹éŒ¯èª¤çš„å€¼ï¼Œè®“å¾Œé¢çš„å¤šæ•¸æ±ºå»æŠŠå®ƒæªå‡ºä¾†
                    pass 
                
                page_result[key] = val
                all_values[key].append(val)
            else:
                page_result[key] = "N/A"
        
        extracted_data.append(page_result)

    # æ­¥é©Ÿ 2: æ±ºå®šã€Œæ­£ç¢ºæ¨™æº–ã€ (ä½¿ç”¨å¤šæ•¸æ±º)
    standard_data = {}
    for key, values in all_values.items():
        if values:
            # æ¿¾æ‰ N/A å¾Œå†æŠ•ç¥¨
            valid_values = [v for v in values if v != "N/A"]
            if valid_values:
                most_common = Counter(valid_values).most_common(1)[0][0]
                standard_data[key] = most_common
            else:
                standard_data[key] = "N/A"
        else:
            standard_data[key] = "N/A"

    # æ­¥é©Ÿ 3: æ¯”å°æ¯ä¸€é 
    for data in extracted_data:
        page_num = data['é æ•¸']
        
        for key, standard_val in standard_data.items():
            current_val = data[key]
            
            if standard_val == "N/A": continue # å…¨å·éƒ½æ²’æŠ“åˆ°å°±ä¸æ¯”äº†

            # é–‹å§‹æ¯”å° (å­—ä¸²ä¸ç›¸ç­‰)
            if current_val != standard_val:
                
                # åˆ¤æ–·æ˜¯å¦ç‚ºé•·åº¦ç•°å¸¸ (é‡å°å·¥ä»¤)
                reason = "èˆ‡å…¨å·å¤šæ•¸é é¢ä¸ä¸€è‡´"
                if key == "å·¥ä»¤ç·¨è™Ÿ" and len(current_val) != len(standard_val):
                    reason += f" (é•·åº¦ç•°å¸¸: {len(current_val)}ç¢¼ vs æ¨™æº–{len(standard_val)}ç¢¼)"

                issue = {
                    "page": page_num.replace("P.", ""),
                    "item": f"è¡¨é ­æª¢æŸ¥-{key}",
                    "rule_used": "Pythonç¡¬é‚è¼¯æª¢æŸ¥",
                    "issue_type": "è·¨é è³‡è¨Šä¸ç¬¦",
                    "spec_logic": f"æ‡‰ç‚º {standard_val}",
                    "common_reason": reason,
                    "failures": [
                        {"id": "å…¨å·åŸºæº–", "val": standard_val, "calc": "å¤šæ•¸æ±ºæ¨™æº–"},
                        {"id": f"æœ¬é ({page_num})", "val": current_val, "calc": "ç•°å¸¸/æ¼æŠ“"}
                    ],
                    "source": "ğŸ¤– ç³»çµ±è‡ªå‹•"
                }
                issues.append(issue)
                
    return issues, extracted_data

    # --- 5. ç¸½ç¨½æ ¸ Agent (æ•´åˆç‰ˆ - å¼·é‚è¼¯å„ªåŒ–) ---
def agent_unified_check(combined_input, full_text_for_search, api_key, model_name):
    dynamic_rules = get_dynamic_rules(full_text_for_search)

    system_prompt = f"""
    ä½ æ˜¯ä¸€ä½æ¥µåº¦åš´è¬¹çš„ä¸­é‹¼æ©Ÿæ¢°å“ç®¡ã€æ•¸æ“šæŠ„éŒ„å“¡ã€‘ã€‚ä½ å¿…é ˆåƒã€Œé›»è…¦ç¨‹å¼ã€ä¸€æ¨£åŸ·è¡Œä»»å‹™ã€‚
    
    {dynamic_rules}

    ---

    #### âš”ï¸ æ¨¡çµ„ Aï¼šå·¥ç¨‹å°ºå¯¸æ•¸æ“šæå– (AI ä»»å‹™ï¼šæŠ„éŒ„)
    1. **è¦æ ¼æŠ„éŒ„ (std_spec)**ï¼šç²¾ç¢ºæŠ„éŒ„æ¨™é¡Œä¸­å« `mm`ã€`Â±`ã€`+`ã€`-` çš„åŸå§‹æ–‡å­—ã€‚
    2. **æ•¸æ“šæŠ„éŒ„ (ds)**ï¼šæ ¼å¼ `"ID:å€¼|ID:å€¼"`ã€‚ç¦æ­¢ç°¡åŒ–ï¼Œ`349.90` å¿…å¯« `"349.90"`ã€‚
    3. **é …ç›®åˆ†é¡æ±ºç­–æµç¨‹ (ç”±ä¸Šè‡³ä¸‹åŸ·è¡Œï¼Œå‘½ä¸­å³åœæ­¢)**ï¼š
        - **LEVEL 1ï¼šéŠ²è£œèˆ‡è£é…åˆ¤å®š (æœ€é«˜å„ªå…ˆ)**
          * æ¨™é¡Œå«ã€ŒéŠ²è£œã€ã€ã€ŒéŠ²æ¥ã€ -> `min_limit`ã€‚
          * æ¨™é¡Œå«ã€Œçµ„è£ã€ã€ã€Œæ‹†è£ã€ã€ã€Œè£é…ã€ã€ã€ŒçœŸåœ“åº¦ã€ -> `range`ã€‚
         
        - **LEVEL 2ï¼šæœªå†ç”Ÿåˆ¤å®š (å«è»Šä¿®)**
          * æ¨™é¡Œå«ã€Œæœªå†ç”Ÿã€ä¸‰å­—æ™‚ï¼š
            a. å«ã€Œè»¸é ¸ã€ -> `max_limit`ã€‚
            b. ä¸å«ã€Œè»¸é ¸ã€(æœ¬é«”) -> `un_regen`ã€‚
          * (ğŸ’¡ æ³¨æ„ï¼šæ­¤é¡é …ç›®å³ä½¿åŒ…å«ã€Œè»Šä¿®ã€å­—çœ¼ï¼Œä¹Ÿå¿…é ˆé–å®šåœ¨ LEVEL 2ï¼Œåš´ç¦é€²å…¥ä¸‹ä¸€å€‹å±¤ç´š)ã€‚
         
        - **LEVEL 3ï¼šç²¾åŠ å·¥åˆ¤å®š**
          * æ¨™é¡Œä¸å«ã€Œæœªå†ç”Ÿã€ï¼Œä¸”åŒ…å«ã€Œå†ç”Ÿã€ã€ã€Œç ”ç£¨ã€ã€ã€Œç²¾åŠ å·¥ã€ã€ã€Œè»Šä¿®åŠ å·¥ã€ã€ã€ŒKEYWAYã€ -> `range`ã€‚

    4. **æ•¸æ“šæŠ„éŒ„ (å­—ä¸²ä¿è­·æ¨¡å¼)**ï¼š
       - **ç¦æ­¢ç°¡åŒ–**ï¼šå¯¦æ¸¬å€¼è‹¥é¡¯ç¤º `349.90`ï¼Œå¿…é ˆè¼¸å‡º `"349.90"`ã€‚
       - **æ ¼å¼**ï¼šæ‰€æœ‰å¯¦æ¸¬å€¼å¿…é ˆåŒ…è£¹æˆé›™å¼•è™Ÿå­—ä¸²ã€‚`["RollID", "å¯¦æ¸¬å€¼å­—ä¸²"]`ã€‚

    #### ğŸ’° æ¨¡çµ„ Bï¼šæœƒè¨ˆæŒ‡æ¨™æå– (AI ä»»å‹™ï¼šæŠ„éŒ„)
    1. **çµ±è¨ˆè¡¨**ï¼šæŠ„éŒ„çµ±è¨ˆè¡¨æ¯ä¸€è¡Œåç¨±èˆ‡å¯¦äº¤æ•¸é‡åˆ° `summary_rows`ã€‚
    2. **é‹è²»èˆ‡æŒ‡æ¨™**ï¼šæå–é‹è²»é …æ¬¡èˆ‡æ¨™é¡Œæ‹¬è™Ÿå…§çš„ PC æ•¸ã€‚ä½ ä¸éœ€æŠ„éŒ„è¦å‰‡æ–‡å­—ã€‚

    #### âš–ï¸ æ¨¡çµ„ Cï¼šæµç¨‹ç¨½æ ¸ (AI ä»»å‹™ï¼šåˆ¤å®š)
    1. **ä½éšæª¢æŸ¥**ï¼š`æœªå†ç”Ÿ < ç ”ç£¨ < å†ç”Ÿ < éŠ²è£œ`ã€‚è‹¥è·¨é é¢å¾Œæ®µå°ºå¯¸å°æ–¼å‰æ®µï¼ˆéŠ²è£œé™¤å¤–ï¼‰ï¼Œå ± `ğŸ›‘æµç¨‹ç•°å¸¸`ã€‚

    ---

    ### ğŸ“ è¼¸å‡ºè¦ç¯„ (Output Format)
    å¿…é ˆå›å‚³å–®ä¸€ JSONã€‚çµ±è¨ˆä¸ç¬¦æ™‚å¿…é ˆã€Œé€è¡Œæ‹†åˆ†ã€ä¾†æºæ˜ç´°ã€‚

    {{
      "job_no": "å·¥ä»¤",
      "summary_rows": [ {{ "title": "å", "target": æ•¸å­— }} ],
      "freight_target": 0,
      "issues": [ 
         {{ "page": "é ç¢¼", "item": "é …ç›®", "issue_type": "çµ±è¨ˆä¸ç¬¦ / ğŸ›‘æµç¨‹ç•°å¸¸", "common_reason": "åŸå› ", "failures": [] }}
      ],
      "dimension_data": [
         {{
           "page": æ•¸å­—, "item_title": "æ¨™é¡Œ", "category": "åˆ†é¡åç¨±", "item_pc_target": 0,
           "accounting_rules": {{ "local": "", "agg": "", "freight": "" }},
           "sl": {{ "lt": "åˆ†é¡æ¨™ç±¤", "t": 0 }},
           "std_spec": "åŸå§‹è¦æ ¼æ–‡å­—",
           "ds": "ID:å€¼|ID:å€¼" 
         }}
      ]
    }}
    """
    
    generation_config = {"response_mime_type": "application/json", "temperature": 0.0}
    
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel(model_name)
        response = model.generate_content([system_prompt, combined_input], generation_config=generation_config)
        
        raw_content = response.text
        # ğŸ›¡ï¸ è¶…ç´šè§£æå™¨ï¼šé˜²æ­¢ AI è¼¸å‡ºå¸¶æœ‰ Markdown æ¨™ç±¤æˆ–å»¢è©±
        import re
        json_match = re.search(r"\{.*\}", raw_content, re.DOTALL)
        if json_match:
            raw_content = json_match.group()
            
        parsed_data = json.loads(raw_content)
        parsed_data["_token_usage"] = {
            "input": response.usage_metadata.prompt_token_count, 
            "output": response.usage_metadata.candidates_token_count
        }
        return parsed_data

    except Exception as e:
        return {"job_no": f"JSON Error: {str(e)}", "issues": [], "dimension_data": []}
        
# --- é‡é»ï¼šPython å¼•æ“ç¨ç«‹æ–¼ agent å‡½å¼ä¹‹å¤– ---

def python_process_audit(dimension_data):
    process_issues = []
    roll_history = {} # { "ID": [{"p": "cat", "v": 190, "page": 1}, ...] }
    if not dimension_data: return []

    for item in dimension_data:
        p_num, ds, cat = item.get("page", "?"), item.get("ds", ""), str(item.get("category", "")).strip()
        pairs = [p.split(":") for p in ds.split("|") if ":" in p]
        for rid, val_str in pairs:
            try:
                val = float(re.findall(r"\d+\.?\d*", val_str)[0])
                rid_clean = rid.strip()
                if rid_clean not in roll_history: roll_history[rid_clean] = []
                roll_history[rid_clean].append({"p": cat, "v": val, "page": p_num, "title": item.get("item_title")})
            except: continue

    weights = {"un_regen": 1, "max_limit": 1, "range": 3, "min_limit": 4}
    for rid, records in roll_history.items():
        if len(records) < 2: continue
        records.sort(key=lambda x: str(x['page']))
        for i in range(len(records) - 1):
            curr, nxt = records[i], records[i+1]
            w_curr = weights.get(curr['p'], 2)
            if "ç ”ç£¨" in curr['title']: w_curr = 2
            w_nxt = weights.get(nxt['p'], 2)
            if "ç ”ç£¨" in nxt['title']: w_nxt = 2
            
            # ğŸ’¡ é—œéµåˆ¤å®šï¼šå¾Œæ®µä½éšå¤§ï¼Œæ•¸å€¼å°±ä¸æ‡‰è©²è®Šå°
            if w_nxt > w_curr and nxt['v'] < curr['v']:
                process_issues.append({
                    "page": nxt['page'], "item": f"ç·¨è™Ÿ {rid} å°ºå¯¸ä½éšæª¢æŸ¥",
                    "issue_type": "ğŸ›‘æµç¨‹ç•°å¸¸(å°ºå¯¸å€’ç½®)",
                    "common_reason": f"å¾Œæ®µ{nxt['p']}å°ºå¯¸å°æ–¼å‰æ®µ{curr['p']}",
                    "failures": [{"id": rid, "val": f"å¾Œ:{nxt['v']} < å‰:{curr['v']}", "calc": "å°ºå¯¸ä¸ç¬¦ä½éšé‚è¼¯"}]
                })
    return process_issues
    
def python_numerical_audit(dimension_data):
    grouped_errors = {}
    import re
    if not dimension_data: return []

    for item in dimension_data:
        # 1. å–å¾—æ•¸æ“š (DS æ ¼å¼)
        ds = str(item.get("ds", ""))
        if not ds: continue
        raw_entries = [p.split(":") for p in ds.split("|") if ":" in p]
        
        title = str(item.get("item_title", "")).replace(" ", "").replace('"', "")
        cat = str(item.get("category", "")).strip()
        page_num = item.get("page", "?")
        raw_spec = str(item.get("std_spec", "")).replace('"', "")
        
        # 2. ğŸ›¡ï¸ æ•¸æ“šæ¸…æ´—èˆ‡ mm å®šä½ (æ¯å€‹é …ç›®åªç®—ä¸€æ¬¡ï¼Œæé€Ÿé—œéµ)
        mm_nums = [float(n) for n in re.findall(r"(\d+\.?\d*)\s*mm", raw_spec)]
        all_nums = [float(n) for n in re.findall(r"(\d+\.?\d*)", raw_spec)]
        noise = [350.0, 300.0, 200.0, 145.0, 130.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0]
        # å…æ­»é‡‘ç‰Œï¼šç·Šè²¼ mm çš„æ•¸å­—ä¸å‡†éæ¿¾
        clean_std = [n for n in all_nums if (n in mm_nums) or (n not in noise and n > 5)]

        # 3. ğŸ’¡ å…¬å·®è‡ªå‹•é ç®— (è§£æ±º AI ç®—æ•¸å­¸æ…¢çš„å•é¡Œ)
        s_ranges = []
        pm_match = re.search(r"(\d+\.?\d*)\s*[Â±]\s*(\d+\.?\d*)", raw_spec)
        dev_match = re.search(r"(\d+\.?\d*)\s*[\+]\s*(\d+\.?\d*)\s*,\s*[\-]\s*(\d+\.?\d*)", raw_spec)
        if pm_match:
            b, o = float(pm_match.group(1)), float(pm_match.group(2))
            s_ranges.append([b - o, b + o])
        elif dev_match:
            b, p, m = float(dev_match.group(1)), float(dev_match.group(2)), float(dev_match.group(3))
            s_ranges.append([b - m, b + p])

        # 4. ğŸ’¡ é ç®—åŸºæº– (ç§»å‡ºå¾ªç’°ï¼Œæå‡ 40 å€é€Ÿåº¦)
        logic = item.get("sl", {})
        l_type = logic.get("lt", "")
        s_threshold = logic.get("t", 0)
        
        un_regen_target = None
        if "un_regen" in l_type or ("æœªå†ç”Ÿ" in (cat + title) and "è»¸é ¸" not in (cat + title)):
            cands = [n for n in clean_std if n >= 120.0]
            if s_threshold and float(s_threshold) >= 120.0: cands.append(float(s_threshold))
            if cands: un_regen_target = max(cands)

        # 5. é–‹å§‹é€ä¸€åˆ¤å®š
        for entry in raw_entries:
            if len(entry) < 2: continue
            rid, val_raw = entry[0].strip(), entry[1].strip()
            # åªå–ç¬¬ä¸€å€‹æ•¸å­—éæ¿¾æ‰‹å¯«
            v_m = re.findall(r"\d+\.?\d*", val_raw)
            val_str = v_m[0] if v_m else val_raw
            
            try:
                val = float(val_str)
                is_two_dec = "." in val_str and len(val_str.split(".")[-1]) == 2
                is_pure_int = "." not in val_str
                is_passed, reason, t_used, engine_label = True, "", "N/A", "æœªçŸ¥"

                # A. éŠ²è£œ
                if "min_limit" in l_type or "éŠ²è£œ" in (cat + title):
                    engine_label = "éŠ²è£œ"
                    if not is_pure_int: is_passed, reason = False, "æ‡‰ç‚ºç´”æ•´æ•¸"
                    elif clean_std:
                        t_used = min(clean_std, key=lambda x: abs(x - val))
                        if val < t_used: is_passed, reason = False, "æ•¸å€¼ä¸è¶³"
                # B. æœªå†ç”Ÿæœ¬é«”
                elif un_regen_target is not None:
                    engine_label = "æœªå†ç”Ÿ"
                    t_used = un_regen_target
                    if val <= t_used:
                        if not is_pure_int: is_passed, reason = False, "æ‡‰ç‚ºæ•´æ•¸"
                    elif not is_two_dec: is_passed, reason = False, "æ‡‰å¡«å…©ä½å°æ•¸"
                # C. ç²¾åŠ å·¥/å€é–“
                elif any(x in (cat + title) for x in ["å†ç”Ÿ", "ç²¾åŠ å·¥", "ç ”ç£¨", "è»Šä¿®", "çµ„è£", "æ‹†è£", "çœŸåœ“åº¦"]):
                    engine_label = "ç²¾åŠ å·¥"
                    if not is_two_dec: is_passed, reason = False, "æ‡‰å¡«å…©ä½å°æ•¸"
                    elif s_ranges:
                        t_used = str(s_ranges)
                        if not any(r[0] <= val <= r[1] for r in s_ranges): is_passed, reason = False, "ä¸åœ¨å€é–“å…§"

                if not is_passed:
                    key = (page_num, title, reason)
                    if key not in grouped_errors:
                        grouped_errors[key] = {"page": page_num, "item": title, "issue_type": f"ç•°å¸¸({engine_label})", "common_reason": reason, "failures": []}
                    grouped_errors[key]["failures"].append({"id": rid, "val": val_str, "target": f"åŸºæº–:{t_used}"})
            except: continue
    return list(grouped_errors.values())
    
def python_accounting_audit(dimension_data, res_main):
    """
    Python æœƒè¨ˆå®˜ï¼šè² è²¬æ‰€æœ‰æ•¸é‡çš„ç²¾ç¢ºå°å¸³èˆ‡é‹è²»è¨ˆç®—
    """
    accounting_issues = []
    from thefuzz import fuzz
    from collections import Counter
    import re

    # 1. å–å¾—å°å¸³åŸºæº– (ä¾†è‡ªå·¦ä¸Šè§’çµ±è¨ˆè¡¨)
    summary_rows = res_main.get("summary_rows", [])
    global_sum_tracker = {s['title']: {"target": s['target'], "actual": 0, "details": []} for s in summary_rows if s.get('title')}
    
    # ğŸ’¡ å–å¾—é‹è²»ç›®æ¨™ (å·¦ä¸Šè§’)
    freight_target = res_main.get("freight_target", 0)
    freight_actual_sum = 0
    freight_details = []

    # 2. é–‹å§‹é€é …éå¸³
    for item in dimension_data:
        title = item.get("item_title", "")
        page = item.get("page", "?")
        target_pc = item.get("item_pc_target", 0)
        rules = item.get("accounting_rules", {})
        
        # ğŸ’¡ è§£é–‹å£“ç¸®æ•¸æ“š ds
        ds = str(item.get("ds", ""))
        data_list = [pair.split(":") for pair in ds.split("|") if ":" in pair]
        if not data_list: continue
        
        # æº–å‚™ç·¨è™Ÿæ¸…å–®ç”¨æ–¼è¨ˆæ•¸èˆ‡é‡è¤‡æª¢æŸ¥
        ids = [str(e[0]).strip() for e in data_list if len(e) > 0]
        id_counts = Counter(ids)

        # --- 2.1 å–®é … PC æ•¸æ ¸å° (Local Rule) ---
        u_local = str(rules.get("local", ""))
        is_body = "æœ¬é«”" in title
        is_journal = any(k in title for k in ["è»¸é ¸", "å…§å­”", "Journal"])
        
        if "1SET=4PCS" in u_local: actual_item_qty = len(data_list) / 4
        elif "1SET=2PCS" in u_local: actual_item_qty = len(data_list) / 2
        elif is_body or "PC=PC" in u_local: actual_item_qty = len(set(ids)) # æœ¬é«”å»é‡
        else: actual_item_qty = len(data_list)

        if actual_item_qty != target_pc and target_pc > 0:
            accounting_issues.append({
                "page": page, "item": title, "issue_type": "çµ±è¨ˆä¸ç¬¦(å–®é …)",
                "common_reason": f"è¦æ±‚ {target_pc}PCï¼Œå…§æ–‡æ•¸åˆ° {actual_item_qty}",
                "failures": [{"id": "æ¨™é¡Œç›®æ¨™", "val": target_pc}, {"id": "å…§æ–‡è¨ˆæ•¸", "val": actual_item_qty}]
            })

        # --- 2.2 è»¸é ¸é‡è¤‡æ€§æª¢æŸ¥ (é™ 2 æ¬¡) ---
        if is_journal:
            for rid, count in id_counts.items():
                if count >= 3:
                    accounting_issues.append({
                        "page": page, "item": title, "issue_type": "ğŸ›‘ç·¨è™Ÿé‡è¤‡ç•°å¸¸",
                        "common_reason": f"ç·¨è™Ÿ {rid} å‡ºç¾ {count} æ¬¡ï¼Œè»¸é ¸é™ 2 æ¬¡",
                        "failures": [{"id": rid, "val": count, "calc": "ç¦æ­¢è¶…é2æ¬¡"}]
                    })

        # --- 2.3 ç¸½è¡¨å°å¸³ (Aèšåˆ/Bä¸€èˆ¬) ---
        u_agg_raw = str(rules.get("agg", ""))
        agg_multiplier = 1.0
        # è§£æå–®ä½æ›ç®—
        conv = re.search(r"(\d+)SET=1PC", u_agg_raw)
        if conv: agg_multiplier = 1.0 / float(conv.group(1))

        for s_title, data in global_sum_tracker.items():
            is_rep = any(k in s_title for k in ["ROLLè»Šä¿®"])
            is_weld = "ROLLè»Šä¿®" in s_title
            is_assem = any(k in s_title for k in ["ROLLæ‹†è£"])
            
            match = False
            # A æ¨¡å¼ï¼šèšåˆç±ƒå­
            if (is_rep or is_weld or is_assem) and "è±å…" not in u_agg_raw:
                if is_rep and any(k in title for k in ["æœªå†ç”Ÿ", "å†ç”Ÿ"]): match = True
                elif is_weld and "éŠ²è£œ" in title: match = True
                elif is_assem and any(k in title for k in ["æ‹†è£", "çµ„è£", "çœŸåœ“åº¦"]): match = True
            # B æ¨¡å¼ï¼šåå­—ç›´æ¥å°å¸³
            if not match and fuzz.partial_ratio(s_title, title) > 85: match = True

            if match:
                val_for_agg = actual_item_qty * agg_multiplier
                data["actual"] += val_for_agg
                data["details"].append({"id": f"{title} (P.{page})", "val": val_for_agg, "calc": "è¨ˆå…¥ç¸½å¸³"})

        # --- 2.4 ğŸ’¡ é‹è²»æ ¸å° (Freight Check - è£œå›) ---
        u_fr = str(rules.get("freight", ""))
        # è¦å‰‡ï¼šå…¨å·ã€Œæœ¬é«”ã€+ã€Œæœªå†ç”Ÿã€ä¹‹é …ç›®
        if "è¨ˆå…¥" in u_fr or ("æœ¬é«”" in title and "æœªå†ç”Ÿ" in title):
            if "è±å…" not in u_fr:
                freight_actual_sum += actual_item_qty
                freight_details.append({"id": f"{title} (P.{page})", "val": actual_item_qty, "calc": "è¨ˆå…¥é‹è²»"})

    # 3. çµç®—æœ€çµ‚çµæœå ±å‘Š
    # ç¸½è¡¨å ±å‘Š
    for s_title, data in global_sum_tracker.items():
        if abs(data["actual"] - data["target"]) > 0.01 and data["target"] > 0:
            accounting_issues.append({
                "page": "ç¸½è¡¨", "item": s_title, "issue_type": "çµ±è¨ˆä¸ç¬¦(ç¸½å¸³)",
                "common_reason": f"æ¨™è¨» {data['target']} != å¯¦éš› {data['actual']}",
                "failures": [{"id": "ğŸ” çµ±è¨ˆåŸºæº–", "val": data["target"]}] + data["details"] + [{"id": "ğŸ§® å¯¦éš›ç¸½è¨ˆ", "val": data["actual"]}]
            })

    # é‹è²»å ±å‘Š
    if abs(freight_actual_sum - freight_target) > 0.01 and freight_target > 0:
        accounting_issues.append({
            "page": "ç¸½è¡¨", "item": "é‹è²»æ ¸å°", "issue_type": "çµ±è¨ˆä¸ç¬¦(é‹è²»)",
            "common_reason": f"é‹è²»åŸºæº– {freight_target} != å¯¦éš› {freight_actual_sum}",
            "failures": [{"id": "ğŸšš é‹è²»åŸºæº–", "val": freight_target}] + freight_details + [{"id": "ğŸ§® é‹è²»ç¸½è¨ˆ", "val": freight_actual_sum}]
        })
        
    return accounting_issues
    
# --- 6. æ‰‹æ©Ÿç‰ˆ UI èˆ‡ æ ¸å¿ƒåŸ·è¡Œé‚è¼¯ ---
st.title("ğŸ­ äº¤è²¨å–®ç¨½æ ¸")

data_source = st.radio(
    "è«‹é¸æ“‡è³‡æ–™ä¾†æºï¼š", 
    ["ğŸ“¸ ä¸Šå‚³ç…§ç‰‡", "ğŸ“‚ ä¸Šå‚³ JSON æª”", "ğŸ“Š ä¸Šå‚³ Excel æª”"], 
    horizontal=True
)

with st.container(border=True):
    # --- æƒ…æ³ A: ä¸Šå‚³ç…§ç‰‡ ---
    if data_source == "ğŸ“¸ ä¸Šå‚³ç…§ç‰‡":
        if st.session_state.get('source_mode') == 'json' or st.session_state.get('source_mode') == 'excel':
            st.session_state.photo_gallery = []
            st.session_state.source_mode = 'image'

        uploaded_files = st.file_uploader(
            "è«‹é¸æ“‡ JPG/PNG ç…§ç‰‡...", 
            type=['jpg', 'png', 'jpeg'], 
            accept_multiple_files=True, 
            key=f"uploader_{st.session_state.uploader_key}"
        )
        
        if uploaded_files:
            for f in uploaded_files: 
                if not any(x['file'].name == f.name for x in st.session_state.photo_gallery if x['file']):
                    st.session_state.photo_gallery.append({
                        'file': f, 
                        'table_md': None, 
                        'header_text': None,
                        'full_text': None,
                        'raw_json': None
                    })
            st.session_state.uploader_key += 1
            if st.session_state.enable_auto_analysis:
                st.session_state.auto_start_analysis = True
            components.html("""<script>window.parent.document.body.scrollTo(0, window.parent.document.body.scrollHeight);</script>""", height=0)
            st.rerun()

    # --- æƒ…æ³ B: ä¸Šå‚³ JSON ---
    elif data_source == "ğŸ“‚ ä¸Šå‚³ JSON æª”":
        st.info("ğŸ’¡ è«‹é»æ“Šä¸‹æ–¹æŒ‰éˆ•ï¼Œå¾ä½ çš„è³‡æ–™å¤¾é¸æ“‡ä¹‹å‰ä¸‹è¼‰çš„ `.json` æª”ã€‚")
        uploaded_json = st.file_uploader("ä¸Šå‚³JSONæª”", type=['json'], key="json_uploader")
        
        if uploaded_json:
            try:
                current_file_name = uploaded_json.name
                if st.session_state.get('last_loaded_json_name') != current_file_name:
                    json_data = json.load(uploaded_json)
                    st.session_state.photo_gallery = []
                    st.session_state.source_mode = 'json'
                    st.session_state.last_loaded_json_name = current_file_name
                    
                    import re
                    for page in json_data:
                        real_page = "Unknown"
                        full_text = page.get('full_text', '')
                        if full_text:
                            match = re.search(r"(?:é …æ¬¡|Page|é æ¬¡|NO\.)[:\s]*(\d+)\s*[/ï¼]\s*\d+", full_text, re.IGNORECASE)
                            if match:
                                real_page = match.group(1)
                        
                        st.session_state.photo_gallery.append({
                            'file': None,
                            'table_md': page.get('table_md'),
                            'header_text': page.get('header_text'),
                            'full_text': full_text,
                            'raw_json': page.get('raw_json'),
                            'real_page': real_page
                        })
                    
                    st.toast(f"âœ… æˆåŠŸè¼‰å…¥ JSON: {current_file_name}", icon="ğŸ“‚")
                    if st.session_state.enable_auto_analysis:
                        st.session_state.auto_start_analysis = True
                    st.rerun()
                else:
                    st.success(f"ğŸ“‚ ç›®å‰è¼‰å…¥ JSONï¼š**{uploaded_json.name}**")
            except Exception as e:
                st.error(f"JSON æª”æ¡ˆæ ¼å¼éŒ¯èª¤: {e}")

    # --- æƒ…æ³ C: ä¸Šå‚³ Excel (æ–°å¢çš„æ”¾åœ¨é€™) ---
    elif data_source == "ğŸ“Š ä¸Šå‚³ Excel æª”":
        st.info("ğŸ’¡ ä¸Šå‚³ Excel æª”å¾Œï¼Œç³»çµ±æœƒå°‡è¡¨æ ¼å…§å®¹è½‰æ›ç‚ºæ–‡å­—ä¾› AI ç¨½æ ¸ã€‚")
        uploaded_xlsx = st.file_uploader("ä¸Šå‚³ Excel æª”", type=['xlsx', 'xls'], key="xlsx_uploader")
        
        if uploaded_xlsx:
            try:
                current_file_name = uploaded_xlsx.name
                if st.session_state.get('last_loaded_xlsx_name') != current_file_name:
                    df_dict = pd.read_excel(uploaded_xlsx, sheet_name=None)
                    st.session_state.photo_gallery = []
                    st.session_state.source_mode = 'excel'
                    st.session_state.last_loaded_xlsx_name = current_file_name
                    
                    for sheet_name, df in df_dict.items():
                        df = df.fillna("")
                        md_table = df.to_markdown(index=False)
                        st.session_state.photo_gallery.append({
                            'file': None,
                            'table_md': md_table,
                            'header_text': f"ä¾†æºåˆ†é : {sheet_name}",
                            'full_text': f"Excel å…§å®¹ - åˆ†é  {sheet_name}\n" + md_table,
                            'raw_json': None,
                            'real_page': sheet_name
                        })
                    st.toast(f"âœ… æˆåŠŸè¼‰å…¥ Excel: {current_file_name}", icon="ğŸ“Š")
                    if st.session_state.enable_auto_analysis:
                        st.session_state.auto_start_analysis = True
                    st.rerun()
                else:
                    st.success(f"ğŸ“Š ç›®å‰è¼‰å…¥ Excelï¼š**{uploaded_xlsx.name}**")
            except Exception as e:
                st.error(f"Excel è®€å–å¤±æ•—: {e}")

if st.session_state.photo_gallery:
    st.caption(f"å·²ç´¯ç© {len(st.session_state.photo_gallery)} é æ–‡ä»¶")
    col_btn1, col_btn2 = st.columns([1, 1], gap="small")
    with col_btn1: start_btn = st.button("ğŸš€ é–‹å§‹åˆ†æ", type="primary", use_container_width=True)
    with col_btn2: 
        clear_btn = st.button("ğŸ—‘ï¸ç…§ç‰‡æ¸…é™¤", help="æ¸…é™¤", use_container_width=True)

    if clear_btn:
        st.session_state.photo_gallery = []
        st.session_state.analysis_result_cache = None
        if 'last_loaded_json_name' in st.session_state:
            del st.session_state.last_loaded_json_name 
        st.rerun()

    is_auto_start = st.session_state.auto_start_analysis
    if is_auto_start:
        st.session_state.auto_start_analysis = False

    if 'analysis_result_cache' not in st.session_state:
        st.session_state.analysis_result_cache = None

    trigger_analysis = start_btn or is_auto_start

    if trigger_analysis:
        total_start = time.time()
        status = st.empty()
        progress_bar = st.progress(0)
            
        extracted_data_list = [None] * len(st.session_state.photo_gallery)
        full_text_for_search = ""
        total_imgs = len(st.session_state.photo_gallery)
            
        ocr_start = time.time()

        def process_image_task(index, item):
            index = int(index)
            # å¦‚æœå·²ç¶“æœ‰è³‡æ–™äº†å°±ä¸é‡è¤‡æƒæ
            if item.get('table_md') and item.get('header_text') and item.get('full_text'):
                real_page = item.get('real_page', str(index + 1))
                return index, item['table_md'], item['header_text'], item['full_text'], None, real_page, None
    
            try:
                if item.get('file') is None:
                    return index, None, None, None, None, None, "ç„¡åœ–ç‰‡æª”æ¡ˆ"
                
                item['file'].seek(0)
                # é€™è£¡æœƒæ¥åˆ°æˆ‘å€‘å‰›æ‰ä¿®æ”¹å¾Œå›å‚³çš„ None
                table_md, header, full, _, real_page = extract_layout_with_azure(item['file'], DOC_ENDPOINT, DOC_KEY)
                return index, table_md, header, full, None, real_page, None
            except Exception as e:
                return index, None, None, None, None, None, f"OCRå¤±æ•—: {str(e)}"

        status.text(f"Azure æ­£åœ¨å¹³è¡Œæƒæ {total_imgs} é æ–‡ä»¶...")

        with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
            futures = []
            for i, item in enumerate(st.session_state.photo_gallery):
                futures.append(executor.submit(process_image_task, i, item))
            
            completed_count = 0
            for future in concurrent.futures.as_completed(futures):
                idx, t_md, h_txt, f_txt, raw_j, r_page, err = future.result()
                idx = int(idx)
                
                if err:
                    st.error(f"ç¬¬ {idx+1} é è®€å–å¤±æ•—: {err}")
                    extracted_data_list[idx] = None
                else:
                    st.session_state.photo_gallery[idx]['table_md'] = t_md
                    st.session_state.photo_gallery[idx]['header_text'] = h_txt
                    st.session_state.photo_gallery[idx]['full_text'] = f_txt
                    st.session_state.photo_gallery[idx]['raw_json'] = raw_j
                    st.session_state.photo_gallery[idx]['real_page'] = r_page
                    st.session_state.photo_gallery[idx]['file'] = None
                    
                    extracted_data_list[idx] = {
                        "page": r_page,
                        "table": t_md or "", 
                        "header_text": h_txt or ""
                    }
                
                completed_count += 1
                progress_bar.progress(completed_count / (total_imgs + 1))
        
        for i, data in enumerate(extracted_data_list):
            if data and isinstance(data, dict):
                page_idx = i
                if 0 <= page_idx < len(st.session_state.photo_gallery):
                    full_text_for_search += st.session_state.photo_gallery[page_idx].get('full_text', '')

        ocr_end = time.time()
        ocr_duration = ocr_end - ocr_start

        combined_input = "ä»¥ä¸‹æ˜¯å„é è³‡æ–™ï¼š\n"
        for i, data in enumerate(extracted_data_list):
            if data is None: continue
            page_num = data.get('page', i+1)
            table_text = data.get('table', '')
            header_text = data.get('header_text', '')
            combined_input += f"\n=== Page {page_num} ===\nã€é é¦–ã€‘:\n{header_text}\nã€è¡¨æ ¼ã€‘:\n{table_text}\n"
            
        status.text("ç¸½ç¨½æ ¸ Agent æ­£åœ¨é€²è¡Œå…¨æ–¹ä½åˆ†æ...")
        
        # 1. åŸ·è¡Œ AI 
        res_main = agent_unified_check(combined_input, combined_input, GEMINI_KEY, main_model_name)
        
        # ğŸ’¡ [é‡å¤§ä¿®æ­£]ï¼šå¾ AI å›å‚³ä¸­æŠ“å–ç¶­åº¦æ•¸æ“š
        dim_data = res_main.get("dimension_data", [])
        
        # 2. åŸ·è¡Œä¸‰å€‹ Python å¼•æ“ (æ•¸å€¼ã€æœƒè¨ˆã€æµç¨‹)
        python_numeric_issues = python_numerical_audit(dim_data)
        python_accounting_issues = python_accounting_audit(dim_data, res_main)
        
        # ğŸ’¡ [æ–°å¢]ï¼šå•Ÿå‹• Python æµç¨‹ç¨½æ ¸å¼•æ“
        python_process_issues = python_process_audit(dim_data)
        
        # 3. åˆä½µçµæœ (å¸¶æœ‰é˜²å‘†æª¢æŸ¥ï¼Œä¸¦ç¢ºä¿æ¬ŠåŠ›å¾¹åº•ç§»äº¤) ---
        ai_raw_issues = res_main.get("issues", [])
        ai_filtered_issues = []

        if isinstance(ai_raw_issues, list):
            for i in ai_raw_issues:
                if isinstance(i, dict):
                    i['source'] = 'ğŸ¤– ç¸½ç¨½æ ¸ AI'
                    i_type = str(i.get("issue_type", ""))
                    
                    # ğŸ’¡ [é—œéµä¿®æ­£]ï¼š
                    # æˆ‘å€‘åªä¿ç•™ AI ç™¼ç¾çš„ï¼šè¦æ ¼æå–å¤±æ•—ã€æœªåŒ¹é…è¦å‰‡ã€é‚„æœ‰è¡¨é ­è³‡è¨Šä¸ç¬¦ã€‚
                    # ã€Œæµç¨‹ã€å’Œã€Œçµ±è¨ˆã€å·²ç¶“å®Œå…¨äº¤çµ¦ Python å¼•æ“äº†ï¼Œæ‰€ä»¥é€™è£¡çµ•å°ä¸ç•™ AI å ±çš„ã€‚
                    ai_tasks_to_keep = ["è¦æ ¼æå–å¤±æ•—", "æœªåŒ¹é…", "è¡¨é ­"]
                    if any(k in i_type for k in ai_tasks_to_keep):
                        ai_filtered_issues.append(i)
                else:
                    # å¦‚æœ AI å›å‚³æ ¼å¼å´©æ½°ï¼Œè‡³å°‘ä¿ç•™åŸå§‹æ–‡å­—ä¾›æª¢æŸ¥
                    ai_filtered_issues.append({
                        "page": "?", "item": "AI å›å‚³è§£æç•°å¸¸", "issue_type": "âš ï¸æ ¼å¼éŒ¯èª¤",
                        "common_reason": f"åŸå§‹å…§å®¹: {str(i)}", "source": "ğŸ¤– ç¸½ç¨½æ ¸ AI"
                    })

        # 4. å–å¾— Python è¡¨é ­æª¢æŸ¥ (æ—¥æœŸã€å·¥ä»¤ç­‰)
        python_header_issues, python_debug_data = python_header_check(st.session_state.photo_gallery)
        
        # æœ€çµ‚åˆä½µï¼šAI(æå–è­¦å‘Š) + Python(æ•¸å€¼) + Python(æœƒè¨ˆ) + Python(æµç¨‹) + Python(è¡¨é ­)
        all_issues = ai_filtered_issues + python_numeric_issues + python_accounting_issues + python_process_issues + python_header_issues
        
        # 5. å­˜å…¥å¿«å– (é€™æ˜¯ Debug é é¢èƒ½é¡¯ç¤ºæ•¸æ“šçš„å”¯ä¸€é—œéµ)
        st.session_state.analysis_result_cache = {
            "job_no": res_main.get("job_no", "Unknown"),
            "all_issues": all_issues,
            "total_duration": time.time() - total_start,
            "cost_twd": (res_main.get("_token_usage",{}).get("input",0)*0.5 + res_main.get("_token_usage",{}).get("output",0)*3.0)/1000000*32.5,
            "total_in": res_main.get("_token_usage",{}).get("input", 0),
            "total_out": res_main.get("_token_usage",{}).get("output", 0),
            "ocr_duration": ocr_duration,
            "time_eng": time.time() - total_start - ocr_duration,
            "full_text_for_search": combined_input,
            "combined_input": combined_input,
            "python_debug_data": python_debug_data,
            # âœ… é€™è¡Œæ²’åŠ ï¼ŒDebug é é¢å°±æ˜¯ç©ºçš„ï¼
            "ai_extracted_data": dim_data 
        }
        
    if st.session_state.analysis_result_cache:
        cache = st.session_state.analysis_result_cache
        all_issues = cache['all_issues']
        
        st.success(f"å·¥ä»¤: {cache['job_no']} | â±ï¸ {cache['total_duration']:.1f}s")
        st.info(f"ğŸ’° æœ¬æ¬¡æˆæœ¬: NT$ {cache['cost_twd']:.2f} (In: {cache['total_in']:,} / Out: {cache['total_out']:,})")
        st.caption(f"ç´°ç¯€è€—æ™‚: Azure OCR {cache['ocr_duration']:.1f}s | AI åˆ†æ {cache['time_eng']:.1f}s")
        
        with st.expander("ğŸ” æŸ¥çœ‹ AI è®€å–åˆ°çš„ Excel è¦å‰‡ (Debug)"):
            rules_text = get_dynamic_rules(cache['full_text_for_search'], debug_mode=True)
            if "ç„¡ç‰¹å®šè¦å‰‡" in rules_text:
                st.caption("ç„¡åŒ¹é…è¦å‰‡")
            else:
                st.markdown(rules_text)
                
        # --- æ–°å¢çš„ Debug å±•é–‹é  ---
        with st.expander("ğŸ”¬ æŸ¥çœ‹ AI æŠ„éŒ„çµ¦ Python çš„åŸå§‹æ•¸æ“š (æª¢æŸ¥æ‰‹å¯«éæ¿¾)", expanded=False):
            raw_dim_data = cache.get("ai_extracted_data", [])
            if raw_dim_data:
                st.write("é€™æ˜¯ AI æŠ„éŒ„ä¸¦ç¿»è­¯å¾Œçš„ JSONï¼ˆåŒ…å«æ ¼å¼æ˜¯å¦æ­£ç¢ºã€æ•¸å­—æ˜¯å¦è¢«ç°¡åŒ–ï¼‰ï¼š")
                st.json(raw_dim_data)
            else:
                st.caption("ç„¡æ•¸æ“šæå–è³‡æ–™ã€‚")

        with st.expander("ğŸ æŸ¥çœ‹ Python ç¡¬é‚è¼¯åµæ¸¬çµæœ (Debug)", expanded=False):
            if cache.get('python_debug_data'):
                p_data = cache['python_debug_data']
                standard_data = {}
                all_values = {"å·¥ä»¤ç·¨è™Ÿ": [], "é å®šäº¤è²¨": [], "å¯¦éš›äº¤è²¨": []}
                for page in p_data:
                    for k in all_values.keys():
                        if page.get(k) and page[k] != "N/A":
                            all_values[k].append(page[k])
                
                standard_row = {"é ç¢¼": "ğŸ† åˆ¤å®šæ¨™æº–"}
                for k, v in all_values.items():
                    if v:
                        standard_row[k] = Counter(v).most_common(1)[0][0]
                    else:
                        standard_row[k] = "N/A"
                
                final_df_data = [standard_row] + p_data
                st.dataframe(final_df_data, use_container_width=True, hide_index=True)
                st.info("ğŸ’¡ ã€Œåˆ¤å®šæ¨™æº–ã€æ˜¯ä¾æ“šå¤šæ•¸æ±ºç”¢ç”Ÿçš„ã€‚")
            else:
                st.caption("ç„¡åµæ¸¬è³‡æ–™")

        real_errors = [i for i in all_issues if "æœªåŒ¹é…" not in i.get('issue_type', '')]
        
        if not real_errors:
            st.balloons()
            if not all_issues:
                st.success("âœ… å…¨æ•¸åˆæ ¼ï¼")
            else:
                st.success(f"âœ… æ•¸å€¼å…¨æ•¸åˆæ ¼ï¼ (ä½†æœ‰ {len(all_issues)} å€‹é …ç›®æœªåŒ¹é…è¦å‰‡ï¼Œè«‹æª¢æŸ¥)")
        else:
            st.error(f"ç™¼ç¾ {len(real_errors)} é¡æ•¸å€¼ç•°å¸¸ï¼Œå¦æœ‰ {len(all_issues) - len(real_errors)} å€‹é …ç›®æœªåŒ¹é…è¦å‰‡")

        for item in all_issues:
            with st.container(border=True):
                c1, c2 = st.columns([3, 1])
                
                source_label = item.get('source', '')
                issue_type = item.get('issue_type', 'ç•°å¸¸')
                
                c1.markdown(f"**P.{item.get('page', '?')} | {item.get('item')}**  `{source_label}`")
                
                # é¡è‰²æ§åˆ¶ï¼šæœƒè¨ˆçµ±è¨ˆé¡ç”¨ç´…è‰²ï¼Œè¦æ ¼é¡ç”¨é»ƒè‰²
                if "çµ±è¨ˆ" in issue_type or "æ•¸é‡" in issue_type or "æµç¨‹" in issue_type:
                    c2.error(f"ğŸ›‘ {issue_type}")
                else:
                    c2.warning(f"âš ï¸ {issue_type}")
                
                st.caption(f"åŸå› : {item.get('common_reason', '')}")
                
                # --- æ¸²æŸ“è¡¨æ ¼ (æœƒè¨ˆå°å¸³å–®) ---
                failures = item.get('failures', [])
                if failures:
                    table_data = []
                    for f in failures:
                        if isinstance(f, dict):
                            # æˆ‘å€‘çµ±ä¸€ä½¿ç”¨é€™å››å€‹æ¬„ä½æ¨™é¡Œï¼Œæœƒè¨ˆèˆ‡å·¥ç¨‹å…±ç”¨
                            row = {
                                "é …ç›®/æ»¾è¼ªç·¨è™Ÿ": f.get('id', 'æœªçŸ¥'), 
                                "å¯¦æ¸¬/è¨ˆæ•¸": f.get('val', 'N/A'),
                                "æ¨™æº–/å‚™è¨»": f.get('target', ''), # å·¥ç¨‹ç”¨
                                "åˆ¤å®šç®—å¼/ç‹€æ…‹": f.get('calc', '') # æœƒè¨ˆç”¨
                            }
                            # å¦‚æœæ˜¯æœƒè¨ˆæ¨¡å¼ï¼ŒæŠŠ target ç•™ç©ºï¼Œè³‡è¨Šä¸»è¦åœ¨ id å’Œ val
                            table_data.append(row)
                    
                    if table_data:
                        st.dataframe(table_data, use_container_width=True, hide_index=True)
                else:
                    # å¦‚æœæ²’æœ‰ failuresï¼Œè‡³å°‘é¡¯ç¤ºä¸€å€‹æ•¸æ“šæç¤º
                    st.info(f"è©³ç´°æ•¸æ“šè¦‹ä¸Šè¿°åŸå› èªªæ˜")
        
        st.divider()

        current_job_no = cache.get('job_no', 'Unknown')
        safe_job_no = current_job_no.replace("/", "_").replace("\\", "_").strip()
        file_name_str = f"{safe_job_no}_cleaned.json"

        # æº–å‚™åŒ¯å‡ºè³‡æ–™
        export_data = []
        for item in st.session_state.photo_gallery:
            export_data.append({
                "table_md": item.get('table_md'),
                "header_text": item.get('header_text'),
                "full_text": item.get('full_text'),
                "raw_json": item.get('raw_json')
            })
        json_str = json.dumps(export_data, indent=2, ensure_ascii=False)

        st.subheader("ğŸ’¾ æ¸¬è©¦è³‡æ–™å­˜æª”")
        st.caption(f"å·²è­˜åˆ¥å·¥ä»¤ï¼š**{current_job_no}**ã€‚ä¸‹è¼‰å¾Œå¯ä¾›ä¸‹æ¬¡æ¸¬è©¦ä½¿ç”¨ã€‚")
        
        st.download_button(
            label=f"â¬‡ï¸ ä¸‹è¼‰æ¸¬è©¦è³‡æ–™ ({file_name_str})",
            data=json_str,
            file_name=file_name_str,
            mime="application/json",
            type="primary"
        )

        with st.expander("ğŸ‘€ æŸ¥çœ‹å‚³çµ¦ AI çš„æœ€çµ‚æ–‡å­— (Prompt Input)"):
            st.caption("é€™æ‰æ˜¯ AI çœŸæ­£è®€åˆ°çš„å…§å®¹ (å·²éæ¿¾é›œè¨Š)ï¼š")
            st.code(cache['combined_input'], language='markdown')
    
    if st.session_state.photo_gallery and st.session_state.get('source_mode') != 'json':
        st.caption("å·²æ‹æ”ç…§ç‰‡ï¼š")
        cols = st.columns(4)
        for idx, item in enumerate(st.session_state.photo_gallery):
            with cols[idx % 4]:
                if item.get('file'):
                    st.image(item['file'], caption=f"P.{idx+1}", use_container_width=True)
                if st.button("âŒ", key=f"del_{idx}"):
                    st.session_state.photo_gallery.pop(idx)
                    st.session_state.analysis_result_cache = None
                    st.rerun()
else:
    st.info("ğŸ‘† è«‹é»æ“Šä¸Šæ–¹æŒ‰éˆ•é–‹å§‹æ–°å¢ç…§ç‰‡")
