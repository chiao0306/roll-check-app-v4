import streamlit as st
import streamlit.components.v1 as components
from azure.core.credentials import AzureKeyCredential
from azure.ai.documentintelligence import DocumentIntelligenceClient
from azure.ai.documentintelligence.models import AnalyzeResult
import google.generativeai as genai
from openai import OpenAI
import json
import time
import concurrent.futures
import pandas as pd
from thefuzz import fuzz
from collections import Counter
import re

# --- 1. é é¢è¨­å®š ---
st.set_page_config(page_title="äº¤è²¨å–®ç¨½æ ¸", page_icon="ğŸ­", layout="centered")

# --- CSS æ¨£å¼ ---
st.markdown("""
<style>
/* 1. æ¨™é¡Œå¤§å°æ§åˆ¶ */
h1 {
    font-size: 1.7rem !important; 
    white-space: nowrap !important;
    overflow: hidden !important; 
    text-overflow: ellipsis !important;
}

/* 2. ä¸»åŠŸèƒ½æŒ‰éˆ• (ç´…è‰² Primary) -> è®Šå¤§ã€è®Šé«˜ */
/* é€™æœƒå½±éŸ¿ã€Œé–‹å§‹åˆ†æã€å’Œã€Œç…§ç‰‡æ¸…é™¤ã€ */
button[kind="primary"] {
    height: 60px;               
    font-size: 20px !important; 
    font-weight: bold !important;
    border-radius: 10px !important;
    margin-top: 0px !important;    
    margin-bottom: 5px !important; 
    width: 100%;                
}

/* 3. æ¬¡è¦æŒ‰éˆ• (ç°è‰² Secondary) -> ä¿æŒåŸç‹€ */
/* é€™æœƒå½±éŸ¿æ¯ä¸€å¼µç…§ç‰‡ä¸‹é¢çš„ã€ŒXã€æŒ‰éˆ•ï¼Œè®“å®ƒç¶­æŒå°å°çš„ */
button[kind="secondary"] {
    height: auto !important;
    font-weight: normal !important;
}
</style>
""", unsafe_allow_html=True)
# --- 2. ç§˜å¯†é‡‘é‘°è®€å– ---
try:
    DOC_ENDPOINT = st.secrets["DOC_ENDPOINT"]
    DOC_KEY = st.secrets["DOC_KEY"]
    GEMINI_KEY = st.secrets["GEMINI_KEY"]
    OPENAI_KEY = st.secrets.get("OPENAI_KEY", "")
except:
    st.error("æ‰¾ä¸åˆ°é‡‘é‘°ï¼è«‹åœ¨ Streamlit Cloud è¨­å®š Secretsã€‚")
    st.stop()

# --- 3. åˆå§‹åŒ– Session State ---
if 'photo_gallery' not in st.session_state: st.session_state.photo_gallery = []
if 'uploader_key' not in st.session_state: st.session_state.uploader_key = 0
if 'auto_start_analysis' not in st.session_state: st.session_state.auto_start_analysis = False

# --- å´é‚Šæ¬„æ¨¡å‹è¨­å®š (åˆä½µç‚ºå–®ä¸€é¸æ“‡) ---
with st.sidebar:
    st.header("æ¨¡å‹è¨­å®š")
    
    # é€™è£¡åŠ å…¥æœ€æ–°çš„ Gemini æ¨¡å‹
    model_options = {
        "Gemini 3 Flash preview": "gemini-3-flash-preview",
        "Gemini 2.5 Flash": "models/gemini-2.5-flash",
        "Gemini 2.5 Flash Lite": "gemini-2.5-flash-lite",
        "Gemini 2.5 Pro": "models/gemini-2.5-pro",
        #"GPT-5(ç„¡æ•ˆ)": "models/gpt-5",
        #"GPT-5 Mini(ç„¡æ•ˆ)": "models/gpt-5-mini",
    }
    options_list = list(model_options.keys())
    
    st.subheader("ğŸ¤– ç¸½ç¨½æ ¸ Agent")
    model_selection = st.selectbox(
        "è² è²¬ï¼šè¦æ ¼ã€è£½ç¨‹ã€æ•¸é‡ã€çµ±è¨ˆå…¨åŒ…", 
        options=options_list, 
        index=1, 
        key="main_model"
    )
    main_model_name = model_options[model_selection]
    
    st.divider()
    
    default_auto = st.query_params.get("auto", "true") == "true"
    def update_url_param():
        current_state = "true" if st.session_state.enable_auto_analysis else "false"
        st.query_params["auto"] = current_state

    st.toggle(
        "âš¡ ä¸Šå‚³å¾Œè‡ªå‹•åˆ†æ", 
        value=default_auto, 
        key="enable_auto_analysis", 
        on_change=update_url_param
    )

# --- Excel è¦å‰‡è®€å–å‡½æ•¸ (å–®ä¸€ä»£ç†æ•´åˆç‰ˆ) ---
@st.cache_data
def get_dynamic_rules(ocr_text, debug_mode=False):
    try:
        df = pd.read_excel("rules.xlsx")
        df.columns = [c.strip() for c in df.columns]
        ocr_text_clean = str(ocr_text).upper().replace(" ", "").replace("\n", "")
        specific_rules = []

        for index, row in df.iterrows():
            item_name = str(row.get('Item_Name', '')).strip()
            # ğŸ’¡ è·³éåŸæœ¬çš„ã€Œ(é€šç”¨)ã€é …ç›®ï¼ŒåªæŠ“ç‰¹è¦
            if not item_name or "(é€šç”¨)" in item_name: continue
            
            # ä½¿ç”¨æ¨¡ç³ŠåŒ¹é…åˆ¤æ–·æ˜¯å¦ç‚ºç•¶å‰è™•ç†çš„é …ç›®
            score = fuzz.partial_ratio(item_name.upper().replace(" ", ""), ocr_text_clean)
            if score >= 85:
                # æå–ç‰¹è¦è³‡è¨Š
                spec = str(row.get('Standard_Spec', ''))
                logic = str(row.get('Logic_Prompt', ''))
                u_local = str(row.get('Unit_Rule_Local', ''))
                u_agg = str(row.get('Unit_Rule_Agg', ''))
                u_freight = str(row.get('Unit_Rule_Freight', ''))
                
                desc = f"- **[ç‰¹å®šé …ç›®è¦å‰‡] {item_name}**\n"
                if spec != 'nan' and spec: desc += f"  - [å¼·åˆ¶è¦æ ¼]: {spec}\n"
                if logic != 'nan' and logic: desc += f"  - [ä¾‹å¤–æŒ‡ä»¤]: {logic}\n"
                if u_local != 'nan' and u_local: desc += f"  - [æœƒè¨ˆå–®é …]: {u_local}\n"
                if u_agg != 'nan' and u_agg: desc += f"  - [æœƒè¨ˆèšåˆ]: {u_agg}\n"
                if u_freight != 'nan' and u_freight: desc += f"  - [æœƒè¨ˆé‹è²»]: {u_freight}\n"
                specific_rules.append(desc)
        
        return "\n".join(specific_rules) if specific_rules else "ç„¡ç‰¹å®šå°ˆæ¡ˆè¦å‰‡ï¼Œè«‹ä¾ç…§é€šç”¨æ†²æ³•åŸ·è¡Œã€‚"
    except Exception as e:
        return f"è®€å–è¦å‰‡æª”æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}"
        
# --- 4. æ ¸å¿ƒå‡½æ•¸ï¼šAzure ç¥ä¹‹çœ¼ ---
def extract_layout_with_azure(file_obj, endpoint, key):
    client = DocumentIntelligenceClient(endpoint=endpoint, credential=AzureKeyCredential(key))
    file_content = file_obj.getvalue()
    
    poller = client.begin_analyze_document("prebuilt-layout", file_content, content_type="application/octet-stream")
    result: AnalyzeResult = poller.result()
    
    markdown_output = ""
    full_content_text = ""
    real_page_num = "Unknown"
    
    bottom_stop_keywords = ["æ³¨æ„äº‹é …", "ä¸­æ©Ÿå“æª¢å–®ä½", "ä¿å­˜æœŸé™", "è¡¨å–®ç·¨è™Ÿ", "FORM NO", "ç°½ç« "]
    top_right_noise_keywords = [
        "æª¢é©—é¡åˆ¥", "å°ºå¯¸æª¢é©—", "ä¾åœ–é¢æ¨™è¨˜", "ææ–™æª¢é©—", "æˆä»½åˆ†æ", 
        "éç ´å£æ€§", "æ­£å¸¸åŒ–", "é€€ç«", "æ·¬.å›ç«", "è¡¨é¢ç¡¬åŒ–", "è©¦è»Š",
        "æ€§èƒ½æ¸¬è©¦", "è©¦å£“è©¦æ¼", "å‹•.éœå¹³è¡¡è©¦é©—", ":selected:", ":unselected:",
        "æŠ—æ‹‰", "ç¡¬åº¦è©¦é©—", "UT", "PT", "MT"
    ]
    
    if result.tables:
        for idx, table in enumerate(result.tables):
            page_num = "Unknown"
            if table.bounding_regions: page_num = table.bounding_regions[0].page_number
            markdown_output += f"\n### Table {idx + 1} (Page {page_num}):\n"
            rows = {}
            stop_processing_table = False 
            
            for cell in table.cells:
                if stop_processing_table: break
                content = cell.content.replace("\n", " ").strip()
                
                for kw in bottom_stop_keywords:
                    if kw in content:
                        stop_processing_table = True
                        break
                if stop_processing_table: break
                
                is_noise = False
                for kw in top_right_noise_keywords:
                    if kw in content:
                        is_noise = True
                        break
                if is_noise: content = "" 

                r, c = cell.row_index, cell.column_index
                if r not in rows: rows[r] = {}
                rows[r][c] = content
            
            for r in sorted(rows.keys()):
                row_cells = []
                if rows[r]:
                    max_col = max(rows[r].keys())
                    for c in range(max_col + 1): 
                        row_cells.append(rows[r].get(c, ""))
                    markdown_output += "| " + " | ".join(row_cells) + " |\n"
    
    if result.content:
        match = re.search(r"(?:é …æ¬¡|Page|é æ¬¡|NO\.)[:\s]*(\d+)\s*[/ï¼]\s*\d+", result.content, re.IGNORECASE)
        if match:
            real_page_num = match.group(1)

        cut_index = len(result.content)
        for keyword in bottom_stop_keywords:
            idx = result.content.find(keyword)
            if idx != -1 and idx < cut_index:
                cut_index = idx
        
        temp_text = result.content[:cut_index]
        for noise in top_right_noise_keywords:
            temp_text = temp_text.replace(noise, "")
            
        full_content_text = temp_text
        header_snippet = full_content_text[:800]
    else:
        full_content_text = ""
        header_snippet = ""

    return markdown_output, header_snippet, full_content_text, None, real_page_num

# --- Python ç¡¬é‚è¼¯ï¼šè¡¨é ­ä¸€è‡´æ€§æª¢æŸ¥ (é•·åº¦æ•æ„Ÿç‰ˆ) ---
def python_header_check(photo_gallery):
    issues = []
    if not photo_gallery:
        return issues, []

    # å®šç¾© Regex (é‡å° "å»ç©ºç™½+å»æ›è¡Œ" å¾Œçš„å­—ä¸²è¨­è¨ˆ)
    patterns = {
        # ã€ä¿®æ”¹é» 1ã€‘å·¥ä»¤ Regex æ”¾å¯¬ï¼š
        # åŸæœ¬åªæŠ“ W é–‹é ­ï¼Œç¾åœ¨æ”¹æŠ“ "ç·¨è™Ÿ" å¾Œé¢æ¥çš„ "ä»»ä½•è‹±æ•¸å­—ä¸²"
        # é€™æ¨£å°±ç®—å®ƒå¯« WW363... æˆ–æ˜¯ 12345... éƒ½èƒ½æ•´ä¸²æŠ“å‡ºä¾†æ¯”å°
        "å·¥ä»¤ç·¨è™Ÿ": r"[å·¥åœŸä¸‹][ä»¤å†·ä»Š]ç·¨è™Ÿ[:\.]*([A-Za-z0-9\-\_]+)", 
        
        "é å®šäº¤è²¨": r"[é é¢„é …é ‚][å®šäº¤].*?(\d{2,4}[\.\-/]\d{1,2}[\.\-/]\d{1,2})",
        "å¯¦éš›äº¤è²¨": r"[å¯¦çœŸ][éš›äº¤].*?(\d{2,4}[\.\-/]\d{1,2}[\.\-/]\d{1,2})"
    }

    extracted_data = [] 
    all_values = {key: [] for key in patterns}

    for i, page in enumerate(photo_gallery):
        # æš´åŠ›æ¸…æ´—ï¼šå»æ›è¡Œã€å»ç©ºæ ¼ã€è½‰å¤§å¯«
        raw_text = page.get('header_text', '') + page.get('full_text', '')
        clean_text = raw_text.replace("\n", "").replace(" ", "").replace("\r", "").upper()
        
        # ã€ä¿®æ”¹é» 2ã€‘é ç¢¼é˜²å‘†ï¼šç¢ºä¿ä¸€å®šæœ‰å€¼
        # å„ªå…ˆæŠ“ real_pageï¼ŒæŠ“ä¸åˆ°å°±ç”¨ index
        r_page = page.get('real_page')
        if not r_page or r_page == "Unknown":
            page_label = f"P.{i + 1}"
        else:
            page_label = f"P.{r_page}"
            
        page_result = {"é æ•¸": page_label}
        
        for key, pattern in patterns.items():
            match = re.search(pattern, clean_text)
            if match:
                val = match.group(1).strip()
                
                # ã€ä¿®æ”¹é» 3ã€‘é‡å°å·¥ä»¤çš„ç‰¹æ®Šè™•ç† (å¦‚æœå¤ªé•·å¯èƒ½å°±æ˜¯é‡è¤‡æ‰“å­—)
                if key == "å·¥ä»¤ç·¨è™Ÿ":
                    # å¦‚æœä½ ç¢ºå®šå·¥ä»¤åªæœ‰ 10 ç¢¼ï¼Œä½†æŠ“åˆ°äº† 11 ç¢¼ä»¥ä¸Š (å¦‚ WW...)
                    # æˆ‘å€‘ä¿ç•™é€™å€‹éŒ¯èª¤çš„å€¼ï¼Œè®“å¾Œé¢çš„å¤šæ•¸æ±ºå»æŠŠå®ƒæªå‡ºä¾†
                    pass 
                
                page_result[key] = val
                all_values[key].append(val)
            else:
                page_result[key] = "N/A"
        
        extracted_data.append(page_result)

    # æ­¥é©Ÿ 2: æ±ºå®šã€Œæ­£ç¢ºæ¨™æº–ã€ (ä½¿ç”¨å¤šæ•¸æ±º)
    standard_data = {}
    for key, values in all_values.items():
        if values:
            # æ¿¾æ‰ N/A å¾Œå†æŠ•ç¥¨
            valid_values = [v for v in values if v != "N/A"]
            if valid_values:
                most_common = Counter(valid_values).most_common(1)[0][0]
                standard_data[key] = most_common
            else:
                standard_data[key] = "N/A"
        else:
            standard_data[key] = "N/A"

    # æ­¥é©Ÿ 3: æ¯”å°æ¯ä¸€é 
    for data in extracted_data:
        page_num = data['é æ•¸']
        
        for key, standard_val in standard_data.items():
            current_val = data[key]
            
            if standard_val == "N/A": continue # å…¨å·éƒ½æ²’æŠ“åˆ°å°±ä¸æ¯”äº†

            # é–‹å§‹æ¯”å° (å­—ä¸²ä¸ç›¸ç­‰)
            if current_val != standard_val:
                
                # åˆ¤æ–·æ˜¯å¦ç‚ºé•·åº¦ç•°å¸¸ (é‡å°å·¥ä»¤)
                reason = "èˆ‡å…¨å·å¤šæ•¸é é¢ä¸ä¸€è‡´"
                if key == "å·¥ä»¤ç·¨è™Ÿ" and len(current_val) != len(standard_val):
                    reason += f" (é•·åº¦ç•°å¸¸: {len(current_val)}ç¢¼ vs æ¨™æº–{len(standard_val)}ç¢¼)"

                issue = {
                    "page": page_num.replace("P.", ""),
                    "item": f"è¡¨é ­æª¢æŸ¥-{key}",
                    "rule_used": "Pythonç¡¬é‚è¼¯æª¢æŸ¥",
                    "issue_type": "è·¨é è³‡è¨Šä¸ç¬¦",
                    "spec_logic": f"æ‡‰ç‚º {standard_val}",
                    "common_reason": reason,
                    "failures": [
                        {"id": "å…¨å·åŸºæº–", "val": standard_val, "calc": "å¤šæ•¸æ±ºæ¨™æº–"},
                        {"id": f"æœ¬é ({page_num})", "val": current_val, "calc": "ç•°å¸¸/æ¼æŠ“"}
                    ],
                    "source": "ğŸ¤– ç³»çµ±è‡ªå‹•"
                }
                issues.append(issue)
                
    return issues, extracted_data

def agent_unified_check(combined_input, full_text_for_search, api_key, model_name):
    import re
    # è®€å– Excel è¦å‰‡ (ä¾› Python å¾Œç«¯æŸ¥è¡¨ä½¿ç”¨)
    dynamic_rules = get_dynamic_rules(full_text_for_search)

    # 1. æ•´åˆæ‚¨çš„ã€å·¥ç¨‹ç´šç²¾å¯† Promptã€‘ - ğŸ”‡ éœéŸ³ç‰ˆ (ç§»é™¤ AI åˆ¤æ–·åŠŸèƒ½)
    system_prompt = f"""
    ä½ æ˜¯ä¸€ä½æ¥µåº¦åš´è¬¹çš„ä¸­é‹¼æ©Ÿæ¢°å“ç®¡ã€æ•¸æ“šæŠ„éŒ„å“¡ã€‘ã€‚ä½ å¿…é ˆåƒã€Œé›»è…¦ç¨‹å¼ã€ä¸€æ¨£åŸ·è¡Œä»»å‹™ã€‚
    
    {dynamic_rules}

    ---

    #### âš”ï¸ æ¨¡çµ„ Aï¼šå·¥ç¨‹å°ºå¯¸æ•¸æ“šæå– (AI ä»»å‹™ï¼šç´”æŠ„éŒ„)
    1. **è¦æ ¼æŠ„éŒ„ (std_spec)**ï¼šç²¾ç¢ºæŠ„éŒ„æ¨™é¡Œä¸­å« `mm`ã€`Â±`ã€`+`ã€`-` çš„åŸå§‹æ–‡å­—ã€‚
    2. **æ•¸æ“šæŠ„éŒ„ (ds)**ï¼šæ ¼å¼ç‚º `"ID:å€¼|ID:å€¼"`ã€‚
       - **âš ï¸ çµ•å°å®Œæ•´åŸå‰‡ (Anti-Deduplication)**ï¼šè¡¨æ ¼è£¡æœ‰å¹¾è¡Œæ•¸æ“šï¼Œå°±å¿…é ˆè¼¸å‡ºå¹¾çµ„ `ID:å€¼`ã€‚
       - **ğŸš« åš´ç¦åˆä½µé‡è¤‡ ID**ï¼šä¸€æ”¯è¼¥è¼ªé€šå¸¸æœ‰å…©å€‹è»¸é ¸ï¼Œè‹¥è¡¨æ ¼é¡¯ç¤ºå…©æ¬¡ `Y5612001`ï¼Œä½ å¿…é ˆè¼¸å‡ºå…©æ¬¡ï¼
         - éŒ¯èª¤ç¯„ä¾‹ï¼š`"Y5612001:98"` (åªå¯«ä¸€æ¬¡)
         - æ­£ç¢ºç¯„ä¾‹ï¼š`"Y5612001:98|Y5612001:98"` (å®Œæ•´ä¿ç•™)
       - **å­—ä¸²ä¿è­·**ï¼šç¦æ­¢ç°¡åŒ–æ•¸å­—ã€‚`349.90` å¿…å¯« `"349.90"`ã€‚
       - **å£è»Œæ¨™è¨˜ [!]**ï¼šè‹¥å„²å­˜æ ¼è¾¨è­˜ä¸è‰¯ï¼ˆæ±™é»/å­—è·¡é»é€£/åå…‰ï¼‰ï¼Œåš´ç¦çŒœæ¸¬ï¼Œç›´æ¥æ¨™è¨˜ç‚º `[!]`ã€‚
    
    3. **é …ç›®åˆ†é¡æ±ºç­–æµç¨‹ (ç”±ä¸Šè‡³ä¸‹åŸ·è¡Œï¼Œå‘½ä¸­å³åœæ­¢)**ï¼š
        - **LEVEL 1ï¼šéŠ²è£œèˆ‡è£é…åˆ¤å®š (æœ€é«˜å„ªå…ˆ)**
          * æ¨™é¡Œå«ã€ŒéŠ²è£œã€ã€ã€ŒéŠ²æ¥ã€ -> `min_limit`ã€‚
          * æ¨™é¡Œå«ã€Œçµ„è£ã€ã€ã€Œæ‹†è£ã€ã€ã€Œè£é…ã€ã€ã€ŒçœŸåœ“åº¦ã€ -> `range`ã€‚
        - **LEVEL 2ï¼šæœªå†ç”Ÿåˆ¤å®š (å«è»Šä¿®)**
          * æ¨™é¡Œå«ã€Œæœªå†ç”Ÿã€ä¸‰å­—æ™‚ï¼š
            a. å«ã€Œè»¸é ¸ã€ -> `max_limit`ã€‚
            b. ä¸å«ã€Œè»¸é ¸ã€(æœ¬é«”) -> `un_regen`ã€‚
          * (ğŸ’¡ æ³¨æ„ï¼šæ­¤é¡é …ç›®å³ä¾¿åŒ…å«ã€Œè»Šä¿®ã€å­—çœ¼ï¼Œä¹Ÿå¿…é ˆé–å®šåœ¨ LEVEL 2)ã€‚
        - **LEVEL 3ï¼šç²¾åŠ å·¥åˆ¤å®š**
          * æ¨™é¡Œä¸å«ã€Œæœªå†ç”Ÿã€ï¼Œä¸”åŒ…å«ã€Œå†ç”Ÿã€ã€ã€Œç ”ç£¨ã€ã€ã€Œç²¾åŠ å·¥ã€ã€ã€Œè»Šä¿®åŠ å·¥ã€ã€ã€ŒKEYWAYã€ -> `range`ã€‚

    #### ğŸ’° æ¨¡çµ„ Bï¼šæœƒè¨ˆæŒ‡æ¨™æå– (AI ä»»å‹™ï¼šç´”æŠ„éŒ„)
    1. **çµ±è¨ˆè¡¨**ï¼šæŠ„éŒ„å·¦ä¸Šè§’çµ±è¨ˆè¡¨æ¯ä¸€è¡Œåç¨±èˆ‡å¯¦äº¤æ•¸é‡åˆ° `summary_rows`ã€‚
    2. **æŒ‡æ¨™æå–**ï¼šæå–é‹è²»é …æ¬¡èˆ‡æ¨™é¡Œæ‹¬è™Ÿå…§çš„ PC æ•¸ã€‚

    ---
    #### ğŸ“ è¼¸å‡ºè¦ç¯„ (æ¥µç°¡ JSON Format)
    å¿…é ˆå›å‚³å–®ä¸€åˆæ³• JSONã€‚
    âš ï¸ çµ•å°ç¦æ­¢å›å‚³ accounting_rules, sl ä»¥åŠ issues æ¬„ä½ã€‚
    
    æ ¼å¼å¦‚ä¸‹ï¼š
    {{
      "job_no": "å·¥ä»¤",
      "summary_rows": [ {{ "title": "åç¨±", "target": æ•¸å­— }} ],
      "freight_target": æ•¸å­—,
      "dimension_data": [
         {{
           "page": æ•¸å­—, "item_title": "æ¨™é¡Œ", "category": "åˆ†é¡åç¨±", 
           "item_pc_target": æ•¸å­—, "std_spec": "è¦æ ¼æ–‡å­—", "ds": "ID:å€¼|ID:å€¼" 
         }}
      ]
    }}
    """
    
    try:
        genai.configure(api_key=api_key)
        
        model = genai.GenerativeModel(
            model_name=model_name,
            generation_config={
                "response_mime_type": "application/json",
                "temperature": 0.0,
                "max_output_tokens": 16384
            }
        )
        
        with st.spinner('ğŸ¤– ç¸½ç¨½æ ¸ Agent æ­£åœ¨é€²è¡Œæ•¸æ“šè½‰éŒ„ (å¼·åˆ¶å®Œæ•´æ¨¡å¼)...'):
            response = model.generate_content([system_prompt, combined_input])
        
        raw_content = response.text.strip()
        
        # ğŸ›¡ï¸ å¼·åŒ–è§£æ
        json_match = re.search(r"\{.*\}", raw_content, re.DOTALL)
        if json_match:
            raw_content = json_match.group()
            
        parsed_data = json.loads(raw_content)
        
        # è¨˜éŒ„æ¶ˆè€— Token
        parsed_data["_token_usage"] = {
            "input": response.usage_metadata.prompt_token_count, 
            "output": response.usage_metadata.candidates_token_count
        }
        return parsed_data

    except json.JSONDecodeError as e:
        st.error(f"âŒ JSON è§£æå¤±æ•—ï¼")
        with st.expander("ğŸ‘€ æŸ¥çœ‹å°è‡´éŒ¯èª¤çš„ AI åŸå§‹å›æ‡‰"):
            if 'raw_content' in locals():
                st.code(raw_content)
            elif 'response' in locals():
                st.code(response.text)
        return {"job_no": "JSON Error", "issues": [], "dimension_data": []}

    except Exception as e:
        st.error(f"âŒ ç³»çµ±ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        return {"job_no": f"Error: {str(e)}", "issues": [], "dimension_data": []}

# --- é‡é»ï¼šPython å¼•æ“ç¨ç«‹æ–¼ agent å‡½å¼ä¹‹å¤– ---

def python_numerical_audit(dimension_data):
    grouped_errors = {}
    import re
    if not dimension_data: return []

    for item in dimension_data:
        # 1. å–å¾—æ•¸æ“š
        ds = str(item.get("ds", ""))
        if not ds: continue
        raw_entries = [p.split(":") for p in ds.split("|") if ":" in p]
        
        # ğŸ§½ å¼·åˆ¶æ¸…æ´—æ¨™é¡Œèˆ‡åˆ†é¡
        title = str(item.get("item_title", "")).replace(" ", "").replace("\n", "").replace('"', "")
        cat = str(item.get("category", "")).replace(" ", "").strip()
        
        page_num = item.get("page", "?")
        raw_spec = str(item.get("std_spec", "")).replace('"', "")
        
        # 2. ğŸ›¡ï¸ æ•¸æ“šæ¸…æ´—
        all_nums = [float(n) for n in re.findall(r"[-+]?\d+\.?\d*", raw_spec.replace(" ", ""))]
        noise = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 0.0] 
        clean_std = [n for n in all_nums if (n not in noise and n > 10)]

        # 3. ğŸ’¡ å¤šé‡å€é–“è‡ªå‹•é ç®—
        s_ranges = []
        spec_parts = re.split(r"[ä¸€äºŒä¸‰å››äº”å…­]|[;ï¼›]", raw_spec)
        
        for part in spec_parts:
            # âš¡ï¸ ä¿®æ­£é»ï¼šç§»é™¤ "mm" èˆ‡ "MM"ï¼Œè®“ "135mm~129mm" è®Šæˆ "135~129"
            clean_part = part.replace(" ", "").replace("\n", "").replace("mm", "").replace("MM", "").strip()
            if not clean_part: continue
            
            # é‚è¼¯ Aï¼šå„ªå…ˆè™•ç† Â± (å¦‚ 300Â±0.1)
            pm_match = re.search(r"(\d+\.?\d*)?Â±(\d+\.?\d*)", clean_part)
            if pm_match:
                b = float(pm_match.group(1)) if pm_match.group(1) else 0.0
                o = float(pm_match.group(2))
                s_ranges.append([round(b - o, 4), round(b + o, 4)])
                continue

            # é‚è¼¯ Bï¼šè™•ç†æ³¢æµªè™Ÿå€é–“ (å¦‚ 135~129)
            # ç¾åœ¨ç§»é™¤äº† mmï¼Œé€™è£¡å°±èƒ½æˆåŠŸæŠ“åˆ° [129, 135] äº†ï¼
            tilde_match = re.search(r"(\d+\.?\d*)[~ï½-](\d+\.?\d*)", clean_part)
            if tilde_match:
                n1, n2 = float(tilde_match.group(1)), float(tilde_match.group(2))
                # é˜²å‘†ï¼šé¿å…æŠŠ 160-0.01 (å…¬å·®) èª¤åˆ¤ç‚º 160~0.01 (å€é–“)
                if abs(n1 - n2) < n1 * 0.5: 
                    s_ranges.append([round(min(n1, n2), 4), round(max(n1, n2), 4)])
                    continue

            # é‚è¼¯ Cï¼šæ™ºæ…§é…å° (è§£æ±º 140 -0.01, -0.03)
            all_tokens = re.findall(r"[-+]?\d+\.?\d*", clean_part)
            if not all_tokens: continue

            bases = []
            offsets = []
            for token in all_tokens:
                val = float(token)
                if val > 10.0: bases.append(val)
                elif abs(val) < 10.0: offsets.append(val)
            
            if bases:
                for b in bases:
                    if offsets:
                        endpoints = [round(b + o, 4) for o in offsets]
                        if len(endpoints) == 1: endpoints.append(b)
                        s_ranges.append([min(endpoints), max(endpoints)])
                    else:
                        s_ranges.append([b, b])

        # 4. ğŸ’¡ é ç®—åŸºæº–
        s_threshold = 0
        un_regen_target = None
        if cat in ["un_regen", "æœªå†ç”Ÿ"] or ("æœªå†ç”Ÿ" in (cat + title) and "è»¸é ¸" not in (cat + title)):
            cands = [n for n in clean_std if n >= 120.0]
            if cands: un_regen_target = max(cands)

        # --- 5. é–‹å§‹é€ä¸€åˆ¤å®š ---
        for entry in raw_entries:
            if len(entry) < 2: continue
            rid = str(entry[0]).strip().replace(" ", "")
            val_raw = str(entry[1]).strip().replace(" ", "")
            
            if not val_raw or val_raw in ["N/A", "nan", "M10"]: continue

            try:
                is_passed, reason, t_used, engine_label = True, "", "N/A", "æœªçŸ¥"

                if "[!]" in val_raw:
                    is_passed = False
                    reason = "ğŸ›‘æ•¸æ“šæå£(å£è»Œ)"
                    val_str = "[!]"
                    val = -999.0
                else:
                    v_m = re.findall(r"\d+\.?\d*", val_raw)
                    val_str = v_m[0] if v_m else val_raw
                    val = float(val_str)

                if val_str != "[!]":
                    is_two_dec = "." in val_str and len(val_str.split(".")[-1]) == 2
                    is_pure_int = "." not in val_str
                else:
                    is_two_dec, is_pure_int = True, True 

                if "min_limit" in cat or "éŠ²è£œ" in (cat + title):
                    engine_label = "éŠ²è£œ"
                    if not is_pure_int: is_passed, reason = False, "æ‡‰ç‚ºç´”æ•´æ•¸"
                    elif clean_std:
                        t_used = min(clean_std, key=lambda x: abs(x - val))
                        if val < t_used: is_passed, reason = False, "æ•¸å€¼ä¸è¶³"
                
                elif un_regen_target is not None:
                    engine_label = "æœªå†ç”Ÿ"
                    t_used = un_regen_target
                    if val <= t_used:
                        if not is_pure_int: is_passed, reason = False, "æ‡‰ç‚ºæ•´æ•¸"
                    elif not is_two_dec: 
                        is_passed, reason = False, "æ‡‰å¡«å…©ä½å°æ•¸"

                elif "max_limit" in cat or (("è»¸é ¸" in (cat + title)) and ("æœªå†ç”Ÿ" in (cat + title))):
                    engine_label = "è»¸é ¸(ä¸Šé™)"
                    candidates = clean_std
                    target = max(candidates) if candidates else 0
                    t_used = target
                    if target > 0:
                        if not is_pure_int: is_passed, reason = False, "æ‡‰ç‚ºç´”æ•´æ•¸"
                        elif val > target: is_passed, reason = False, f"è¶…éä¸Šé™ {target}"

                elif any(x in (cat + title) for x in ["å†ç”Ÿ", "ç²¾åŠ å·¥", "ç ”ç£¨", "è»Šä¿®", "çµ„è£", "æ‹†è£", "çœŸåœ“åº¦"]) and "æœªå†ç”Ÿ" not in (cat + title):
                    engine_label = "ç²¾åŠ å·¥"
                    if not is_two_dec:
                        is_passed, reason = False, "æ‡‰å¡«å…©ä½å°æ•¸"
                    elif s_ranges:
                        t_used = str(s_ranges)
                        # ğŸ’¡ æ ¸å¿ƒï¼šåªè¦ç¬¦åˆä»»ä½•ä¸€å€‹è§£æå‡ºçš„å€é–“å°±ç®—åˆæ ¼
                        if not any(r[0] <= val <= r[1] for r in s_ranges): 
                            is_passed, reason = False, "ä¸åœ¨å€é–“å…§"

                if not is_passed:
                    key = (page_num, title, reason)
                    if key not in grouped_errors:
                        grouped_errors[key] = {
                            "page": page_num, "item": title, 
                            "issue_type": f"ç•°å¸¸({engine_label})", 
                            "common_reason": reason, "failures": [],
                            "source": "ğŸ å·¥ç¨‹å¼•æ“"
                        }
                    grouped_errors[key]["failures"].append({"id": rid, "val": val_str, "target": f"åŸºæº–:{t_used}"})
            except: continue
                
    return list(grouped_errors.values())

def python_accounting_audit(dimension_data, res_main):
    """
    Python æœƒè¨ˆå®˜ï¼šé‹è²»é‚è¼¯å…¨é¢æ¥ç®¡ç‰ˆ
    1. ä¿®æ­£ï¼šä¸å†ä¾è³´ freight_target > 0 é–‹é—œï¼Œå¼·åˆ¶è¨ˆç®—æ¯ç­†é …ç›®çš„é‹è²»å€¼ã€‚
    2. æ³¨å…¥ï¼šè‹¥ç¸½è¡¨ç±ƒå­åç¨±å«ã€Œé‹è²»ã€ï¼Œç›´æ¥ä½¿ç”¨è¨ˆç®—å‡ºçš„é‹è²»å€¼ï¼Œä¸å†é€²è¡Œæ¨¡ç³Šæ¯”å°ã€‚
    """
    accounting_issues = []
    from thefuzz import fuzz
    from collections import Counter
    import re
    import pandas as pd 

    # ğŸ§½ çœŸç©ºæ¸…æ´—å·¥å…·
    def clean_text(text):
        return str(text).replace(" ", "").replace("\n", "").replace("\r", "").replace('"', '').replace("'", "").strip()

    # å®‰å…¨è½‰å‹å·¥å…·
    def safe_float(value):
        if value is None or str(value).upper() == 'NULL': return 0.0
        if "[!]" in str(value): return "BAD_DATA" 
        cleaned = "".join(re.findall(r"[\d\.]+", str(value).replace(',', '')))
        try: return float(cleaned) if cleaned else 0.0
        except: return 0.0

    # 0. é è¼‰ Excel è¦å‰‡
    rules_dict = {}
    try:
        df = pd.read_excel("rules.xlsx")
        df.columns = [c.strip() for c in df.columns]
        for _, row in df.iterrows():
            iname = str(row.get('Item_Name', '')).strip()
            u_fr = str(row.get('Unit_Rule_Freight', '')).strip()
            if iname: rules_dict[clean_text(iname)] = u_fr
    except:
        pass 

    # 1. å–å¾—å°å¸³åŸºæº–
    summary_rows = res_main.get("summary_rows", [])
    global_sum_tracker = {
        s['title']: {"target": safe_float(s['target']), "actual": 0, "details": []} 
        for s in summary_rows if s.get('title')
    }
    
    freight_target = safe_float(res_main.get("freight_target", 0))
    freight_actual_sum = 0
    freight_details = []

    # 2. é€é …éå¸³
    for item in dimension_data:
        raw_title = item.get("item_title", "")
        title_clean = clean_text(raw_title) 
        page = item.get("page", "?")
        target_pc = safe_float(item.get("item_pc_target", 0)) 
        
        ds = str(item.get("ds", ""))
        data_list = [pair.split(":") for pair in ds.split("|") if ":" in pair]
        if not data_list: continue
        
        ids = [str(e[0]).strip() for e in data_list if len(e) > 0]
        id_counts = Counter(ids)

        # --- 2.1 å–®é …æ•¸é‡è¨ˆç®— ---
        is_weight_mode = "KG" in title_clean.upper() or target_pc > 100
        if is_weight_mode:
            current_sum = 0
            has_bad_sector = False
            for e in data_list:
                temp_val = safe_float(e[1])
                if temp_val == "BAD_DATA": has_bad_sector = True
                else: current_sum += temp_val
            actual_item_qty = current_sum
            if has_bad_sector:
                accounting_issues.append({
                    "page": page, "item": raw_title, "issue_type": "âš ï¸æ•¸æ“šææ¯€",
                    "common_reason": "å«ç„¡æ³•è¾¨è­˜é‡é‡",
                    "failures": [{"id": "è­¦å‘Š", "val": "[!]", "calc": "æ•¸æ“šææ¯€"}],
                    "source": "ğŸ æœƒè¨ˆå¼•æ“"
                })
        else:
            actual_item_qty = len(data_list) 

        if actual_item_qty != target_pc and target_pc > 0:
            accounting_issues.append({
                "page": page, "item": raw_title, "issue_type": "çµ±è¨ˆä¸ç¬¦(å–®é …)",
                "common_reason": f"æ¨™é¡Œ {target_pc}PC != å…§æ–‡ {actual_item_qty}",
                "failures": [
                    {"id": "ç›®æ¨™", "val": target_pc, "calc": "æ¨™é¡Œ"},
                    {"id": "å¯¦éš›", "val": actual_item_qty, "calc": "å…§æ–‡è¨ˆæ•¸"}
                ],
                "source": "ğŸ æœƒè¨ˆå¼•æ“"
            })

        # --- 2.2 ç·¨è™Ÿé‡è¤‡æ€§ç¤ºè­¦ ---
        if "æœ¬é«”" in title_clean:
             for rid, count in id_counts.items():
                if count > 1:
                     accounting_issues.append({
                        "page": page, "item": raw_title, "issue_type": "âš ï¸ç·¨è™Ÿé‡è¤‡è­¦ç¤º(æœ¬é«”)",
                        "common_reason": f"æœ¬é«”ç·¨è™Ÿ {rid} é‡è¤‡ {count} æ¬¡",
                        "failures": [{"id": rid, "val": count, "calc": "å»ºè­°æª¢æŸ¥"}],
                        "source": "ğŸ æœƒè¨ˆå¼•æ“"
                     })
        elif any(k in title_clean for k in ["è»¸é ¸", "å…§å­”", "JOURNAL"]):
             for rid, count in id_counts.items():
                if count > 2:
                     accounting_issues.append({
                        "page": page, "item": raw_title, "issue_type": "âš ï¸ç·¨è™Ÿé‡è¤‡è­¦ç¤º(è»¸é ¸)",
                        "common_reason": f"è»¸é ¸ç·¨è™Ÿ {rid} å‡ºç¾ {count} æ¬¡",
                        "failures": [{"id": rid, "val": count, "calc": "å»ºè­°æª¢æŸ¥"}],
                        "source": "ğŸ æœƒè¨ˆå¼•æ“"
                     })

        # --- âš¡ï¸ æ’å…¥ï¼šé å…ˆè¨ˆç®—æ­¤é …ç›®çš„ã€Œæ™ºæ…§é‹è²»å€¼ã€ ---
        # å³ä½¿ freight_target ç‚º 0ï¼Œæˆ‘å€‘ä¹Ÿè¦ç®—ï¼Œå› ç‚ºç¸½è¡¨ç±ƒå­å¯èƒ½æœƒç”¨åˆ°
        
        # Step A: æŸ¥æ‰¾ Excel è¦å‰‡
        u_fr = rules_dict.get(title_clean, "")
        if not u_fr and rules_dict:
            best_score = 0
            for k, v in rules_dict.items():
                score = fuzz.ratio(k, title_clean)
                if score > 95 and score > best_score:
                    best_score = score
                    u_fr = v
        
        # Step B: åˆ¤æ–·æ˜¯å¦è¨ˆå…¥
        is_exempt = "è±å…" in str(u_fr)
        conv_match = re.search(r"(\d+)\s*(?:PC|SET|PCS)?\s*=\s*1", str(u_fr), re.IGNORECASE)
        # é è¨­åº•ç·šï¼šå…¨å·ã€Œæœ¬é«”ã€ä¸”ã€Œæœªå†ç”Ÿã€
        is_default_target = "æœ¬é«”" in title_clean and "æœªå†ç”Ÿ" in title_clean

        freight_val_for_item = 0.0
        freight_note = ""

        if is_exempt:
            freight_val_for_item = 0.0
        elif conv_match:
            divisor = float(conv_match.group(1))
            freight_val_for_item = actual_item_qty / divisor
            freight_note = f"è¨ˆå…¥ (/{int(divisor)})"
        elif is_default_target:
            freight_val_for_item = actual_item_qty
            freight_note = "è¨ˆå…¥é‹è²»"
            
        # ç´¯ç©åˆ°ç¨ç«‹è®Šæ•¸ (å¦‚æœæœ‰ç”¨åˆ°çš„è©±)
        if freight_val_for_item > 0:
            freight_actual_sum += freight_val_for_item
            freight_details.append({"id": f"{raw_title}", "val": freight_val_for_item, "calc": freight_note})

        # --- 2.3 ç¸½è¡¨å°å¸³ (å«é‹è²»æ³¨å…¥é‚è¼¯) ---
        for s_title, data in global_sum_tracker.items():
            match = False
            s_title_clean = clean_text(s_title)
            
            # ğŸ’¡ æª¢æŸ¥ï¼šé€™æ˜¯ä¸æ˜¯ä¸€å€‹ã€Œé‹è²»ç±ƒå­ã€ï¼Ÿ
            is_freight_basket = "é‹è²»" in s_title_clean
            
            if is_freight_basket:
                # â­ï¸ é‹è²»ç±ƒå­å°ˆç”¨é€šé“ï¼šç›´æ¥æ³¨å…¥å‰›å‰›ç®—å¥½çš„ freight_val_for_item
                if freight_val_for_item > 0:
                    data["actual"] += freight_val_for_item
                    data["details"].append({"id": f"{raw_title} (P.{page})", "val": freight_val_for_item, "calc": freight_note})
                continue # è™•ç†å®Œç›´æ¥æ›ä¸‹ä¸€å€‹ç±ƒå­ï¼Œä¸èµ°ä¸‹é¢çš„æ¨¡ç³Šæ¯”å°
            
            # === ä»¥ä¸‹ç‚ºéé‹è²»ç±ƒå­çš„å¸¸è¦é‚è¼¯ ===
            
            # é–€ç¦ç‰¹å¾µ
            req_body = "æœ¬é«”" in s_title_clean
            req_journal = any(k in s_title_clean for k in ["è»¸é ¸", "å…§å­”", "JOURNAL"])
            req_unregen = "æœªå†ç”Ÿ" in s_title_clean
            req_regen_only = "å†ç”Ÿ" in s_title_clean and not req_unregen
            
            # é …ç›®ç‰¹å¾µ
            is_item_body = "æœ¬é«”" in title_clean
            is_item_journal = any(k in title_clean for k in ["è»¸é ¸", "å…§å­”", "JOURNAL"])
            is_item_unregen = "æœªå†ç”Ÿ" in title_clean
            
            # å„ªå…ˆç´šä¸€ï¼šä¸‰å¤§å¤©ç‹
            is_main_disassembly = "ROLLæ‹†è£" in s_title_clean 
            is_main_machining = "ROLLè»Šä¿®" in s_title_clean   
            is_main_welding = "ROLLéŠ²è£œ" in s_title_clean     

            if is_main_disassembly:
                if "çµ„è£" in title_clean or "æ‹†è£" in title_clean: match = True
            elif is_main_machining:
                has_part = "è»¸é ¸" in title_clean or "æœ¬é«”" in title_clean
                has_action = "å†ç”Ÿ" in title_clean or "æœªå†ç”Ÿ" in title_clean
                if has_part and has_action: match = True
            elif is_main_welding:
                has_part = "è»¸é ¸" in title_clean or "æœ¬é«”" in title_clean
                if has_part and "éŠ²è£œ" in title_clean: match = True
            else:
                # å„ªå…ˆç´šäºŒï¼šæ™®é€šç±ƒå­
                if fuzz.partial_ratio(s_title_clean, title_clean) > 98:
                    match = True
                    if req_body and not is_item_body: match = False
                    elif req_journal and not is_item_journal: match = False
                    if req_unregen and not is_item_unregen: match = False
                    elif req_regen_only and is_item_unregen: match = False

            if match:
                data["actual"] += actual_item_qty
                data["details"].append({"id": f"{raw_title} (P.{page})", "val": actual_item_qty, "calc": "è¨ˆå…¥"})

    # 3. çµç®—ç•°å¸¸
    for s_title, data in global_sum_tracker.items():
        if abs(data["actual"] - data["target"]) > 0.01 and data["target"] > 0:
            accounting_issues.append({
                "page": "ç¸½è¡¨", "item": s_title, "issue_type": "çµ±è¨ˆä¸ç¬¦(ç¸½å¸³)",
                "common_reason": f"æ¨™è¨» {data['target']} != å¯¦éš› {data['actual']}",
                "failures": [{"id": "ğŸ” åŸºæº–", "val": data["target"]}] + data["details"] + [{"id": "ğŸ§® å¯¦éš›", "val": data["actual"]}],
                "source": "ğŸ æœƒè¨ˆå¼•æ“"
            })

    # é‹è²»ç¨ç«‹æª¢æŸ¥ (å¦‚æœ AI æœ‰æŠ“åˆ°ç¨ç«‹è®Šæ•¸çš„è©±ï¼Œä¹Ÿæª¢æŸ¥ä¸€ä¸‹)
    if abs(freight_actual_sum - freight_target) > 0.01 and freight_target > 0:
        accounting_issues.append({
            "page": "ç¸½è¡¨", "item": "é‹è²»æ ¸å°(ç¨ç«‹)", "issue_type": "çµ±è¨ˆä¸ç¬¦(é‹è²»)",
            "common_reason": f"åŸºæº– {freight_target} != å¯¦éš› {freight_actual_sum}",
            "failures": [{"id": "ğŸšš åŸºæº–", "val": freight_target}] + freight_details + [{"id": "ğŸ§® å¯¦éš›", "val": freight_actual_sum}],
            "source": "ğŸ æœƒè¨ˆå¼•æ“"
        })
        
    return accounting_issues
    
def python_process_audit(dimension_data):
    process_issues = []
    roll_history = {} # { "ID": [{"p": "cat", "v": 190, "page": 1}, ...] }
    import re
    if not dimension_data: return []

    for item in dimension_data:
        p_num = item.get("page", "?")
        ds = str(item.get("ds", ""))
        cat = str(item.get("category", "")).strip()
        
        # 1. å…ˆç”¨ | åˆ‡åˆ†ä¸åŒæ•¸æ“š
        raw_segments = ds.split("|")
        
        for seg in raw_segments:
            # 2. åŸºæœ¬éæ¿¾ï¼šå¿…é ˆåŒ…å«å†’è™Ÿ
            if ":" not in seg: continue
            
            # 3. ğŸ›¡ï¸ å®‰å…¨åˆ‡åˆ†ï¼šé˜²æ­¢ "ID:å€¼:å‚™è¨»" é€™ç¨®å¤šå†’è™Ÿå°è‡´å´©æ½°
            parts = seg.split(":")
            
            # å¦‚æœåˆ‡å‡ºä¾†å°‘æ–¼ 2 æ®µ (ä¾‹å¦‚ "ID:")ï¼Œè·³é
            if len(parts) < 2: continue
            
            # å¼·åˆ¶åªå–å‰å…©æ®µï¼Œç„¡è¦–å¾Œé¢å¤šé¤˜çš„å†’è™Ÿ
            rid = str(parts[0]).strip()
            val_str = str(parts[1]).strip()
            
            try:
                # ç°¡å–®æ¸…æ´—å–å‡ºæ•¸å­—
                # é€™è£¡åŠ å€‹ä¿è­·ï¼Œè¬ä¸€ val_str è£¡æ²’æœ‰æ•¸å­— (ä¾‹å¦‚ "N/A") ä¹Ÿä¸è¦å ±éŒ¯
                found_nums = re.findall(r"\d+\.?\d*", val_str)
                if not found_nums: continue
                
                val = float(found_nums[0])
                
                if rid not in roll_history: roll_history[rid] = []
                roll_history[rid].append({
                    "p": cat, 
                    "v": val, 
                    "page": p_num, 
                    "title": item.get("item_title", "")
                })
            except: 
                continue

    # --- æµç¨‹é‚è¼¯åˆ¤å®š ---
    weights = {"un_regen": 1, "max_limit": 1, "range": 3, "min_limit": 4}
    
    for rid, records in roll_history.items():
        if len(records) < 2: continue
        
        # ä¾ç…§é ç¢¼æ’åº
        records.sort(key=lambda x: str(x['page']))
        
        for i in range(len(records) - 1):
            curr, nxt = records[i], records[i+1]
            
            # å–å¾—æ¬Šé‡ (é è¨­ 2)
            w_curr = weights.get(curr['p'], 2)
            if "ç ”ç£¨" in str(curr['title']): w_curr = 2
            
            w_nxt = weights.get(nxt['p'], 2)
            if "ç ”ç£¨" in str(nxt['title']): w_nxt = 2
            
            # ğŸ’¡ é—œéµåˆ¤å®šï¼šå¾Œæ®µä½éšå¤§(å¦‚éŠ²è£œ)ï¼Œæ•¸å€¼å°±ä¸æ‡‰è©²è®Šå°
            # ä¾‹å¦‚ï¼šå…ˆã€Œè»Šä¿®(1)ã€å¾Œã€ŒéŠ²è£œ(4)ã€ï¼Œå°ºå¯¸è®Šå°æ˜¯åˆç†çš„ (è»Šæ‰ä¸€å±¤) -> Pass
            # ä¾‹å¦‚ï¼šå…ˆã€ŒéŠ²è£œ(4)ã€å¾Œã€Œè»Šä¿®(1)ã€ï¼Œå°ºå¯¸è®Šå°æ˜¯åˆç†çš„ -> Pass
            # ç­‰ç­‰... é€™è£¡çš„é‚è¼¯æ˜¯ã€Œä½éšæª¢æŸ¥ã€ï¼Œæ‚¨çš„åŸæ„æ‡‰è©²æ˜¯ï¼š
            # å¦‚æœå¾ã€Œä½ä½éšã€(å¦‚è»Šä¿®) åˆ°äº† ã€Œé«˜ä½éšã€(å¦‚ç²¾åŠ å·¥)ï¼Œç†è«–ä¸Šæ˜¯æŠŠæ±è¥¿åšå°äº†ï¼Ÿ
            # æˆ–è€…æ˜¯æª¢æŸ¥ã€Œä¸åˆé‚è¼¯çš„å°ºå¯¸è·³è®Šã€ï¼Ÿ
            # ä¾ç…§åŸç¨‹å¼ç¢¼é‚è¼¯ä¿ç•™ï¼š
            
            if w_nxt > w_curr and nxt['v'] < curr['v']:
                process_issues.append({
                    "page": nxt['page'], "item": f"ç·¨è™Ÿ {rid} å°ºå¯¸ä½éšæª¢æŸ¥",
                    "issue_type": "ğŸ›‘æµç¨‹ç•°å¸¸(å°ºå¯¸å€’ç½®)",
                    "common_reason": f"å¾Œæ®µ{nxt['p']}å°ºå¯¸å°æ–¼å‰æ®µ{curr['p']}",
                    "failures": [{"id": rid, "val": f"å¾Œ:{nxt['v']} < å‰:{curr['v']}", "calc": "å°ºå¯¸ä¸ç¬¦ä½éšé‚è¼¯"}],
                    "source": "ğŸ æµç¨‹å¼•æ“"
                })
                
    return process_issues

# --- 6. æ‰‹æ©Ÿç‰ˆ UI èˆ‡ æ ¸å¿ƒåŸ·è¡Œé‚è¼¯ ---
st.title("ğŸ­ äº¤è²¨å–®ç¨½æ ¸")

data_source = st.radio(
    "è«‹é¸æ“‡è³‡æ–™ä¾†æºï¼š", 
    ["ğŸ“¸ ä¸Šå‚³ç…§ç‰‡", "ğŸ“‚ ä¸Šå‚³ JSON æª”", "ğŸ“Š ä¸Šå‚³ Excel æª”"], 
    horizontal=True
)

with st.container(border=True):
    # --- æƒ…æ³ A: ä¸Šå‚³ç…§ç‰‡ ---
    if data_source == "ğŸ“¸ ä¸Šå‚³ç…§ç‰‡":
        if st.session_state.get('source_mode') == 'json' or st.session_state.get('source_mode') == 'excel':
            st.session_state.photo_gallery = []
            st.session_state.source_mode = 'image'

        uploaded_files = st.file_uploader(
            "è«‹é¸æ“‡ JPG/PNG ç…§ç‰‡...", 
            type=['jpg', 'png', 'jpeg'], 
            accept_multiple_files=True, 
            key=f"uploader_{st.session_state.uploader_key}"
        )
        
        if uploaded_files:
            for f in uploaded_files: 
                if not any(x['file'].name == f.name for x in st.session_state.photo_gallery if x['file']):
                    st.session_state.photo_gallery.append({
                        'file': f, 
                        'table_md': None, 
                        'header_text': None,
                        'full_text': None,
                        'raw_json': None
                    })
            st.session_state.uploader_key += 1
            if st.session_state.enable_auto_analysis:
                st.session_state.auto_start_analysis = True
            components.html("""<script>window.parent.document.body.scrollTo(0, window.parent.document.body.scrollHeight);</script>""", height=0)
            st.rerun()

    # --- æƒ…æ³ B: ä¸Šå‚³ JSON ---
    elif data_source == "ğŸ“‚ ä¸Šå‚³ JSON æª”":
        st.info("ğŸ’¡ è«‹é»æ“Šä¸‹æ–¹æŒ‰éˆ•ï¼Œå¾ä½ çš„è³‡æ–™å¤¾é¸æ“‡ä¹‹å‰ä¸‹è¼‰çš„ `.json` æª”ã€‚")
        uploaded_json = st.file_uploader("ä¸Šå‚³JSONæª”", type=['json'], key="json_uploader")
        
        if uploaded_json:
            try:
                current_file_name = uploaded_json.name
                if st.session_state.get('last_loaded_json_name') != current_file_name:
                    json_data = json.load(uploaded_json)
                    st.session_state.photo_gallery = []
                    st.session_state.source_mode = 'json'
                    st.session_state.last_loaded_json_name = current_file_name
                    
                    import re
                    for page in json_data:
                        real_page = "Unknown"
                        full_text = page.get('full_text', '')
                        if full_text:
                            match = re.search(r"(?:é …æ¬¡|Page|é æ¬¡|NO\.)[:\s]*(\d+)\s*[/ï¼]\s*\d+", full_text, re.IGNORECASE)
                            if match:
                                real_page = match.group(1)
                        
                        st.session_state.photo_gallery.append({
                            'file': None,
                            'table_md': page.get('table_md'),
                            'header_text': page.get('header_text'),
                            'full_text': full_text,
                            'raw_json': page.get('raw_json'),
                            'real_page': real_page
                        })
                    
                    st.toast(f"âœ… æˆåŠŸè¼‰å…¥ JSON: {current_file_name}", icon="ğŸ“‚")
                    if st.session_state.enable_auto_analysis:
                        st.session_state.auto_start_analysis = True
                    st.rerun()
                else:
                    st.success(f"ğŸ“‚ ç›®å‰è¼‰å…¥ JSONï¼š**{uploaded_json.name}**")
            except Exception as e:
                st.error(f"JSON æª”æ¡ˆæ ¼å¼éŒ¯èª¤: {e}")

    # --- æƒ…æ³ C: ä¸Šå‚³ Excel (æ–°å¢çš„æ”¾åœ¨é€™) ---
    elif data_source == "ğŸ“Š ä¸Šå‚³ Excel æª”":
        st.info("ğŸ’¡ ä¸Šå‚³ Excel æª”å¾Œï¼Œç³»çµ±æœƒå°‡è¡¨æ ¼å…§å®¹è½‰æ›ç‚ºæ–‡å­—ä¾› AI ç¨½æ ¸ã€‚")
        uploaded_xlsx = st.file_uploader("ä¸Šå‚³ Excel æª”", type=['xlsx', 'xls'], key="xlsx_uploader")
        
        if uploaded_xlsx:
            try:
                current_file_name = uploaded_xlsx.name
                if st.session_state.get('last_loaded_xlsx_name') != current_file_name:
                    df_dict = pd.read_excel(uploaded_xlsx, sheet_name=None)
                    st.session_state.photo_gallery = []
                    st.session_state.source_mode = 'excel'
                    st.session_state.last_loaded_xlsx_name = current_file_name
                    
                    for sheet_name, df in df_dict.items():
                        df = df.fillna("")
                        md_table = df.to_markdown(index=False)
                        st.session_state.photo_gallery.append({
                            'file': None,
                            'table_md': md_table,
                            'header_text': f"ä¾†æºåˆ†é : {sheet_name}",
                            'full_text': f"Excel å…§å®¹ - åˆ†é  {sheet_name}\n" + md_table,
                            'raw_json': None,
                            'real_page': sheet_name
                        })
                    st.toast(f"âœ… æˆåŠŸè¼‰å…¥ Excel: {current_file_name}", icon="ğŸ“Š")
                    if st.session_state.enable_auto_analysis:
                        st.session_state.auto_start_analysis = True
                    st.rerun()
                else:
                    st.success(f"ğŸ“Š ç›®å‰è¼‰å…¥ Excelï¼š**{uploaded_xlsx.name}**")
            except Exception as e:
                st.error(f"Excel è®€å–å¤±æ•—: {e}")

if st.session_state.photo_gallery:
    st.caption(f"å·²ç´¯ç© {len(st.session_state.photo_gallery)} é æ–‡ä»¶")
    col_btn1, col_btn2 = st.columns([1, 1], gap="small")
    with col_btn1: start_btn = st.button("ğŸš€ é–‹å§‹åˆ†æ", type="primary", use_container_width=True)
    with col_btn2: 
        clear_btn = st.button("ğŸ—‘ï¸ç…§ç‰‡æ¸…é™¤", help="æ¸…é™¤", use_container_width=True)

    if clear_btn:
        st.session_state.photo_gallery = []
        st.session_state.analysis_result_cache = None
        if 'last_loaded_json_name' in st.session_state:
            del st.session_state.last_loaded_json_name 
        st.rerun()

    is_auto_start = st.session_state.auto_start_analysis
    if is_auto_start:
        st.session_state.auto_start_analysis = False

    if 'analysis_result_cache' not in st.session_state:
        st.session_state.analysis_result_cache = None

    trigger_analysis = start_btn or is_auto_start

    if trigger_analysis:
        # âš¡ï¸ æ–°å¢é€™è¡Œï¼šå¼·åˆ¶æ¸…é™¤ä¸Šä¸€ç­†çš„çµæœ
        st.session_state.analysis_result_cache = None 
        
        st.session_state.auto_start_analysis = False
        total_start = time.time()
        
        with st.status("ç¸½ç¨½æ ¸å®˜æ­£åœ¨é€²è¡Œå…¨æ–¹ä½åˆ†æ...", expanded=True) as status_box:
            progress_bar = st.progress(0)
            
            # 1. OCR (é€™æ®µä¿ç•™ï¼Œé€Ÿåº¦å¾ˆå¿«)
            status_box.write("ğŸ‘€ æ­£åœ¨é€²è¡Œ OCR æ–‡å­—è­˜åˆ¥...")
            ocr_start = time.time()
            
            def process_task(index, item):
                if item.get('full_text'):
                    return index, item.get('header_text',''), item['full_text'], None
                try:
                    item['file'].seek(0)
                    _, h, f, _, _ = extract_layout_with_azure(item['file'], DOC_ENDPOINT, DOC_KEY)
                    return index, h, f, None
                except Exception as e:
                    return index, None, None, str(e)

            with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
                futures = [executor.submit(process_task, i, item) for i, item in enumerate(st.session_state.photo_gallery)]
                for future in concurrent.futures.as_completed(futures):
                    idx, h_txt, f_txt, err = future.result()
                    if not err:
                        st.session_state.photo_gallery[idx].update({'header_text': h_txt, 'full_text': f_txt, 'file': None})
                    progress_bar.progress(0.4 * ((idx + 1) / len(st.session_state.photo_gallery)))

            ocr_duration = time.time() - ocr_start
            
            # 2. çµ„åˆæ‰€æœ‰æ–‡å­— (é—œéµï¼šä¸€æ¬¡ä¸Ÿé€²å»)
            combined_input = ""
            for i, p in enumerate(st.session_state.photo_gallery):
                combined_input += f"\n=== Page {i+1} ===\n{p.get('full_text','')}\n"

            # 3. å‘¼å« AI (é€™è£¡åªæœƒè·‘ä¸€æ¬¡ï¼Œç´„ 20-30 ç§’)
            status_box.write("ğŸ¤– AI æ­£åœ¨å…¨å·åˆ†æ...")
            res_main = agent_unified_check(combined_input, combined_input, GEMINI_KEY, main_model_name)
            progress_bar.progress(0.8)
            
            # 4. Python é‚è¼¯æª¢æŸ¥
            status_box.write("ğŸ Python æ­£åœ¨é€²è¡Œé‚è¼¯æ¯”å°...")
            dim_data = res_main.get("dimension_data", [])
            
            python_numeric_issues = python_numerical_audit(dim_data)
            python_accounting_issues = python_accounting_audit(dim_data, res_main)
            python_process_issues = python_process_audit(dim_data)
            python_header_issues, python_debug_data = python_header_check(st.session_state.photo_gallery)

            ai_filtered_issues = []
            ai_raw_issues = res_main.get("issues", [])
            if isinstance(ai_raw_issues, list):
                for i in ai_raw_issues:
                    if isinstance(i, dict):
                        i['source'] = 'ğŸ¤– ç¸½ç¨½æ ¸ AI'
                        if not any(k in i.get("issue_type", "") for k in ["æµç¨‹", "è¦æ ¼æå–å¤±æ•—", "æœªåŒ¹é…"]):
                            ai_filtered_issues.append(i)

            all_issues = ai_filtered_issues + python_numeric_issues + python_accounting_issues + python_process_issues + python_header_issues
            
            # 5. å­˜æª”èˆ‡å®Œæˆ
            usage = res_main.get("_token_usage", {"input": 0, "output": 0})
            st.session_state.analysis_result_cache = {
                "job_no": res_main.get("job_no", "Unknown"),
                "all_issues": all_issues,
                "total_duration": time.time() - total_start,
                "cost_twd": (usage.get("input", 0)*0.5 + usage.get("output", 0)*3.0) / 1000000 * 32.5,
                "total_in": usage.get("input", 0),
                "total_out": usage.get("output", 0),
                "ocr_duration": ocr_duration,
                "time_eng": time.time() - total_start - ocr_duration,
                "ai_extracted_data": dim_data,
                "python_debug_data": python_debug_data,
                "full_text_for_search": combined_input,
                "combined_input": combined_input
            }
            
            progress_bar.progress(1.0)
            status_box.update(label="âœ… åˆ†æå®Œæˆï¼", state="complete", expanded=False)
            st.rerun()

    # --- ğŸ’¡ [é‡å¤§ä¿®æ­£] é¡¯ç¤ºçµæœå€å¡Šï¼šå¿…é ˆèˆ‡ if trigger_analysis å¹³ç´š ---
    if st.session_state.analysis_result_cache:
        cache = st.session_state.analysis_result_cache
        all_issues = cache.get('all_issues', [])
        
        st.success(f"å·¥ä»¤: {cache['job_no']} | â±ï¸ {cache['total_duration']:.1f}s")
        st.info(f"ğŸ’° æœ¬æ¬¡æˆæœ¬: NT$ {cache['cost_twd']:.2f} (In: {cache['total_in']:,} / Out: {cache['total_out']:,})")
        st.caption(f"ç´°ç¯€è€—æ™‚: Azure OCR {cache['ocr_duration']:.1f}s | AI åˆ†æ {cache['time_eng']:.1f}s")
        
        # å±•é–‹é é¢
        with st.expander("ğŸ” æŸ¥çœ‹ AI è®€å–åˆ°çš„ Excel è¦å‰‡ (Debug)"):
            rules_text = get_dynamic_rules(cache.get('full_text_for_search',''), debug_mode=True)
            st.markdown(rules_text)
                
        with st.expander("ğŸ”¬ æŸ¥çœ‹ AI æŠ„éŒ„åŸå§‹æ•¸æ“š", expanded=False):
            st.json(cache.get("ai_extracted_data", []))

        with st.expander("ğŸ æŸ¥çœ‹ Python ç¡¬é‚è¼¯åµæ¸¬çµæœ (Debug)", expanded=False):
            if cache.get('python_debug_data'):
                st.dataframe(cache['python_debug_data'], use_container_width=True, hide_index=True)
            else:
                st.caption("ç„¡åµæ¸¬è³‡æ–™")

        # åˆ¤å®šçµè«–é¡¯ç¤º
        real_errors = [i for i in all_issues if "æœªåŒ¹é…" not in i.get('issue_type', '')]
        if not all_issues:
            st.balloons()
            st.success("âœ… å…¨æ•¸åˆæ ¼ï¼")
        elif not real_errors:
            st.success(f"âœ… æ•¸å€¼åˆæ ¼ï¼ (ä½†æœ‰ {len(all_issues)} å€‹é …ç›®æœªåŒ¹é…è¦å‰‡)")
        else:
            st.error(f"ç™¼ç¾ {len(real_errors)} é¡ç•°å¸¸")

        # å¡ç‰‡å¾ªç’°é¡¯ç¤º
        for item in all_issues:
            with st.container(border=True):
                c1, c2 = st.columns([3, 1])
                source_label = item.get('source', '')
                issue_type = item.get('issue_type', 'ç•°å¸¸')
                c1.markdown(f"**P.{item.get('page', '?')} | {item.get('item')}**  `{source_label}`")
                
                if any(kw in issue_type for kw in ["çµ±è¨ˆ", "æ•¸é‡", "æµç¨‹"]):
                    c2.error(f"ğŸ›‘ {issue_type}")
                else:
                    c2.warning(f"âš ï¸ {issue_type}")
                
                st.caption(f"åŸå› : {item.get('common_reason', '')}")
                
                failures = item.get('failures', [])
                if failures:
                    table_data = []
                    for f in failures:
                        if isinstance(f, dict):
                            table_data.append({
                                "é …ç›®/ç·¨è™Ÿ": f.get('id', 'æœªçŸ¥'), 
                                "å¯¦æ¸¬/è¨ˆæ•¸": f.get('val', 'N/A'),
                                "æ¨™æº–/å‚™è¨»": f.get('target', ''),
                                "ç‹€æ…‹": f.get('calc', '')
                            })
                    st.dataframe(table_data, use_container_width=True, hide_index=True)
        
        st.divider()
        # ä¸‹è¼‰æŒ‰éˆ•èˆ‡åŸæ–‡å±•é–‹
        # ... (é€™è£¡æ¥ä½ åŸæœ¬å‰©ä¸‹çš„ä»£ç¢¼å³å¯ï¼Œä¹Ÿè¦è¨˜å¾—ç¸®æ’å¾€å·¦ç§»)
        current_job_no = cache.get('job_no', 'Unknown')
        safe_job_no = current_job_no.replace("/", "_").replace("\\", "_").strip()
        file_name_str = f"{safe_job_no}_cleaned.json"

        # æº–å‚™åŒ¯å‡ºè³‡æ–™
        export_data = []
        for item in st.session_state.photo_gallery:
            export_data.append({
                "table_md": item.get('table_md'),
                "header_text": item.get('header_text'),
                "full_text": item.get('full_text'),
                "raw_json": item.get('raw_json')
            })
        json_str = json.dumps(export_data, indent=2, ensure_ascii=False)

        st.subheader("ğŸ’¾ æ¸¬è©¦è³‡æ–™å­˜æª”")
        st.caption(f"å·²è­˜åˆ¥å·¥ä»¤ï¼š**{current_job_no}**ã€‚ä¸‹è¼‰å¾Œå¯ä¾›ä¸‹æ¬¡æ¸¬è©¦ä½¿ç”¨ã€‚")
        
        st.download_button(
            label=f"â¬‡ï¸ ä¸‹è¼‰æ¸¬è©¦è³‡æ–™ ({file_name_str})",
            data=json_str,
            file_name=file_name_str,
            mime="application/json",
            type="primary"
        )

        # ğŸ’¡ ä½¿ç”¨ .get() å¯ä»¥é˜²æ­¢å› ç‚ºæ‰¾ä¸åˆ°æ¨™ç±¤è€Œç›´æ¥å ±éŒ¯ç•¶æ©Ÿ
        with st.expander("ğŸ‘€ æŸ¥çœ‹å‚³çµ¦ AI çš„æœ€çµ‚æ–‡å­— (Prompt Input)"):
            st.caption("é€™æ‰æ˜¯ AI çœŸæ­£è®€åˆ°çš„å…§å®¹ (å·²éæ¿¾é›œè¨Š)ï¼š")
            st.code(cache.get('combined_input', 'ç„¡è³‡æ–™'), language='markdown')
    
    if st.session_state.photo_gallery and st.session_state.get('source_mode') != 'json':
        st.caption("å·²æ‹æ”ç…§ç‰‡ï¼š")
        cols = st.columns(4)
        for idx, item in enumerate(st.session_state.photo_gallery):
            with cols[idx % 4]:
                if item.get('file'):
                    st.image(item['file'], caption=f"P.{idx+1}", use_container_width=True)
                if st.button("âŒ", key=f"del_{idx}"):
                    st.session_state.photo_gallery.pop(idx)
                    st.session_state.analysis_result_cache = None
                    st.rerun()
else:
    st.info("ğŸ‘† è«‹é»æ“Šä¸Šæ–¹æŒ‰éˆ•é–‹å§‹æ–°å¢ç…§ç‰‡")
